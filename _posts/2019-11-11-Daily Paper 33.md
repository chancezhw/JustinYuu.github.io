---
layout: post
title: "Daily Paper 33："
description: "Notes"
categories: [MMML-VLN]
tags: [Paper]
redirect_from:
  - /2019/11/11/
---

# Daily Paper 33 - Reinforced Cross-Modal Matching and Self-Supervised Imitation Learning for Vision-Language Navigation  

## Introduction  

由于组内布置的自监督跨模态学习方向的paper已经看完了，我就可以继续看我的VLN领域的paper了。今天的这篇paper的来头可不小，2019CVPR的best paper，总分满分排名第一，是UCSB、微软和杜克大学共同发表的。  

VLN指的是对一个embodied agent进行导航，使其在真实的3D环境中执行自然语言指令。这篇paper主要聚焦VLN任务中比较困难三个部分：跨媒体grounding，不适当反馈和泛化问题。首先，作者提出了一个强化跨模态匹配RCM系统，通过强化学习的方式来增强了局部和全局跨模态匹配的表现。特别的，使用一个对应的critic来提供固有的奖励，用来激励指令和轨迹之间的全局同步，使用一个推理导航器来进行局部视野的跨模态识别。在VLN领域的benchmark数据集上进行评价，得出作者的RCM模型能够显著的在SPL上提升10%的表现，并能够得到当今最好的表现。最后，为了提高学习策略的泛化性能，作者进一步介绍了一个自监督模仿学习（SIL）方法，通过使模型模仿自己在过去做的好决定，来探索新的未曾见到过的新环境。作者证明了SIL学习策略能够逼近一个更为高效和有效的策略，这显著的降低了VLN模型在新环境和旧环境之间的表现差距。  

近年来，由于VLN在日常生活中的广泛应用，比如家用机器人和私人助理，vision-language grounded embodied agents受到了越来越多的关注。与此同时，这样的agent将其自身放置在一个第一视角下的实时学习场景下，从而进行visual和language grounding。在众多任务中，VLN视觉语言导航是一个典型的例子，它接收一系列自然语言指令，然后根据指令在自然世界中导航agent。VLN要求对于语言语义、视觉感知有充分的理解，更重要的是对于两者理解后的对齐。  

VLN有一些比较独特的挑战：1.理解视觉图片和自然语言可能会比较难，agent需要将自然语言指令在局部视觉空间中处理成为一个序列状的词语表示，并且要把指令和在全局时序空间中的视觉轨迹相对应。第二，除了严格遵守专家指定路线以外，整个VLN任务的反馈很弱，因为成功的标志是作者找到了目标物体，但是却并不关心具体的路径是什么，那么agent有可能按照指令走，也有可能按照随机路线走，这并没有一个合适的反馈来判别。此外即使agent行走的路径完完全全按照原指令进行，但是最终停止的位置略微早了一点，那么VLN系统会判定最终的定位失败，整个流程会被判定为不成功，但是这个时候路径的成功并不会被探测到，也会被标记为错误，给学习带来了误导，从而导致最终的结果偏离了最优的策略学习。第三，现有的工作泛化性能都不是太好，在熟悉和陌生的环境中的表现差距太大。  

这篇文章中，作者主要将强化学习和模仿学习的力量应用在了VLN任务中，首先他们介绍了RCM，使得模型可以通过强化学习进行局部和全局的跨模态grounding。特别的，作者设计了一个推理导航器reasoning navigator，同时通过局部视角和语言指令来学习跨模态grounding，从而使得agent可以推理应该着重注意哪些子指令。以及应该看哪里。从全局视角来看，作者给agent装备了一个matching critic，这个东西可以评估agent行进的路径，具体方法是重建原始指令的概率，这也被作者称为循环重建奖励。简单来说，这个东西的作用就是提供一个固定的奖励信号，用来使得模型不断修正其路径，使得最终的路径和指示中的路径尽可能的一致。  

那么总结一下，模型会得到两个reward，第一个是内在的固有reward，用于评价路径是否正确，另一个是外在的环境reward，用于评价是否找到目标点。作者的RCM模型显著的超越了现有所有模型的表现，并在R2R数据集上获得了STOA表现。  

此外为了增强模型的泛化性，作者提出了一个自监督模仿学习模型，对未标注的未知环境进行探索，agent试图模仿之前的好的经历来解决新环境中的问题。具体来说，在我们的框架里，导航器进行了多次的roll-outs，通过matching critic决定的好的学习策略被存储在一个回放缓冲池中用于后续导航器的模仿，因此导航器可以逐渐逼近它的最好表现，从而得到一个好的策略。  


作者又将其贡献总结了一遍，我真的不想第三次复述了，就把这重复的一段略过吧。  

## Related Works  

这篇paper介绍的详细一点，简单说下Related Works，首先是Vision-and-Language Grounding部分，近年来大家都在想如何把NLP和CV这两个巨火无比的领域和坐起来，于是就搞了跨媒体这么一个领域，主要研究视觉和语言或者视觉和听觉之间的联系。大家研究的比较多的领域比如VQA、VCR、MR等等，都有一个共同点，即视频是不变的，视频就是给定的那个视频，比如机器翻译和AVC等问题，音频也是不变的，模型只需要给什么处理什么就好了，按照同样的范式处理即可，VQA和VCR等任务只是给定的问题不同，视频场景还是不变的。那么到了VLN问题中，视频会随着agent的时间和空间位置而动态变化，这是因为agent是在不断移动的，而agent能看到的空间视觉信息只有它自己的第一视角，那么VLN任务就需要agent能够与周围的环境进行实时的交流，也相应的会更加困难。  

其次再来看Embodied Navigation Agent，对于一个移动信息系统来说，进行导航应该说是比较基础和重要的任务。Anderson首先提出了VLN问题，并给出了一个基于注意力机制的seq2seq baseline模型。Wang等人接着提出了一个hybrid模型，将model-free和model-based的强化学习方法结合在了一起，用于提高模型的泛化能力。最近Fried等人提出了一个speaker-follower模型，使用数据增强、全景动作空间和改正版集束搜索来处理VLN问题，并在R2R上得到了STOA的结果。作者提出的RCM模型虽然也是使用强化学习方法，但是还是有一些不同的，具体的不同有三点：1.将强化学习和模仿学习结合在了一起，而Fried等人的方法只用了监督学习；2.作者的推理导航器进行了跨模态grounding，而不是单一模态的时序注意力机制；3.作者的matching critic虽然在网络架构上和Fried等人的模型非常相似，但是作者使用了cycle-reconstruction固有奖励来进行RL和SIL的训练，而后者只是用了简单的数据增强。  

最后是Exploration，在改善exploration方面已经进行了许多工作，因为在exploration与exploitation之间的权衡是RL的基本挑战之一，也是博弈论的一个经典议题，其主要内容是平衡探索新事物带来的风险和利用旧事物带来的收益。agent需要利用(exploit)其学到的知识来获得最大的奖励，并探索(explore)新的领域以更好地进行策略搜索。好奇心或不确定性已被用作exploration的信号。最近，Oh等人建议exploit过去的经验来更好地exploration，并从理论上证明其有效性。我们的自监督模仿学习（SIL）方法具有相同的想法。但是，我们并不是在游戏上进行的测试，而是使用SIL在更为实际的VLN任务上验证了其有效性和效率。  

## System  

先来看第一个重要的模块RCM，RCM主要有两个子模块：推理导航器和matching critic。模型接收到自然语言指令的单词序列χ之后，推理导航器会进行一系列行动a1,a2,...,a<sub>T</sub>∈A，并生成了一个路径τ。导航器与环境直接进行互动，并在执行行动动作的时候感知新的视觉场景。为了提高模型的泛化性和强化策略学习，作者介绍了两个奖励函数：由环境提供的外部奖励，通过衡量导航成功的信号和每一个action的导航错误得到；以及一个内部固有的奖励，它来自作者的matching critic，主要衡量语言指令χ和导航器轨迹τ之间的对齐程度。  

### Reasoning navigator  

接下来重点介绍一下这两个子模块。首先看跨模态推理导航器，推理导航器π<sub>θ</sub>是一个基于策略的agent，能够将输入的指令χ映射到一个动作序列A中。在每一个时间步t，导航器都会从环境中接收到一个状态s<sub>t</sub>,并需要将收到的语言指令映射到第一视角下的视觉场景中。因此作者设计了一个跨模态的推理导航器，它能够学习历史轨迹、文本指令和第一视角下的视觉注意力，从而组成了一个多模态的推理路径，从而激励在该时间步下的多模态局部运动。导航器装备了全景相机，能够看到各个视角的图像，导航器主动的将全景图像分成了m个视角下的图像，那么在t时间步下视觉状态的全景特征s<sub>t</sub>可以被表示成{v<sub>t,j</sub>}，v<sub>t,j</sub>代表t时刻下j视角下的图像预训练CNN特征。  

导航器一旦迈出第一步，那么它的视角就开始不断的变化，随着自身位置的不同，导航器的全景视角也会不同。这个时候我们就必须要求导航器需要拥有一定的记忆功能，从而将之前看到的一些视觉信息记住，这里自然而然的会想到LSTM。作者使用一个基于注意力机制的轨迹编码器LSTM：h<sub>t</sub> = LSTM(\[v<sub>t</sub>, a<sub>t-1</sub>], h<sub>t-1</sub>)。作者使用点积注意力机制，具体的公式是v<sub>t</sub> = attention(h<sub>t-1</sub>,{v<sub>t,j</sub>}) = Σsoftmax(h<sub>t-1</sub>W<sub>h</sub>(v<sub>t,j</sub>W<sub>v</sub>)<sup>T</sup>)v<sub>t,j</sub>，Wh和Wv是两个可学习投影矩阵。  

由于上述LSTM注意力机制编码器记录了历史的信息，导航器就可以利用过去的信息来识别当前的状态，并且理解哪些单词或者子指令需要在接下来完成。这就需要我们学习基于历史语境的文本上下文。作者使用一个语言编码器LSTM来将语言指令X编码为一系列语言指令{w<sub>i</sub>}，在每一个时间步都用注意力机制进行计算，公式为c<sub>t</sub><sup>text</sup> = attention(h<sub>t</sub>,{w<sub>i</sub>}<sub>i=1</sub><sup>n</sup>)，当c的值越大，说明历史轨迹和当前的视觉状态越吻合。  

作者还需要动态的理解语言指令来确定接下来看哪里，这和上一段的子任务相似，都是使用注意力机制来完成，不过这里是进行文本条件下的视觉上下文分析，公式和上面的公式非常类似，c<sub>t</sub><sup>visual</sup> = attention(c<sub>t</sub><sup>text</sup>,{v<sub>j</sub>}<sub>j=1</sub><supm</sup>)。  

最后，使用一个动作预测器来将上面生成的历史上下文h<sub>t</sub>，文本上下文c<sub>t</sub><sup>text</sup>和视觉上下文c<sub>t</sub><sup>visual</sup>结合在一起，从而决定接下来朝哪个方向走。该预测器使用双线性点乘来计算每一个可能的方向的具体概率p<sub>k</sub>，具体公式是p<sub>k</sub> = softmax(\[h<sub>t</sub>,c<sub>t</sub><sup>text</sup>,c<sub>t</sub><sup>visual</sup>]W<sub>c</sub>(u<sub>k</sub>W<sub>u</sub>)<sup>T</sup>)，其中u<sub>k</sub>是第k个导航方向的动作embedding，它是通过将一个表现特征向量（从该方向的图像像素块中用CNN提取出的特征向量）和一个4维的方向向量\[sinψ;cosψ;sinw;cosw](ψ和w分别代表相对heading和evaluation角度，即方向和高度)结合在一起实现的。  

### Cross-Modal Matching Critic  

接下来看第二个子模块：跨模态Matching Critic，这一子模块的目的是正确的评价agent路径和指令路径的差异，从而得到一个内部固有的奖励信号，该信号可以表示为R<sub>intr</sub> = V<sub>β</sub>(X,τ) = V<sub>β</sub>(X,π<sub>θ</sub>(X))，而我们要做的就是去测量cycle-reconstruction reward p(X = X\|π<sub>θ</sub>(X))，这其实就是给定导航器自己走的轨迹τ=π<sub>θ</sub>(X)之后重建语言指令X的概率，那么显然的，重建的概率越高，所提供给它的导航器路径越好。这里matching critic自然无法在一开始就能完美的评价，所以作者采取用人类注释的真实指令-轨迹对进行监督学习训练。  




---
本博客支持disqus实时评论功能，如有错误或者建议，欢迎在下方评论区提出，共同探讨。  
