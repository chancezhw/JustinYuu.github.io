---
layout: post
title: "Convolutional Neural Networks Chapter 4"
description: "Notes"
categories: [Convolutional-Neural-Networks]
tags: [Python]
redirect_from:
  - /2019/07/03/
---

# Convolutional Neural Networks Chapter 4  

本周是这一门课的第四周，也是最后一周，这一周主要介绍人脸识别，希望这周的课程能和上周一样干货众多。  

## Face Recognition  
首先是介绍了一下吴大大之前在百度时林元庆大佬实验室做的视频，百度入口并不需要工卡，而是通过人脸识别闸机自动识别。值得注意的是，如果用工卡或者图片的话，并不能识别出来，因此冒名顶替是不存在的，也就是说这个人脸识别系统具有活体检测功能，不过这并不是这一节所研究的。现在人脸识别已经在国内广泛的普及开来，两三年前还是比较新的应用现在已经到处可见，可以看出人脸识别近几年发展的火热。  

吴恩达教授首先介绍了人脸识别和人脸校验的区别。人脸校验只是简单的输出所给定的图片或者一个特定的人是不是所声称的那个人，我们称之为一对一问题，但是人脸识别相对而言复杂的多，因为其要对多个人进行分辨和识别，也就是所谓的一对多问题。举个例子：如果一个校验系统能够达到99%的校验率，看起来还不错，但是如果在人脸识别系统中，有100个人需要识别，那么如果对单个人的校验成功率仍然是99%的话，那么犯错的几率相对于单人校验系统就是100倍，因此对于多人的人脸识别系统，我们所需的正确率会更高。显然，人脸识别比人脸校验要复杂的多，应用也广的多。  

人脸识别的第一个挑战是One-shot learning，即从一个照片中就可以学习到该人的特征，并能从不同的图像中识别出这个人。那么自然而然我们会想到softmax，可是问题在于训练的样本实在是太少了，甚至少到一人只有一张照片，显然作为训练数据还是太少了，训练效果会很差，所以我们需要其他的方法来解决这个问题。解决问题的方式具体而讲，是让算法训练一个“相似性”函数("similarity" function)，用d(img1,img2)作为两张照片之间的差异度，并设置一个阈值τ，如果d<=τ，那么就可以认定这两张图片是一个人。这个方法在我的理解下，抛弃了以前让机器“记住”特征的想法，而是直接训练一个新的算法，从而让机器能够自动分辨出新的人和照片里的人是不是同一个人，也就是说教会了机器“分辨”的能力。那么d函数的实现需要通过一个称作“Siamese network”的网络实现。将人脸图片x1导入ConvNet，不用softmax，而是使用全连接层，得到的向量f(x1)，将其看做输入图片x1的编码（encoding）。在遇到新的图片的时候，运用相同的ConvNet对其进行同样的计算，然后直接对比f(x1)和f(x2)的编码，将差异度的计算转化为编码的计算，具体而言，将编码的L2范数取值定为d函数的取值，之后进行比较，差值越小表示越像同一个人，差值越大表示越不是同一个人。这种孪生架构的网络称之为Siamese network。这个思想来自一篇论文：[Taigman et al., 2014, DeepFace closing the gap to human level performance](https://www.cs.toronto.edu/~ranzato/publications/taigman_cvpr14.pdf)。  

为了达到该目标，我们需要定义一种新的损失函数：Triple Loss，即三元损失。对每一种学习对象，也就是人脸，都要定义三种训练数据：Anchor,Positive,Negative,其中锚照片和正负例照片各组成一组，分别代表同一个人的图片和不同两个人的图片，那么显然训练的目标就是使d(A,P)<=d(A,N),也就是使f(A)-f(P)的二次范数小于f(A)-f(N)的二次范数，或者写成两个L2范数相减小于等于0。但是有种情况是所有的结果都是0的话，等式会恒成立，也就是f(x)=0是这个方程的平凡解。但是显然，使所有的网络输出值全为0会使得网络训练的结果退化，并无实际意义，而这样会使得网络训练的结果有可能向着这一方向发展，因此我们需要设置一个margin，也就是一个很小的正数来规避这种情况，从而使两个d之间必须有一定的差距才可以。那么最后总结一下，每组训练数据都需要A,N,P三个图片，也就是输入数据组成，因此该损失函数称为Triple Loss，损失函数写作L(A,P,N) = max(| |f(A)-f(P)| |²-| |f(A)-f(N)| |²+α,0)。训练的阶段需要用一个人的多张照片进行训练，不过这并不是训练机器记住这个人的面部特征，而是训练one-shot learning对不同人的脸能够有效的区分。在训练的时候，随机的选择APN会使得d(A,P)+α<=d(A,N)很轻易的被满足，这是由于随机选择照片的话，选到相同人的两张照片比不同人的两张照片概率小太多了，这就导致训练的结果鲁棒性不够。所以实际中要选择一些不那么容易，或者比较困难去训练的三元数据进行训练。由于这个领域的数据实在过于庞大，有时候甚至能够数以千万计或者数以亿计，因此吴恩达建议不要自己建立数据集，而是直接用别人的数据集。那么话又说回来，既然用了别人的数据集，也就没有必要训练别人的数据了，直接去下载别人训练好的网络和模型就好了，因此吴恩达最后还是建议直接参考别人的已经训练好的模型，直接应用到自己的项目里就好。  

除了triplet loss，还有一种训练Siamese Network参数的方法：Binary Classification。这个方法对于每个样本都训练两张图片，同样输出到Siamese Network中得到两个编码f(x1),f(x2)，但是不同的是它将两个编码通过sigmoid函数做了一个二分分类。

两种方法的形式不同，但是方式都是一样的，都应用了Siamese Network作为预测方类，如果是同一个人就输出1，不同就输出0。损失函数就和二分分类的激活函数输入的形式一样，也是wx+b，只不过x是两个编码的差值。也可以χ相似度公式，把(f(x1)-f(x2))²/(f(x1)+f(x2))当做x，然后把wx+b作为激活函数的输入，这两种激活函数的输入都是可以的。  式，并且在计算时并不需要缓存图片，只需要保存编码即可，这样降低了计算量，也减轻了存储的压力。  

## Neural Style Transfer  

神经风格转移是很有趣的一个功能，它可以将一种图片的风格转移到另一种图片的风格上。要研究神经风格转移，首先要学会深度CNN每一层都在做什么，吴恩达教授通过可视化的方式来研究这一方面。所谓可视化，就是挑选一个layer，在其中选择一个unit，遍历所有训练样本，选择使该unit的激活函数最大的9块图像区域，使用这9个图像块作为这个unit的可视化，这些区域就是这个unit捕捉的特征。随着层数的不断增加，可以明显的看出浅层网络识别出了图像的边缘、颜色、纹理等简单的特征，而深层网络识别出了具体的人、动物等复杂的特征；同时，浅层网络识别的区域比较小，而深层网络能够识别出更大的区域。  

研究完了每层的作用，我们要确定具体的代价函数了，因为目标图片G需要同时具备风格图片S的风格和内容图片C的内容，因此代价函数需要用两部分组成：内容图片与目标图片的内容代价函数和风格图片与目标图片的风格代价函数，即J(G)=α* J(C,G)+β* J(S,G)，而α和β就是两个需要设置的权重超参数。那么G的具体训练方法如下：首先随机化的生成规定像素的G，然后用梯度下降不断的最小化J(G)，随着J(G)不断的变小，可以实时的看到风格不断的向着风格图片转移的同时，内容不断的向着内容函数转移。  

J(G)是由内容代价函数和风格代价函数组成的，首先来看如何计算内容代价函数。吴恩达建议使用ConvNet的一个隐藏层l来计算内容代价函数，并且尽量使用一个预训练的ConvNet，比如VGG。之后，我们需要测量C和G之间内容的相似程度，通过比较ac和ag，即两个激活函数的取值，我们称之为激活因子的大小，取值越接近，表示内容越接近。具体来讲，J(C,G)的取值为ac和ag差值的L2模的1/2。  

接下来看风格函数，如果我们用第l层的激活值去度量风格，那么我们定义风格为不同通道之间激活函数的相关性(correlation)。我们都知道，每个通道都对应了一个特征，因此不同特征之间的相关性就代表了这几个特定的特征是否会同时出现。如果几个给定的特征，也就是所谓的风格，在一个unit中的nw* nh个grid的不同的通道里始终同时出现的话，说明这几个特征有强相关性，也就是代表该图片有风格图片中的风格。因此，对于风格的度量，不应该去分析风格里具体有什么，只需要分析代表风格里的不同特征的值，是否经常同时出现，也就是说是否存在强相关性即可。那么如何确定呢


---
本博客支持disqus实时评论功能，如有错误或者建议，欢迎在下方评论区提出，共同探讨。  
