---
layout: post
title: "Improving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimization Chapter 1"
description: "Notes"
categories: [Neural-Networks-and-Deep-Learning]
tags: [Python]
redirect_from:
  - /2019/03/11/
---

# Improving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimization Chapter 1  

集中刷了两个星期的题目，水平个人认为有了比较明显的长进，不过还是要继续练习。但是在刷OJ题目的同时，我认为也有必要继续学习一些深度学习的内容。  
deepmind的课程一共有五门课，Neural Networks and Deep Learning是这个系列的第一门课，Improving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimization这门名字巨长无比的课是第二门。这门课程更短，一共有三周，争取用早点把它学习完毕。  
这一篇文章主要是记录第一节的笔记，记录的形式依旧和这个系列的上一门课程形式相同。由于我之前学过吴恩达的Machine-learning这门课程，所以我对于之前已经学过的知识点不会再进行详细的记录，而只是会进行简要的概括便略过。同时，我还会将课后练习的代码放在上面。  

## Setting up your Machine Learning Application    

这一章一共有三节，其中还有几个小节，我就以大节为单位记录笔记，将这一大节的知识点全都记录在这一标题下面。  、
这一节主要介绍如何为自己的机器学习应用做一些准备。首先介绍了训练集、交叉验证集（这个课程中也称之为开发集：dev set）、测试集的比例，一般的比例会在6:2:2，但是在训练规模相当庞大的情况下，训练集的比例也可以扩大。其次讲了可能存在的一些问题，例如用于机器学习的训练集和测试/交叉验证集不匹配的问题。这里吴恩达举了一个例子，例子是判断是不是猫这个问题时，训练集都是网上的一些分辨率高的图片，而测试集和交叉验证集却都是分辨率低的从手机中截取的图片，这就会造成两者性质上的不匹配（高分辨率高质量/低分辨率低质量），这就需要保证测试集和训练集的数据分布相同，从而避免出现这种情况。  

这一章的第二节介绍了方差和偏差的关系，这在machine-learning那门课上已经介绍的极为充分了，所以在这里就简要概括一下即可。  
高偏差指的是不管在训练集还是在测试集上误差都很大的情况，即欠拟合；相反，高方差指的是在训练集上误差很小，但是在测试集上误差很大，即过拟合。我们所需要的训练结果，无疑是低方差低偏差的结果。这里吴恩达教授还介绍了一种理想误差，也叫贝叶斯误差(Bayes error)，指的是理想情况下的最小误差，但这并不一定是现实中能达到的，可以理解为一个误差下限。  

最后，吴大大介绍了一个通用的流程，用来解决方差和偏差问题。首先，判断是否存在高偏差，如果是，尝试换一个更大的神经网络。接着，判断是否是高方差（放到交叉验证集中对比），如果是，那么就需要用一个既能够降低方差，又能够不增加偏差的方式来解决这个问题。这个方法就是**正则化**。  

## Regularing your neural network  

我一直很不解的一点就是，为什么要在这么晚才介绍正则化，要知道在machine-learning那门课程中，几乎是在一开始就引入了正则化，然后在接下来的不同模型中不断的进行正则化。在这门课中，用了一整章的知识来集中介绍强大的正则化及其用法和作用。  

这里把之前课程中学习正则化的笔记贴在这里[Machine-Learning第三章笔记](http://justin-yu.me/blog/2019/01/24/Machine-Learning-by-Andrew-Ng-Chapter-3/)
正则化分为L1正则化和L2正则化，区别主要是范数是1还是2，通常都是用L2正则化。正则化可以应用在逻辑回归、线性回归和神经网络等模型中，其形式基本为(λ/(2* m))* Σ||w||²，其中||w||即为w的范数2或者弗罗贝尼乌斯(F-)范数。  
接下来就是核心问题了：为什么正则化可以过拟合？这是一个在之前就困扰我的问题，并且一直没有完全弄懂。这里贴两篇我觉得不错的文章：[文章1](https://charlesliuyx.github.io/2017/10/03/%E3%80%90%E7%9B%B4%E8%A7%82%E8%AF%A6%E8%A7%A3%E3%80%91%E4%BB%80%E4%B9%88%E6%98%AF%E6%AD%A3%E5%88%99%E5%8C%96/),[文章2](https://www.cnblogs.com/alexanderkun/p/6922428.html)，其中文章1还将范数介绍的很详细，值得一看。  

用我自己的语言总结一下，核心就是，在L2正则化中，λ越大，w越小。而这是因为在w的更新时，w含λ的那一项的系数是负的，也就是说加入λ之后w的更新会减小的更快，即权重衰减。w越小，w进入激活函数后取值将会更靠近原点，这样变化会更小。如果λ非常大，那么w就会非常小，以至于进入激活函数后的值接近于0，这就使得隐藏层作用无限减小直至为0，用数学语言说就是作用相对线性。  

接下来介绍的是dropout正则化。之前的文章2中将L1,L2,dropout都囊括了进去，可以看一看。dropout的方式和其名字一样，将神经网络中的某些隐藏单元随机去掉，然后不断更新得到的权值。这在python中可以通过乘以一个布尔型数组来实现。这里提到了一种叫做inverted dropout的方式，我觉得以我自己的语言去解释，不如直接放一篇足够有说服力的文章来的实在、简单：[Analysis of Dropout-P.Galeone](https://pgaleone.eu/deep-learning/regularization/2017/01/10/anaysis-of-dropout/).

介绍了什么是dropout，接下来该讲为什么dropout管用了。由于每一个隐藏节点都有可能会被dropout,因此最终的权重结果不会依赖任何一个结点。接着设置一个称为“留存率（keep-prop）”的概念，这决定了有多少神经元将会被留下。显然，每层都有自己的留存率，我们也通常对每个不同的层设计不同的留存率。  
dropout在CV领域使用非常频繁，因为在CV领域经常会出现训练集不够导致过拟合的例子，而要记住dropout的作用正是为了避免过拟合，这也是为什么把这项技术归为正则化的原因。  

最后一节介绍了其他的正则化方法，比如将图片扭曲、旋转、缩放、变形等。虽然这看起来只是一些小的trick，但是实际上这是一种减少过拟合的方式，通过对同一种结果的数据进行多种不同形状的训练，从而使训练结果脱离掉对于一些过拟合性质的依赖。例如，旋转图片可以使神经网络摆脱对于角度的依赖，这相当于告诉计算机，不要试图通过判断是否是“正的”来判断最终的结果，从而达到消除过拟合的结果，其他的方式也是同理。  

## Setting up your optimization problem  

这章主要讲的是对数据的一些优化处理，首先就是数据的均一化，将所有数据全部均一化到相同尺寸，可以使不同组的数据更为均匀的分布。  
其次是梯度的爆炸和消失问题，名字听起来很吓人，但其实就是在深层神经网络中，梯度随着不断的递推而指数型增大或者缩小，从而达到很大或者很小的状态。这种状态下很难对结果进行分析和操作，因此我们需要尽量避免这种情况。解决这个问题的方法很多，吴大大举了一个例子，是用tanh函数时的一种初始化方法，叫做Xavier初始化，来自这篇文章[Understanding the difficulty of training deep feedforward neural networks])(http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf),还有类似的方法可以应用在ReLU和sigmoid函数上，其目的都是让初始权重有一个默认值，从而确保在这个默认值下不会出现激活值的饱和或者0的现象。  
接下来介绍了一种叫做梯度检验的方法，用来检查自己的梯度是否正确，方法就是微积分里学的取相邻两点求斜率（……），然后对于损失函数里的每一个θ都进行一次这个操作，从而获得一个向量θapprox，然后与自己得到的梯度θ相比。比较方法是测量两个矢量之差的欧氏距离，再除以两个矢量欧氏距离之和，如果结果小于1e-7，那么就说明得到的梯度是正确的。但是梯度检验只能在debug的时候使用，而不能在训练时用，这样会拖慢程序运行的速度。此外，进行dropout时不要进行梯度检验，因为dropout的时候不断的随机删除节点，所以很难计算J



---
本博客支持disqus实时评论功能，如有错误或者建议，欢迎在下方评论区提出，共同探讨。  
