---
layout: post
title: "Daily Paper 31"
description: "Notes"
categories: [MMML-Self_Supervised]
tags: [Paper]
redirect_from:
  - /2019/11/09/
---

# Daily Paper 31 - Weakly Supervised Representation Learning for Unsynchronized Audio-Visual Events  

## Introduction  

这篇paper是CVPR2018 Workshop上的文章，是由萨克雷和特艺合作的一篇paper，主要利用视频和音频两种模态，实现对视频中事件的分类和定位。整个网络是一个弱监督学习系统，仅仅使用视频级别的事件标注，没有标注任何时间信息，但是这个网络却可以从不同步的音频-视频事件中进行学习，并在一个大规模的弱标注音频事件的影响中得到了当今最好的表现。通过对定位的图像区域和音频片段可视化，可以看出该系统是高效的，特别是在处理嘈杂的情况的时候，因为这时与模态有关的暗示异步出现。  

作者之所以选择弱监督学习的方式是因为标注太麻烦了，因为作者想完成的任务是视频中的事件分类和特征性视听元素定位，那么要进行监督学习的话就要对每一个视频中的确定时间点进行事件标注，并且对每一个事件中的特征性元素进行标注，这将会花费大量的事件，并且还不一定准确，因此作者干脆根据整个视频的特征对视频本身进行分类标注，不对时序信息进行标注，让网络自凭本事学习特征。  

这里举了一个火车的例子来解释异步视听信息，比如在一个视频片段的前半部分，铁轨上没有火车，但是有火车的汽笛声，在视频的后半段没有汽笛声，但是有火车出现，那么这时候视听信息都含有火车的信息，但是并不是在同一个时间步出现的。因此要理解视频的语义，只通过同一个时间步的信息进行训练有可能会错过很多信息，因此作者这里采取异步训练的方式。  

该任务和之前AVC、AVTS等任务的区别在于之前只是简单的探究图像和音频的对应关系，但是这里需要学习用于分别真实世界中物体和对应的视听线索的数据表示。这一任务是之前没有人做过的，这本身也就是这篇paper的创新点。这一任务可以被称为Multiple Instance Learning（MIL），MIL问题可以解决那些从一系列例子中进行标注而不是对单一实例进行标注的任务，也就是说该任务可以从多个实例中寻找信息进行学习。这里多个实例就是视频中出现的多个图像区域和音频片段，我对这个任务的理解是需要把时序前后的信息记忆下来，所以我第一时间想到的是LSTM或者GRU。这里作者采取了不同的策略，首先将视觉和听觉部分分开处理，提取特征后计算他们与各个标签的相关性分数，汇总两个模态的分数后再结合起来进行视频级别的分类，学习到用于事件分类和定位的表示。  

作者的主要贡献如下：提出了一个视频分类和视听源定位的联合训练多模态框架，能够处理异步案例，并在视频分类中获得了当今最好的表现。



---
本博客支持disqus实时评论功能，如有错误或者建议，欢迎在下方评论区提出，共同探讨。  
