---
layout: post
title: "Daily Paper 10"
description: "Notes"
categories: [CV-classic]
tags: [Paper]
redirect_from:
  - /2019/10/17/
---

# Daily Paper 10 - Network in network  

## Introduction  

Network in network也是相当经典的一篇论文了，这是一篇新国大的论文，主要提出了一个新的深度网络结构，称作“Network in network”，可以增强模型在感受野内对局部区域的辨别力。传统的卷积层使用一个线性滤波器后接一个非线性激活函数，而作者构建了一些结构稍复杂的微型神经网络来抽象感受野内部的数据。作者使用多层感知机来实例化微型神经网络，特征图是通过像CNN一样在微型网络上滑动实现的，接着被传到下一层去。通过微型网络增强局部的模型，我们就可以在分类层中利用所有特征图的平均池化层，GAP相比于传统的全连接层来说更容易被解释，同时也更不容易过拟合。NIN在CIFAR-10和CIFAR-100数据集上得到了目前最好的表现，也在SVMH和MNIST上面得到了不错的表现。  

CNN的filter是底层数据块的广义线性模型（GLM），但是作者认为这种抽象程度是低的，抽象是指特征对同种概念的变体而言是不变的，而用一个更强大的非线性函数逼近器来替换GLM可以提高局部模型的抽象能力。作者认为，GLM在样本的隐含概念线性可分的情形下抽象效果很好，举例而言比如这些概念的变体都在GLM分割平面的同一边，而传统的CNN默认了样本的隐含概念是线性可分的，但是问题在于同一概念的数据都是非线性流形的，而通常来说都是通过输入的高维非线性函数来捕捉这些概念，因此GLM在这方面并不能很完美的起到抽象的作用。作者采取了用一种微型网络的方式进行代替，选择多层感知机来实例网络，这个多层感知机既是一个函数逼近器，也是一个通过反向传播训练的神经网络。  

最后作者将这个结构称作mlpconv层，即MLP+Conv，线性卷积层和mlpconv层都从局部感受野映射到了输出的特征向量，但是mlpconv将局部块的输入通过一个由FC层和非线性激活函数组成的MLP映射到了输出的特征向量中。在mlpconv中，MLP在所有局部感受野中共享，而NIN的总体结构就是多个mlpconv层的堆叠，这些堆叠组成了一个新的网络，我们称之为Network in network。  

同时作者并没有采取传统的全连接层进行CNN的分类，而是使用最后一个mlpconv层的特征向量的空间平均值，通过一个全局平均池化层实现以上想法作为类别可信度，然后将得到的结果传入softmax里面，这是由于GAP具有更好的可解释性，它使用微型网络构成的局部网络很好的实现了强化特征图和分类之间关系的功能。此外，全连接层更容易过拟合，并严重依赖dropout进行正则化，而GAP本身就是一个结构化的正则化器，从而从整个结构上就避免了过拟合。在我的理解下，全连接层由于参数实在是过多，导致训练这么多参数很容易过拟合，而GAP本身参数就很少，并且去除了将特征向量展开称为一维向量再重新分类的过程，直接对最后的每个channel都赋予实际的意义，这样正则化就不再是曲线的正则化，而是整体结构上的正则化了。  

作者还用了一节标题为Convolutional Neural Network的secton来详细的介绍为什么CNN以及相关的改进是不完美的，以及他们的方法的related work，我感觉这里有点重复和不重要，就不细讲了。  

## System  



---
本博客支持disqus实时评论功能，如有错误或者建议，欢迎在下方评论区提出，共同探讨。  
