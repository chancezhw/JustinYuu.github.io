---
layout: post
title: "Daily Paper 10"
description: "Notes"
categories: [CV-classic]
tags: [Paper]
redirect_from:
  - /2019/10/17/
---

# Daily Paper 10 - Network in network  

## Introduction  

Network in network也是相当经典的一篇论文了，这是一篇新国大的论文，主要提出了一个新的深度网络结构，称作“Network in network”，可以增强模型在感受野内对局部区域的辨别力。传统的卷积层使用一个线性滤波器后接一个非线性激活函数，而作者构建了一些结构稍复杂的微型神经网络来抽象感受野内部的数据。作者使用多层感知机来实例化微型神经网络，特征图是通过像CNN一样在微型网络上滑动实现的，接着被传到下一层去。通过微型网络增强局部的模型，我们就可以在分类层中利用所有特征图的平均池化层，GAP相比于传统的全连接层来说更容易被解释，同时也更不容易过拟合。NIN在CIFAR-10和CIFAR-100数据集上得到了目前最好的表现，也在SVMH和MNIST上面得到了不错的表现。  

CNN的filter是底层数据块的广义线性模型（GLM），但是作者认为这种抽象程度是低的，抽象是指特征对同种概念的变体而言是不变的，而用一个更强大的非线性函数逼近器来替换GLM可以提高局部模型的抽象能力。作者认为，GLM在样本的隐含概念线性可分的情形下抽象效果很好，举例而言比如这些概念的变体都在GLM分割平面的同一边，而传统的CNN默认了样本的隐含概念是线性可分的，但是问题在于同一概念的数据都是非线性流形的，而通常来说都是通过输入的高维非线性函数来捕捉这些概念，因此GLM在这方面并不能很完美的起到抽象的作用。作者采取了用一种微型网络的方式进行代替，选择多层感知机来实例网络，这个多层感知机既是一个函数逼近器，也是一个通过反向传播训练的神经网络。  

最后作者将这个结构称作mlpconv层，即MLP+Conv，线性卷积层和mlpconv层都从局部感受野映射到了输出的特征向量，但是mlpconv将局部块的输入通过一个由FC层和非线性激活函数组成的MLP映射到了输出的特征向量中。在mlpconv中，MLP在所有局部感受野中共享，而NIN的总体结构就是多个mlpconv层的堆叠，这些堆叠组成了一个新的网络，我们称之为Network in network。  

同时作者并没有采取传统的全连接层进行CNN的分类，而是使用最后一个mlpconv层的特征向量的空间平均值，通过一个全局平均池化层实现以上想法作为类别可信度，然后将得到的结果传入softmax里面，这是由于GAP具有更好的可解释性，它使用微型网络构成的局部网络很好的实现了强化特征图和分类之间关系的功能。此外，全连接层更容易过拟合，并严重依赖dropout进行正则化，而GAP本身就是一个结构化的正则化器，从而从整个结构上就避免了过拟合。在我的理解下，全连接层由于参数实在是过多，导致训练这么多参数很容易过拟合，而GAP本身参数就很少，并且去除了将特征向量展开称为一维向量再重新分类的过程，直接对最后的每个channel都赋予实际的意义，这样正则化就不再是曲线的正则化，而是整体结构上的正则化了。  

## Convolutional Neural Network  

经典的卷积网络，如果使用ReLU作为激活函数，那么公式是这样的：f<sub>i,j,k</sub> = max(w<sub>k</sub><sup>T</sup>x<sub>i,j</sub>,0),(i,j)代表处理特征图像素的索引，x<sub>i,j</sub>代表以位置(i,j)为中心的输入块，k代表channel。  

当隐含概念线性可分时，这个公式可以很好的进行抽象，但是要提取更好的抽象，应该使用输入数据的高度非线性函数，我们可以使用一套filter来弥补，但是使用过多的filter会带来太多的计算负担。我们知道高层的filter映射到原始输入的感受野非常大，它通过结合下层的较低级概念来生成较高级概念，作者希望可以在生成高级概念之前就进行很好的抽象。  

作者拿来进行对比的是maxout网络，在该网络中，特征图的数目通过affine feature map，即未进入ReLU的线性卷积结果来做最大池化，这种线性函数的最大化使其能逼近任何凸函数，因此这个网络能够很好的分离凸函数集内的概念，使得其表现非常出色。但是问题在于，maxout的前提是隐含概念位于输入空间的凸集中，但是这是不一定的，因此会产生意外情况，所以作者想找到一个更为通用的函数逼近器。  

## System  

接下来看NIN的具体结构，其实上面已经说得比较具体了，这里就简要提一下。  

首先来看mlpconv层，这也是整个体系的关键部分，作者在选取函数逼近器模型时有两种选择，一种是Radial basis network，另一种是MLP，作者选择了MLP。原因有2:第一，MLP对神经网络和反向传播这种训练模式更为兼容；第二:MLP本身就是一个深度模型，这也与特征的重新利用相符合。在mlpconv层内部，使用ReLU作为非线性激活函数。  

mlpconv层的公式如下：  
f<sub>i,j,k<sub>1</sub></sub><sup>1</sup> = max(w<sub>k<sub>1</sub></sub><sup>1</sup> <sup>T</sup>x<sub>i,j</sub>+b<sub>k<sub>1</sub></sub>,0)  (1)  
f<sub>i,j,k<sub>n</sub></sub><sup>n</sup> = max(w<sub>k<sub>n</sub></sub><sup>n</sup> <sup>T</sup>x<sub>i,j</sub>+b<sub>k<sub>n</sub></sub>,0)  (2)

接下来的内容是我认为这篇paper最难理解的地方，我先将内容复述一下。从cross channel pooling的角度来说，mlpconv的公式(f<sub>n</sub>=max(wf<sub>n-1</sub>+b,0))是在一个普通的卷积层上的cascaded cross channel parametric pooling（级联交叉频道参数池化），每一个卷积层起到在输入特征图上加权重组的效果，然后导入ReLU函数中，然后在后续的层中一遍一遍的重复上述步骤，级联交叉频道参数池化允许复杂的和可学习的交叉频道信息进行交互。我第一遍看的时候完全不知道他在说些什么，所以慢慢查阅资料进行研究，所谓级联，就是不同对象之间的映射关系，简单来讲两个对象级联，代表一个对象发生某种变化的时候另一个对象也会发生某些变化。之后是交叉通道池化，这个概念在Maxout那篇论文中有解释，具体而言通过该池化方式，可以完成m个输出channel到n个输出channel的映射(m>n)，池化过程一般是最大池化，简单来讲就是跨channel的最大池化。那么回过头来重新看，所谓的cross不是交叉，而应当翻译成跨，那么从跨频道池化的角度来说，该公式是在一个普通的卷积层上进行级联跨频道参数池化，每一个卷积层都在输入的特征图上做加权的特征重组后导入ReLU激活函数中。而我们这个公式出发来看，的确也是这个意思，因为这个公式只是不断的进行线性变换和ReLU，线性变换乘的值就是权重w，因此本身就是加权的特征重组，而级联体现在这个公式是一层一层递进的，后续层所乘的参数直接联系到了所有前面的层中。  

那么继续往下，作者认为跨通道参数池化层也等同于1×1卷积，至于1×1我就明白了，因为我学过inception。事实上应该是这篇paper首先提出的1×1卷积，跨通道参数池化层本身就等同于一个卷积核为1×1的卷积层，因为它是跨通道的，所以相当于乘了一个相同规模的卷积核。

---
本博客支持disqus实时评论功能，如有错误或者建议，欢迎在下方评论区提出，共同探讨。  
