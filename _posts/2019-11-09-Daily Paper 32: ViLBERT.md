---
layout: post
title: "Daily Paper 32：ViLBERT"
description: "Notes"
categories: [MMML-Self_Supervised]
tags: [Paper]
redirect_from:
  - /2019/11/09/
---

# Daily Paper 32 - ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks  

## Introduction  

这篇paper是佐治亚理工、FAIR和俄勒冈州立大学共同完成的，目前还在under review，不过我看发表也只是时间问题。这篇paper主要利用最近很火的BERT来解决跨媒体的视觉-语言问题。为了这篇paper我还专门去看了一下BERT，顺便看了一下之前的Word2vec和ELMo，不过由于之前只学过Word2vec，后面两个还是看的云里雾里，这里是我看的几篇博客里面我个人感觉介绍的比较详细的一篇[博客](https://www.cnblogs.com/rucwxb/p/10277217.html)，不过感觉还是不够详细。  

这里作者提出了一个称为ViLBERT(Visual-Language BERT)的模型，能够学习与任务无关的图像内容和自然语言的联合表示。总体来说，相对于传统的BERT来说变化并不多，只是将BERT的单一模态改成了双流模型，然后分别处理视觉和语言两个模态，然后用一个共同注意力机制的Transformer层将其连接起来。作者将该模型在一个大型自动收集的数据集Conceptual Captions上面通过两个代理任务进行预训练，然后将其转为多个已经建立的视频-语言任务，包括VQA、视频常识推理、指代表达、字幕图像检索等，并且这些任务只需要将作者的基础架构略作修改即可解决。作者发现其模型在上述四个细化任务中均取得了显著的性能提升。作者的工作展示了视觉-语言领域任务解决思路的转变：从把学习视觉和语言之间的grounding视为任务训练的一部分到将视觉grounding视为可预训练和可转移的一种功能，这在我的理解下就是不再根据具体的任务学习特定的视觉和语言模态之间的联系，而是学习一种通用的联系，使其能够通过小幅修改应用到不同的视觉-语言任务中。那么自然而然地，学习到的这个通用grounding必须足够强大才行，所幸BERT可以满足这一要求。  

Visual-Language问题已经源远流长了，目前这一问题有很多下游任务，而对于这些任务的解决方法一般都是pretrain-and-transfer，这种方式效果虽然很好，也能预训练出优秀的视频和语言的表示，但是问题是模型有可能无法很好的将这两个表示连接在一起，从而导致最后性能并不好。这里作者就想学习一种通用的视觉grounding模型，用于学习不同模态之间的内在联系，并能利用它进行后续任务的处理，也就是说作者想要对visual grounding本身进行预训练。  

为了学习到视频和语言的联合表示，作者去寻找了一些比较成功的自监督学习的案例，这些案例都能从一些大规模未标注数据集中捕捉大量的语义和结构化信息，这是因为他们可以训练模型来执行所谓的“代理”任务。这些代理任务能够利用数据内部的结构来自动生成监督任务，比如给灰度图像上色或者句子填空。虽然视觉领域的工作表现不断变好，但是到目前为止，自监督学习的最大影响是通过语言模型，例如ELMo，BERT和GPT，它们在许多NLP任务上得到了高水平结果。也就是说，作者要用在语言模型上的方法来应用到视觉领域中，那么要通过类似的方法学习视觉grounding，首先必须找到合适的数据集，其中视觉和语言之间可以对齐。在这项工作中，作者考虑了最近发布的Conceptual Captions数据集，该数据集包含约330万幅图像，这些图像都有自动从网络上收集的弱相关解释性字幕。  

作者训练了一个从视听数据中学习与任务无关的视觉grounding的联合模型，作者称之为ViLBERT。该模型最大的创新点是用两个不同的stream来处理视频和语言，然后通过一个共同注意力机制的transformer模型来进行交流。作者多次提到了用于训练模型的两个代理任务，具体来说两个代理任务分别是预测遮挡词语和图像区域的语义，以及预测某图片和某文本片段是否相关。之后作者将该模型应用到了四个细化任务中，这在上面已经说过了，最终发现这些任务的性能都提升了2到10不等。  

## System  

首先看一下预训练模型BERT，BERT全称为Bidirectional Encoder Representations from Transformers。BERT将词语序列映射为编码，然后通过若干个编码器风格的transformer块，从而产生最后的表示，而Transformer里面又由multi-head attention和前向神经网络组成，多头注意力机制是自注意力机制的改进版本，主要通过矩阵的权重运算实现。BERT对由词汇词和少量特殊标记（SEP，CLS和MASK）组成的离散标记序列进行操作,对于给定的token，输入表示形式是特定于token学习到的embedding与位置（即序列中token的索引）和句段（即如果存在多个token的句子的索引）的编码的总和。BERT在一个大规模语料库中进行端对端的训练，主要训练两个任务：masked language modelling和next sentence prediction。  

而作者的ViLBERT是BERT在视觉领域的延伸，这里具体来说是联合表示静态图像和相关的解释性文本。作者较为直接的做法是直接对BERT进行微调，将视频输入空间通过聚类进行离散化，生成一个个视频化的token，然后使用BERT进行训练。这种简单粗暴的方法有一大堆问题，首先，初始化聚类可能会导致离散错误，丢失重要的视觉细节特征。其次，对于不同的模态都一样对待，但是由于其模态固有的复杂性和输入表示的抽象初始化不同，需要进行处理的层级也不相同，比如图像区域可能相对于句子中的单词来说相互联系更弱；视频特征的初始化更为复杂，因为要经历更深的神经网络。最后强行让预训练权重去适应大量的额外视觉token有可能损害原本BERT的表现。因此作者不采用上述简单粗暴的办法，而是训练了一个双流模型，使用一小部分基于注意力机制的模块来耦合这两个流，这种方法可以使每个模态都有不同的深度，并且在不同深度的层之间都可以进行跨模态交流。  

作者的ViLBERT模型其实也很好理解，每一个流都是一个BERT模型，每一个流都由transformer模块（TRM）和作者提出的新的co-attentional transformer模块(Co-TRM)组成，两个模态的交流严格通过特定的层进行。实验发现，语言流在和视频流进行交流之前进行的处理显著的多一些，这也证实了作者之前的推论：由于图像经过更为深层的网络进行处理，因此两者并行处理是效果不好的。  

接下来介绍最为重要的层，也是作者最大的创新点：Co-Attentional Transformer Layers。对于视觉流来说，输入的H是视觉信息，QKV中的Q是视觉信息，而K和V都是听觉信息，对于听觉流来说恰好相反。这样由于每一个Co-TRM输入的key和value都是另一个模态的输出，这种交替变换螺旋上升的设计能够同时完成视觉流内部的图像条件限制的语言注意力模型训练和听觉流内部的语言条件限制的图像注意力模型训练。剩余的部分和初始TRM相同，由于保留的初始表示残差的加法，最终可以得到一个多模态feature。  

原始的BERT只对词语进行处理，但是这里还需要提取图像表示。作者通过选择边界框和从一个预训练的目标检测网络中提取边框内部的视频特征来生成视频的区域特征。不同于文本中的单词，视频区域缺少一个自然语序，所以作者使用空间位置编码来进行代替，对区域的位置和图像覆盖的比例建立了一个5维的向量


---
本博客支持disqus实时评论功能，如有错误或者建议，欢迎在下方评论区提出，共同探讨。  
