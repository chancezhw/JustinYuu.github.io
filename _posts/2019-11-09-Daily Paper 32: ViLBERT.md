---
layout: post
title: "Daily Paper 32：ViLBERT"
description: "Notes"
categories: [MMML-Self_Supervised]
tags: [Paper]
redirect_from:
  - /2019/11/09/
---

# Daily Paper 32 - ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks  

## Introduction  

这篇paper是佐治亚理工、FAIR和俄勒冈州立大学共同完成的，目前还在under review，不过我看发表也只是时间问题。这篇paper主要利用最近很火的BERT来解决跨媒体的视觉-语言问题。为了这篇paper我还专门去看了一下BERT，顺便看了一下之前的Word2vec和ELMo，不过由于之前只学过Word2vec，后面两个还是看的云里雾里，这里是我看的几篇博客里面我个人感觉介绍的比较详细的一篇[博客](https://www.cnblogs.com/rucwxb/p/10277217.html)，不过感觉还是不够详细。  

这里作者提出了一个称为ViLBERT(Visual-Language BERT)的模型，能够学习与任务无关的图像内容和自然语言的联合表示。总体来说，相对于传统的BERT来说变化并不多，只是将BERT的单一模态改成了双流模型，然后分别处理视觉和语言两个模态，然后用一个共同注意力机制的Transformer层将其连接起来。作者将该模型在一个大型自动收集的数据集Conceptual Captions上面通过两个代理任务进行预训练，然后将其转为多个已经建立的视频-语言任务，包括VQA、视频常识推理、指代表达、字幕图像检索等，并且这些任务只需要将作者的基础架构略作修改即可解决。作者发现其模型在上述四个细化任务中均取得了显著的性能提升。作者的工作展示了视觉-语言领域任务解决思路的转变：从把学习视觉和语言之间的grounding视为任务训练的一部分到将视觉grounding视为可预训练和可转移的一种功能，这在我的理解下就是不再根据具体的任务学习特定的视觉和语言模态之间的联系，而是学习一种通用的联系，使其能够通过小幅修改应用到不同的视觉-语言任务中。那么自然而然地，学习到的这个通用grounding必须足够强大才行，所幸BERT可以满足这一要求。  

Visual-Language问题已经源远流长了，目前这一问题有很多下游任务，而对于这些任务的解决方法一般都是pretrain-and-transfer，这种方式效果虽然很好，也能预训练出优秀的视频和语言的表示，但是问题是模型有可能无法很好的将这两个表示连接在一起，从而导致最后性能并不好。这里作者就想学习一种通用的视觉grounding模型，用于学习不同模态之间的内在联系，并能利用它进行后续任务的处理，也就是说作者想要对visual grounding本身进行预训练。  

为了学习到视频和语言的联合表示，作者去寻找了一些比较成功的自监督学习的案例，这些案例都能从一些大规模未标注数据集中捕捉大量的语义和结构化信息，这是因为他们可以训练模型来执行所谓的“代理”任务。这些代理任务能够利用数据内部的结构来自动生成监督任务，比如给灰度图像上色或者句子填空。虽然视觉领域的工作表现不断变好，但是到目前为止，自监督学习的最大影响是通过语言模型，例如ELMo，BERT和GPT，它们在许多NLP任务上得到了高水平结果。也就是说，作者要用在语言模型上的方法来应用到视觉领域中，那么要通过类似的方法学习视觉grounding，首先必须找到合适的数据集，其中视觉和语言之间可以对齐。在这项工作中，作者考虑了最近发布的Conceptual Captions数据集，该数据集包含约330万幅图像，这些图像都有自动从网络上收集的弱相关解释性字幕。  

作者训练了一个从视听数据中学习与任务无关的视觉grounding的联合模型，作者称之为ViLBERT。该模型最大的创新点是用两个不同的stream来处理视频和语言，然后通过一个共同注意力机制的transformer模型来进行交流。作者多次提到了用于训练模型的两个代理任务，具体来说两个代理任务分别是预测遮挡词语和图像区域的语义，以及预测某图片和某文本片段是否相关。之后作者将该模型应用到了四个细化任务中，这在上面已经说过了，最终发现这些任务的性能都提升了2到10不等。  

## System  

首先看一下预训练模型BERT，BERT全称为Bidirectional Encoder Representations from Transformers。BERT将词语序列映射为编码，然后通过若干个编码器风格的transformer块，从而产生最后的表示，而Transformer里面又由multi-head attention和前向神经网络组成，多头注意力机制是自注意力机制的改进版本，主要通过矩阵的权重运算实现。BERT对由词汇词和少量特殊标记（SEP，CLS和MASK）组成的离散标记序列进行操作,对于给定的token，输入表示形式是特定于token学习到的embedding与位置（即序列中token的索引）和句段（即如果存在多个token的句子的索引）的编码的总和。BERT在一个大规模语料库中进行端对端的训练，主要训练两个任务：masked language modelling和next sentence prediction。  

而作者的ViLBERT是BERT在视觉领域的延伸，这里具体来说是联合表示静态图像和相关的解释性文本。作者较为直接的做法是直接对BERT进行微调，将视频输入空间通过聚类进行离散化，生成一个个视频化的token，然后使用BERT进行训练。这种简单粗暴的方法有一大堆问题，首先，初始化聚类可能会导致离散错误，丢失重要的视觉细节特征。其次，对于不同的模态都一样对待，但是由于其模态固有的复杂性和输入表示的抽象初始化不同，需要进行处理的层级也不相同，比如图像区域可能相对于句子中的单词来说相互联系更弱；视频特征的初始化更为复杂，因为要经历更深的神经网络。最后强行让预训练权重去适应大量的额外视觉token有可能损害原本BERT的表现。因此作者不采用上述简单粗暴的办法，而是训练了一个双流模型，使用一小部分基于注意力机制的模块来耦合这两个流，这种方法可以使每个模态都有不同的深度，并且在不同深度的层之间都可以进行跨模态交流。  

作者的ViLBERT模型其实也很好理解，每一个流都是一个BERT模型，每一个流都由transformer模块（TRM）和作者提出的新的co-attentional transformer模块(Co-TRM)组成，两个模态的交流严格通过特定的层进行。实验发现，语言流在和视频流进行交流之前进行的处理显著的多一些，这也证实了作者之前的推论：由于图像经过更为深层的网络进行处理，因此两者并行处理是效果不好的。  

接下来介绍最为重要的层，也是作者最大的创新点：Co-Attentional Transformer Layers。对于视觉流来说，输入的H是视觉信息，QKV中的Q是视觉信息，而K和V都是听觉信息，对于听觉流来说恰好相反。这样由于每一个Co-TRM输入的key和value都是另一个模态的输出，这种交替变换螺旋上升的设计能够同时完成视觉流内部的图像条件限制的语言注意力模型训练和听觉流内部的语言条件限制的图像注意力模型训练。剩余的部分和初始TRM相同，由于保留的初始表示残差的加法，最终可以得到一个多模态feature。  

原始的BERT只对词语进行处理，但是这里还需要提取图像表示。作者通过选择边界框和从一个预训练的目标检测网络中提取边框内部的视频特征来生成视频的区域特征。不同于文本中的单词，视频区域缺少一个自然语序，所以作者使用空间位置编码来进行代替，对区域的位置和图像覆盖的比例建立了一个5维的向量。  

作者之前提到过有两个代理任务需要进行预训练，即masked multi-modal modelling和multi-modal alignment prediction。第一个任务类似BERT中的masked language modelling任务，随机mask15%的词语和图片区域，然后使用剩下的输入重建mask的部分。对于遮挡住的文本，作者就用在BERT里的处理方法进行处理，该模型并不会直接返回遮挡的特征值，而是直接预测一个对应图像区域语义类的分布，和从目标检测算法处理相同区域得到的分布进行对比，用KL散度比较两个分布，最小化KL散度进行训练。这种方式训练出来的模型只能够识别视觉内容中的高级别语义，并不能够重建完全相同的视频特征。第二个任务是一个经典 的AVC任务，通过ViLBERT计算两个模态的最终表示，然后将两个表示计算内积，通过一个二元分类层来判断是否对齐。  

## Experiments  

作者使用Conceptual Captions数据集进行ViLBERT训练，使用3.1million个带caption的图片进行训练。使用在BookCorpus和English Wikipedia预训练的经典BERT语言模型来初始化ViLBERT的语言流，模型里有12个Transformer。使用在Visual Genome上进行预训练的Faster R-CNN模型进行区域特征的提取。  

预训练ViLBERT模型只是开始，最终的目的还是将其应用到下游任务上，对于每一个下游任务都分别进行调优，下面分别来看看。  

首先是VQA，作者在图片表示和文本表示逐元素相乘的上方加了一个双层MLP，接着将VQA看做是多标签分类任务：将每一个回答和10个人类回答的相关程度打分，使用二元交叉熵损失训练这个分数，使用Adam优化器进行优化。  

接着是Visual Commonsense Reasoning(VCR)，VCR任务需要回答问题和回答的理由，这两个子任务都可以看做多元选择性任务。作者将问题和每个可能的回答组成了四个不同的文本input，和图片一起传入ViLBERT中，在后元素乘积表示的上方学习了一个线性层，为每一个pair预测一个分数，使用softmax对这四个分数进行最终的预测，使用交叉熵损失进行训练。  

Grounding Referring Expressions任务需要根据给定的自然语言参考定位图像区域，作者在RefCOCO+数据集上进行训练和评估。对于这个任务的一般性方法是根据给定的参考表达重新排序一系列图像区域提案，所以作者使用基于Mask R-CNN的算法得到边界框提案，将每一个图像区域的最终表示传入到一个学习完毕的线性层中，得到最终的matching score。作者通过计算与真实边界框的IoU来毒地每一个提案边框进行标注，阈值为0.5，作者用交叉熵损失函数进行训练，最终选取分数最高的区域作为预测值。  

最后一个是带字幕的图像检索，作者用Flickr30k数据集进行训练和评估，使用一个四元组进行训练，具体方法是随机的采样三个干扰对，和正确的image-caption对组合在一起。计算每一组的对齐分数，应用softmax和交叉熵函数进行训练。此外作者还进行了该任务的零样本学习，直接将预训练的ViLBERT应用到该任务中，从而探究预训练模型是否已经有能力对文本进行基础处理，以及是否能够在无调优的情况下泛化到不同的视觉和语言状况下。  

作者使用两个Baseline作为ViLBERT的baseline:单流BERT和未进行代理任务训练的ViLBERT。对于四个下游任务，作者使用了当今的STOA模型作为baseline：DFAF for VQA,MattNet for RefCOCO+,SCAN for caption-based image retrieval。  

最终的结果显示ViLBERT战胜了所有的baseline……BERT的确够顶，不服不行。  

此外作者还研究了一些因素对结果的影响，首先是视频流的深度，最终发现VQA和图像查找任务适合更深的网络，而零样本图像检索亦是如此，但是VCR和RefCOCO+看起来更适合浅层网络。其次作者研究了一下大规模训练集带来的好处，作者选择了25%规模的数据集和50%规模的数据集进行了对比，结果显示随着数据集的不断增大，最终的表现不断提升，代表ViLBERT更适合大数据集。  

最后，作者提出了一个问题：ViLBERT究竟在预训练的时候学到了什么？这也是我比较困惑的问题，为什么训练两个代理任务能够提升在后续不同任务的表现？这里作者将零样本caption-based检索任务和正常的caption-based检索任务表现进行了对比，结果显示预训练的ViLBERT能够更好的学习视频和语言之间的语义意义丰富的对齐。作者还对预训练好的模型进行了定性分析，输入图片，采样图片条件下的文本，这其实就是一种image captioning，结果显示虽然没有调优导致表现一般，但是仍然能够看出预训练模型学到了什么，结果显示很多图片产生了描述图片内容的文本，但是由于字幕是从网络图像替换文本中产生的，其中的许多字幕已经过编辑，并包含对非视觉概念的引用。  

## Conclusion  

总结一下，这篇paper提出了一个新的视听联合监督训练模型ViLBERT，通过借鉴BERT的思想，用一个双流网络成功的学习到了图像和语言的通用表示，并能够很好的通过调优应用到各种不同的下游任务中，在作者进行的四个下游任务实验中均取得了STOA表现。作者坚信BERT仍然能应用到别的视听多模态任务中，期待后续的研究结果。  

---
本博客支持disqus实时评论功能，如有错误或者建议，欢迎在下方评论区提出，共同探讨。  
