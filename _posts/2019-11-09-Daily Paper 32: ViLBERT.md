---
layout: post
title: "Daily Paper 32：ViLBERT"
description: "Notes"
categories: [MMML-Self_Supervised]
tags: [Paper]
redirect_from:
  - /2019/11/09/
---

# Daily Paper 32 - ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks  

## Introduction  

这篇paper是佐治亚理工、FAIR和俄勒冈州立大学共同完成的，目前还在under review，不过我看发表也只是时间问题。这篇paper主要利用最近很火的BERT来解决跨媒体的视觉-语言问题。为了这篇paper我还专门去看了一下BERT，顺便看了一下之前的Word2vec和ELMo，不过由于之前只学过Word2vec，后面两个还是看的云里雾里，这里是我看的几篇博客里面我个人感觉介绍的比较详细的一篇[博客](https://www.cnblogs.com/rucwxb/p/10277217.html)，不过感觉还是不够详细。  

这里作者提出了一个称为ViLBERT(short for Visual-Language BERT)的模型，能够学习与任务无关的图像内容和自然语言的联合表示。总体来说，相对于传统的BERT来说变化并不多，只是将BERT的单一模态改成了双流模型，然后分别处理视觉和语言两个模态，然后用一个共同注意力机制的Transformer层将其连接起来。作者将该模型在一个大型自动收集的数据集Conceptual Captions上面通过两个代理任务进行预训练，然后将其转为多个已经建立的视频-语言任务，包括VQA、视频常识推理、指代表达、字幕图像检索等，并且这些任务只需要将作者的基础架构略作修改即可解决。作者发现其模型在上述四个细化任务中均取得了显著的性能提升。作者的工作展示了视觉-语言领域任务解决思路的转变：从把学习视觉和语言之间的grounding视为任务训练的一部分到将视觉grounding视为可预训练和可转移的一种功能，这在我的理解下就是不再根据具体的任务学习特定的视觉和语言模态之间的联系，而是学习一种通用的联系，使其能够通过小幅修改应用到不同的视觉-语言任务中。那么自然而然地，学习到的这个通用grounding必须足够强大才行，所幸BERT可以满足这一要求。  

Visual-Language问题已经源远流长了，目前这一问题有很多下游任务，而对于这些任务的解决方法一般都是pretrain-and-transfer，这种方式效果虽然很好，也能预训练出优秀的视频和语言的表示，但是问题是模型有可能无法很好的将这两个表示连接在一起，从而导致最后性能并不好。这里作者就想学习一种通用的视觉grounding模型，用于学习不同模态之间的内在联系，并能利用它进行后续任务的处理，也就是说作者想要对visual grounding本身进行预训练。  

为了学习到视频和语言的联合表示，作者去寻找了一些比较成功的自监督学习的案例，这些案例都能从一些大规模未标注数据集中捕捉大量的语义和结构化信息，这是因为他们可以训练模型来执行所谓的“代理”任务。这些代理任务能够利用数据内部的结构来自动生成监督任务，比如给灰度图像上色或者句子填空。虽然视觉领域的工作表现不断变好，但是到目前为止，自监督学习的最大影响是通过语言模型，例如ELMo，BERT和GPT，它们在许多NLP任务上得到了高水平结果。也就是说，作者要用在语言模型上的方法来应用到视觉领域中，那么要通过类似的方法学习视觉grounding，首先必须找到合适的数据集，其中视觉和语言之间可以对齐。 在这项工作中，作者考虑了最近发布的Conceptual Captions数据集，该数据集包含约330万幅图像，这些图像都有自动从网络上收集的弱相关解释性字幕。  

---
本博客支持disqus实时评论功能，如有错误或者建议，欢迎在下方评论区提出，共同探讨。  
