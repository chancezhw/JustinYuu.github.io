---
layout: post
title: "Daily Paper 20"
description: "Notes"
categories: [MMML-VLN]
tags: [Paper]
redirect_from:
  - /2019/10/28/
---

# Daily Paper 20 - Embodied Question Answering  

## Introduction  

因为昨天有点事情耽误了学习没看paper，所以今天加个班把它补上。这篇paper是佐治亚理工和FAIR发表在CVPR18上的，主要提出了一个新的task：Embodied QA，主要内容是给出一个具体的3D真实环境，让agent能够用其第一人称视角自行熟悉周围环境后，回答人类根据该环境提出的问题。Emobodied QA是一个经典的多模态机器学习任务，模型需要学习一系列AI技能，比如动态感知、语言理解、目标导航、常识推理和语言到行动的grounding，作者在这篇论文中提出了一个端到端的强化学习模型，并提出了EmbodiedQA这一任务的评价方式和指标。这类问题类似于视觉语言导航VLN问题，因此把这篇paper放在VLN领域一起看一下。  

作者的长期目标是使agent可以感知周围环境、交流和完成指令。对于这部分功能来说，任何微小的进步都会对人类世界产生很大的帮助。那么为了推动长期目标的发展，作者提出了一个这个大目标中的一个小任务——Embodied QA，并提出了这个问题的配套环境、评价指标和解决这个问题的强化学习模型。在作者的模型中，agent可以执行以下命令：朝左/右转，向前/后/左/右行进，这些动作的目的都是为了获取周围环境的信息，从而回答人类提出的问题。那么一个Embodied QA机器一定要有的功能是对自然语言和视觉信息的理解，除此之外还需要有以下几种功能：1.行动感知，agent必须学会将它得到的视觉信息转化成基于它对周围环境的感知而决定的正确动作。2.常识推理，由于agent并没有收到一个整体的鸟瞰图，所以agent需要在自己的主视角下像人类一样对周围的环境进行推理。3.language grouding，在VLN领域的paper中grounding这个词随处可见，不过我一直不知道如何完美的翻译，我的理解是一种“映射”，将自然语言映射为机器的某种理解，比如物体的视觉信息，空间的地理物质信息等。这里作者指的主要是将语言中的文本词汇与视觉物体形状相对应，这一问题也是语言-视觉问题中的一大难题，特别是在泛化环境中的表现非常差，使得整个模型的应用程度不高。在该模型中，语言信息grounding的并不是具体的图像，而是一个由动作组成的序列。  

从强化学习的角度理解，Emobodied QA是一个非常具有挑战性的学习问题，由于问题可能包含多种信息，而整个环境可能非常复杂，正确的结果又只有一个，那么完全正确的处理信息并得出结论是一个相当困难和复杂的任务。那么对于这一个复杂的任务，作者首先将要做的事情分类，将其分为环境、问题种类、学习范式，从而将这个稀疏的RL奖励与模仿学习和奖励塑造结合在一起。  

作者采用机器人和深度强化学习所使用的范式，在训练的时候给出所有信息，包括agent的位置、深度和环境的语义注释，并允许对无障碍物最短路径的计算。在测试的时候，agent完全按照主视角的视觉信息进行环境信息的获取，并不会得到训练时候的得到的任何辅助信息。agent的整个模块都是端到端进行训练的，从原始的视觉信息和语言信息，到目标导向的室内导航、再到回答提出的视觉问题。  

作者最后总结了一下他们的贡献，主要有：提出了Embodied QA这一新的问题；介绍了一种新的用于获取环境信息的Adaptive Computation Time导航；通过模仿学习初始化了agent并用强化学习方法进行调优，并提高了模型的泛化能力；在House3D场景下评价了模型的表现，提供了基于House3D的EQA数据集；还将House3D渲染器与Amazon Mechanical Turk（AMT）集成在一起，使测试者可以远程操作agent，并收集了基于问题的导航的专家演示，可以作为比较我们建议的算法和未来算法的benchmark。  

## System  

作者用了一页的篇幅介绍related works，由于这是一个新方向，所以相关工作都来自于不同的方向，就不复述了。此外作者还用大量的篇幅介绍他们的EQA数据集，这里我也不做过多的介绍，重点还是看作者使用的模型和实验结果。  



---
本博客支持disqus实时评论功能，如有错误或者建议，欢迎在下方评论区提出，共同探讨。  
