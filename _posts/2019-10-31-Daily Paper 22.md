---
layout: post
title: "Daily Paper 22"
description: "Notes"
categories: [MMML-Self Supervised]
tags: [Paper]
redirect_from:
  - /2019/10/31/
---

# Daily Paper 22 - Audio-Visual Scene Analysis with Self-Supervised Multisensory Features  

## Introduction  

今天的这篇paper是UCB在ECCV18上的paper，也是一篇自监督的视听场景分析的paper。作者使用一个融合的多场景表示来将一段视频的影像和音频联合建模，用自监督学习的方式来预测一段影像和一段音频是否是对齐的，其实也就是昨天看的AVTS问题。作者使用该训练好的模型进行了三个下游任务的应用，分别是1.音频源定位。2.视听动作识别。3.开/关音频源分离，比如在一段外文演讲中分离出翻译的音频。  

本文的主要贡献是：1.学习了一个融合了音频和视频信息的通用视频表示方式。2.定性（声源可视化）和定量（动作识别）的评价了该表示的有效性。3.提出一种新颖的当今唯一能够应用在真实场景下的视频条件源分离方法，该方法使用作者学习的表示来分离屏幕上和屏幕外的声音。  

## AVTS  

因为作者喜欢将模型和实验结果放在一起，所以这次的小标题以研究的部分为分类依据。首先来看作者训练的AVTS表示，作者同样使用了对半的正负样本来进行训练，损失函数使用的最大对数似然，使用的卷积网络是全三维CNN，在网络的前半部分分为两部分，分别处理音频和影像模态，每个部分还是包括若干卷积层和池化层，具体来说在fusion之前，对影像进行一些卷积的预训练以降维，对音频进行1×1卷积使得其采样率与影像层相等。fusion的部分类似ResNet-18，使用了残差块进行深层网络处理。  

这里对比一下nips那篇和这篇对AVTS网络架构的设计异同，简单来讲最大的区别在于nips那篇使用的是使用类Siamese网络分别训练最后用对比度误差联合在一起的训练方式，而这篇使用的是残差网络共同训练，只是在训练之前对两部分做了一些小的处理。此外，nips使用的是图像3维CNN+音频2维CNN，而这里使用的是全3维CNN卷积。那么相应的，损失函数的种类也有所不同，这里使用的是最大对数似然，因为是共同训练的结果，而nips那篇使用的是对比度损失，以此将两个不同网络连接起来。  

最终的模型对于对齐任务的预测成功率在59.9%，作者对人也测了测，成功率在66.6%，不过不管怎么圆，预测准确率还是有点低的，相较之下，nips那篇采取课程学习达到的78.4%准确率就相当高了，不过由于训练集和评价标准可能有所不同，强行横向比较也不太合理。  

## Visualizing the locations of sound sources  

作者拟通过视觉化音源定位来进一步评价其表示的性能，这里音源定位主要是来定位图像中的哪一个动作最有可能引发该声音，听起来还是比较难的。作者使用了class activation map(CAM)来进行音源定位。该方法非常简单，将视频中的一部分像素和音频的特征联合提取，乘以网络最后一层的权重后导入sigmoid，用sigmoid来进行01判断此声音是否与该部分像素有关。  

接下来进行具体的可视化，首先将上述公式得出的top-ranked像素块列出来，在原始图像中以类似热度图的形式展现出来，数据集主要是演讲视频，结果显示大部分top-ranked区域都在说话人的脸和嘴附近。接下来作者使用一些非演讲视频的例子，发声的部位也不再是人脸了，结果显示还是可以比较好的定位。最后，作者探究模型的注意力如何随运动而变化，作者计算了基于CAM的视频可视化，结果定性地表明模型的注意力随着屏幕上的运动而变化，这与单帧方法模型形成了对比，后者主要关注发出声音的对象本身而不是其动作。  

## Action Recognition  

作者在UCF-101上对其模型进行了调优，结果显示无标签训练准确率能够达到82.1%，远远超过了其他自监督学习方法的准确率。其次作者将其结果与在有标签标注数据集上训练的准确率结果进行比较，结果显示还是有很大差距的，监督学习方法的准确率能够达到94.5%。  

作者还分别探究了两个模块对整体性能的影响，结果显示，将音频部分去掉之后准确率会下降5%，表明音频部分也是很重要的。此外，作者还将该预训练模型和一个随机初始化网络进行了对比，结果显示性能准确率的差距会有14%，表明作者研究的新型表示还是很重要的。  

总体来看，作者的模型对于视频单模态和视听多模态的动作识别都是很有效的。  

## On/Off-screen audio-visual source separation  

作者提出了另一个下游任务，即视频内/外音频分离，这其实是一个相当经典的机器学习问题了，我记得当时最开始接触ML时上吴恩达的ML课，第一节课就介绍了这个任务。作者采用了主流的模型，将原有的带音频的视频合成另一端随机音轨，然后让机器将添加的音轨分离，模型的输入采用频谱图的形式，作者将其网络架构与u-net encoder-decoder结合在一起作为该任务的训练框架。  

训练细节就不介绍了，作者主要的对比方式是将其模型与三个模型对比：不使用作者设计的视听表示模型、传统的仅音频模态的分离模型、其他的视听模型。首先是对多感官特征进行替换和隔离，以探究其具体的影响。作者首先将其用I3D网络的特征替换，结果显示表现明显变差，作者认为这是由于作者提出的任务需要大规模的动作分析所致。作者接下来探究模型的优秀表现究竟有多少来自于动作特征，方法是隔离其他变量，比如说话人的性别，结果显示同性别的判别率明显变差，说明其他变量的影响还是挺重要的。作者还研究了早期视听融合是否有用，毕竟网络也融合了频谱图编码器 - 解码器中的模态。为了测试这一点，作者消除了多感官网络的音频流并重新训练了分离模型，模型获得了更差的性能，表明融合的音频即使在其他地方可用时也是有用的。  

作者还探究了该模型的纯音频分离性能，结果在没有视频的前提下，使用PIT损失进行训练，作者的模型几乎优于所有对比模型，除仅在一个指标输给u-net PIT，其他都是胜出。最后作者测了视听分离模型的性能，结果显示作者的模型优于其他所有视听模态语音分离模型。  

总之，该模型效果还是比较好的，并且作者也说明了好的效果来自于作者的跨模态表示，总体来看能够定量的量化模型的优越性，此外作者还通过举例定性的展示其分离效果。  

## Conclusion  

总结一下，作者主要训练了一个AVTS模型，并获得比较不错的识别率，此外作者还研究了该模型在三个下游任务的表现，结果显示表现都比较出色，并用对照实验确定了该模型对于这三个下游任务的有效性。  

---
本博客支持disqus实时评论功能，如有错误或者建议，欢迎在下方评论区提出，共同探讨。  
