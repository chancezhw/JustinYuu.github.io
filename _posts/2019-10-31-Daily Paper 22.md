---
layout: post
title: "Daily Paper 22"
description: "Notes"
categories: [MMML-Self Supervised]
tags: [Paper]
redirect_from:
  - /2019/10/31/
---

# Daily Paper 22 - Audio-Visual Scene Analysis with Self-Supervised Multisensory Features  

## Introduction  

今天的这篇paper是UCB在ECCV18上的paper，也是一篇自监督的视听场景分析的paper。作者使用一个融合的多场景表示来将一段视频的影像和音频联合建模，用自监督学习的方式来预测一段影像和一段音频是否是对齐的，其实也就是昨天看的AVTS问题。作者使用该训练好的模型进行了三个下游任务的应用，分别是1.音频源定位。2.视听动作识别。3.开/关音频源分离，比如在一段外文演讲中分离出翻译的音频。  

本文的主要贡献是：1.学习了一个融合了音频和视频信息的通用视频表示方式。2.定性（声源可视化）和定量（动作识别）的评价了该表示的有效性。3.提出一种新颖的当今唯一能够应用在真实场景下的视频条件源分离方法，该方法使用作者学习的表示来分离屏幕上和屏幕外的声音。  

## AVTS  

因为作者喜欢将模型和实验结果放在一起，所以这次的小标题以研究的部分为分类依据。首先来看作者训练的AVTS表示，作者同样使用了对半的正负样本来进行训练，损失函数使用的最大对数似然，使用的卷积网络是全三维CNN，在网络的前半部分分为两部分，分别处理音频和影像模态，每个部分还是包括若干卷积层和池化层，具体来说在fusion之前，对影像进行一些卷积的预训练以降维，对音频进行1×1卷积使得其采样率与影像层相等。fusion的部分类似ResNet-18，使用了残差块进行深层网络处理。  

这里对比一下nips那篇和这篇对AVTS网络架构的设计异同，简单来讲最大的区别在于nips那篇使用的是使用类Siamese网络分别训练最后用对比度误差联合在一起的训练方式，而这篇使用的是残差网络共同训练，只是在训练之前对两部分做了一些小的处理。此外，nips使用的是图像3维CNN+音频2维CNN，而这里使用的是全3维CNN卷积。那么相应的，损失函数的种类也有所不同，这里使用的是最大对数似然，因为是共同训练的结果，而nips那篇使用的是对比度损失，以此将两个不同网络连接起来。  

最终的模型对于对齐任务的预测成功率在59.9%，作者对人也测了测，成功率在66.6%，不过不管怎么圆，预测准确率还是有点低的，相较之下，nips那篇采取课程学习达到的78.4%准确率就相当高了，不过由于训练集和评价标准可能有所不同，强行横向比较也不太合理。  

## Visualizing the locations of sound sources  

作者拟通过视觉化音源定位来进一步评价其表示的性能，这里音源定位主要是来定位图像中的哪一个动作最有可能引发该声音，听起来还是比较难的。作者使用了class activation map(CAM)来进行音源定位。该方法非常简单，将视频中的一部分像素和音频的特征联合提取，乘以网络最后一层的权重后导入sigmoid，用sigmoid来进行01判断此声音是否与该部分像素有关。  


---
本博客支持disqus实时评论功能，如有错误或者建议，欢迎在下方评论区提出，共同探讨。  
