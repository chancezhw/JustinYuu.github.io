---
layout: post
title: "Daily Paper 12"
description: "Notes"
categories: [CV-classic]
tags: [Paper]
redirect_from:
  - /2019/10/19/
---

# Daily Paper 12 - Very deep convolutional networks for large-scale image recognition  

## Introduction  

这周继续将剩下的几篇paper看完。今天看的是牛津VGG组里的一篇相当经典的文章。这篇文章研究了卷积神经网络的深度对其在大规模图像识别集上的准确率的影响。作者主要的贡献是使用了一个3×3的小卷积核建立了一个CNN架构，对网络深度的增加进行了一个较为彻底的评估，结果显示可以通过增加网络深度来很明显的提高现有的技术配置。作者设计了一个16-19层的网络，并在2014年的ImageNet Challenge定位和分类大赛中分获第一名和第二名。此外该模型的泛化性能也很优秀，作者现将其表现最好的两个网络VGG16和VGG19公开，从而方便其他科研工作的进行。  

之前介绍过的AlexNet成为了众多CNN的基本骨架，也有很多对于AlexNet的改进工作，比如Zeiler&Fergus和Sermanet等人的经典改进，这里VGG网络对其的改进是增加其深度，通过递增的增加卷积层来增加其总深度，并使用小卷积核来降低计算复杂度。  

## System  

首先来看一下网络的架构，在进入网络之前，作者将图像的RGB值减去了整个数据集的RGB平均值，这也是唯一的预处理过程。所有卷积层的filter都是3×3，这也是filter的最小尺寸了，不过在某些实验中作者也使用了额外的1×1卷积，这可以看做是channel的一次线性变换。卷积的步长设置为1，采用spatial padding，padding值为1，一共有五个最大池化层，池化步长为2。所有的隐藏层都采用ReLU作为非线性层，整个网络并不使用LRN归一化，原因是因为作者实验得出这种均一化对ILSVRC数据集上的表现提升并无帮助。  

这里作者建立了五个不同的模型，分别标号A-E，进行实验和对比，A组有11层（8conv+3FC），B组有13层（10conv+3FC），C组有16层（13conv+3FC），D组也有16层（13conv+3FC，不过将C组一维卷积conv1的位置换成了conv3），E组有19层（16conv+3FC），此外还有一组A组的对照组A-LRN，探究LRN对结果的影响。这个结构的对比图也就是广为流传的VGG网络的结构图，还是很经典的。  

此外，这个结构中比较新颖的地方在于用多个小的卷积层来代替了一个大的卷积层，在AlexNet中，网络的卷积层filter尺寸是11×11，步长为4，而这里的只是3×3，不过这里的网络架构中，卷积层基本上是以2为单位进行堆叠，这就导致两个3×3卷积层的感受野等同于一个5×5的卷积层的感受野，三个3×3的感受野等同于一个7×7的感受野，此外由于用了3个卷积层，那么相比于1个7×7网络而言，就多了两个非线性激活函数，从而增加了网络的非线性程度，此外3个3×3的卷积层相较于1个7×7卷积层，参数少了81%，减少了训练的复杂度。这个知识在cs231n的笔记中学到过，这是那门课的笔记[CS231N-CNN Architectures](http://justin-yu.me/blog/2019/08/08/CS231n-Chapter-9-CNN-Architectures/)，当时Serena介绍了几乎所有的经典网络，当时听得云里雾里，看完这些论文之后再看一遍基本上都能看懂了。1×1卷积首先被应用在NIN模型中，前几天也分析过了，这里作者也应用了NIN，从而在不改变维度的前提下增加了其非线性程度（一个ReLU）。  

## Experiment  

模型的训练细节就不多说了，简单点讲，训练使用了小批量随机梯度下降，batch是256，使用了momentum动量优化，momentum参数为0.9，训练过程中使用权重衰减，前两个FC层使用dropout，参数为0.5，学习率初始值为0.01，当验证集准确率不增长的的时候除以10，一共除了三次，在37次迭代后停止训练，训练的速度还是很快的。  

网络权重的初始化也是很重要的，初始化的不好有可能会导致梯度的不稳定，这里作者随机初始化A组的权重，然后用A组的权重来初始化其他深层的对应部分，中间层随机初始化。  

此外还进行了多尺度训练Multi-scale Training，多尺度训练的意义在于图片中的物体的尺度有变化，多尺度可以更好的识别物体。有两种方法进行多尺度训练。第一种方法是在不同的尺度下，训练多个分类器，参数为S，参数的意义就是在做原始图片上的缩放时的短边长度。论文中训练了S=256和S=384两个分类器，其中S=384的分类器的参数使用S=256的参数进行初始化，且将步长调为10e-3。另一种方法是直接训练一个分类器，每次数据输入时，每张图片被重新缩放，缩放的短边S随机从[min, max]中选择，本文中使用区间[256,384]，网络参数初始化时使用S=384时的参数。  

文章还介绍了测试的方法，这里就不详细讲了。实现的细节是使用了4个GPU进行并行训练，得到子batch的梯度后，以平均值作为整个batch的梯度，4个GPU可以加速3.75倍。  

测试的数据集采用ILSVRC-2012，使用top-1和top-5误差作为评价指标，作者进行了单尺度和多尺度的测试。  

### Single Scale Evaluation  

单尺度测试首先表明，使用局部响应标准化(A-LRN网络)并没有提高不使用任何标准化层的模型A的性能。因此我们不在更深的结构(B–E)中使用标准化操作。其次，作者发现分类的错误率随着卷积网络深度的增加而降低了，含有3个1×1卷积层的C组比整个网络都使用3×3卷积层的D组表现要差，但是C组比B组的表现还是要好，代表增加非线性程度仍然会提升性能。作者也对网络B和一个使用5个5×5卷积层的浅层网络进行比较(通过将B中的3×3卷积层对替换成单独的5×5卷积层，它们具有相同的感受野)。在top-1错误率上，浅层网络比网络B要高出7%(在中心裁切图像上)，证明了使用小滤波器的深层网络比使用大滤波器的浅层网络性能更好。在训练阶段使用尺寸抖动(S∈[256;512])比图像最小边使用固定尺寸(S=256 or S=384) 的结果要好得多。这表明了在训练集上通过尺寸抖动的数据增强对于获取多尺度图像统计信息确实是有帮助的。  

### Multi-scale Evaluation  

多尺度测试的结果表明，使用尺寸抖动的模型在测试阶段获得了更好的性能(与使用单一尺度的相同模型相比)。与单尺度测试那一组一样，最深的配置(D和E)表现最好，同时训练阶段使用尺寸抖动比最短边S使用固定尺寸的效果更好。我们最好的单一模型在验证集上的top-1/top-5错误率为24.8%/7.5%。在测试集上，配置E达到了7.3%的top-5错误率。  

### Multi-crop Evaluation  

作者对密集卷积网络评估和多重裁切评估进行了比较，还评估了两种技术通过计算两者softmax输出平均值的互补结果。可以看出，使用多重裁切比密集评估的效果略好，并且两种方法是完全互补的，因为两者组合的效果比每一种都要好。根据以上结果，作者假设这是由对于卷积边界条件的不同处理方法造成的。  

### ConvNet Fusion  

在提交ILSVRC参赛模型时，我们只训练了单尺度网络和一个多尺度模型D(只对全连接层进行调优而不是所有层)。7个模型组合的结果在ILSVRC错误率上为7.3%。在模型提交结束后，我们研究了只使用两个性能最好的多尺度模型的组合(配置D和E)，当使用密集评估时错误率降到了7.0%，而使用密集和多重裁切组合时，错误率仅为6.8%。作为参考，我们性能最好的单一模型错误率为7.1%(模型E，Table5)。  

### Comparision With The State Of The Art  

在与业界的对比中，作者通过对两个网络进行结合，得到了最佳的表现，top-1和top-5错误率均明显小于其他模型。在单一模型上，除了打不过GoogLeNet，其他的表现均不如它，而GoogLeNet基本上抛弃了传统的CNN框架，提出了新的inception模型，而VGG只是对传统CNN框架的改进，应用的广泛性和方便性也更高。  

## Conclusion  

在这篇paper，作者评估了非常深的卷积网络(19个权重层)在大规模图像分类上的性能。结果表明，表达的深度有利于分类准确率的提升，在传统的卷积网络框架中使用更深的层能够在ImageNet数据集上取得优异的结果。

此外作者的模型在广泛的任务和数据集上良好的泛化能力，性能达到甚至超过了围绕较浅深度的图像表示建立的更复杂的识别流程。作者的实验结果再次确认了视觉表示中深度的重要性。  

---
本博客支持disqus实时评论功能，如有错误或者建议，欢迎在下方评论区提出，共同探讨。  
