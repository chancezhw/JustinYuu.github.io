---
layout: post
title: "Daily Paper 0647"
description: "Notes"
categories: [CV-classic]
tags: [Paper]
redirect_from:
  - /2019/10/13/
---

# Daily Paper 07 - Deep Residual Networks for Image Recognition  

## Foreward  

完成了组里的任务，接下来就可以看看自己想看的paper了，在我学习吴恩达的deeplearning系列课程中，他推荐了一系列CV和NLP领域的经典文献，我在我的博客中也都有记录，但是并没有抽出时间来详细的看一下，我认为跨媒体方向至少要对CV和NLP都有基本的了解，那么我的课程学习我认为已经足够了，毕竟上了那么多课程，但是论文还没有读太多，因此在读跨媒体方向的paper之前我想先用两周的时间，每天精读一篇CV和NLP领域的经典文献，对这两个领域的开山之作有一些基本的认识。虽然研究生讲究现学现卖，但是我认为这几篇paper还是必须要读的。  

## Introduction  

这篇paper是何凯明大大在MSRA的经典作品，ResNet在2015年的ILSVRC等近乎所有计算机视觉比赛中傲视群雄，之后因其优秀的表现也被很多系统用来当做backbone。之前接触ResNet还是上吴恩达的课，现在直接看看他的论文，应该会有不一样的收获。  

深度学习网络在图像分类领域中取得了一些不错的进展，它可以整合低/中/高等级的特征，从而进行分类，而我们可以通过堆叠层数的方式来扩充特征的“等级”，事实上有研究表明网络的深度是图像分类性能的非常重要的因素，深度较高的网络可能表现会更好一些。但是问题来了，网络的性能是否等同于层的简单堆叠呢？答案可能是否定的，这是由于深层网络会产生的梯度的消失/爆炸导致的，这一问题已经被均一初始化和中间的均一化层基本解决，使得拥有数十层的深度神经网络能够通过随机梯度下降进行收敛，从而进行反向传播。  

然而，当深度网络能够开始收敛时，会出现一个新的问题，称作退化问题(degration problem)，当层数越深，准确率会逐渐饱和，之后发生骤降。他们的实验证明，退化问题与过拟合并无关系，因为训练集的表现也降低了，对一个表现良好的神经网络强行增加更多的层会导致更高的训练错误率。退化问题说明了并不是所有的神经网络都能够很简单的进行优化，这里作者的实验方案是建立一个浅层的模型，然后再建立一个相同的深层模型与之对照，深层模型的规模是浅层模型+恒等映射。恒等映射的概念不太好懂，但是cs231n中Serena进行了一些解释，简单来讲，就是增添了一些没有什么作用而只是增添层数的层，这样一来两个网络除了层数不同以外，其余功能和结构完全相同。那么我们会有一种自然的理解:层数变多的深层网络至少应该和浅层网络有着相同的表现，但是目前的研究进展并没有达到相同表现的解决方案。  

因此，这篇paper作者试图建立一种称作深度残差学习的模型来解决退化问题。与其让每一个堆叠的层都和底层输入做拟合，不如让他们都拟合一个残差映射，如果将我们需要的底层映射记为H(x)，那么我们让堆叠的非线性层适应另一个映射F(x) := H(x)-x，作者将其称之为残差映射residual mapping，那么我们需要的映射H(x)就变成了F(x)+x。作者猜测优化残差映射比直接优化底层映射本身更为方便，进而进行试验验证。极端来讲，如果非线性层只起到恒等映射的作用，即H(x)=x,那么这个时候原有的underlying mapping就是优化H(x)=x,而residual mapping就是优化F(x)=0，而作者认为后者优化更为方便  

F(x)+x，也就是我们最终需要的映射，可以通过一种叫做shortcut connection的模块实现。简单来讲，shortcut connection就是绕过一层或者多层，在作者的case里面，shortcut connection只是起到了传递x的作用，并没有增添任何新的参数和复杂度，因此可以很简单的被很多框架所使用，而不用修改求解器。作者在ImageNet上进行了实验对比，发现其深层残差网络更容易优化，但plain深层网络优化起来非常难，训练错误很多，同时残差网络可以轻松的通过增添层数而取得更好的效果，同样的结果也出现在了CIFAR-10上。他们最后搞了一个152层的深层网络……但是仍然比VGG的复杂度要低，在ImageNet训练的结果也有着很好的泛化性能，在ImageNet分类、检测、定位和COCO segmentation上都拿了第一……(这也太猛了)  

残差和shortcut connection的概念并不是何凯明自己所创，而是有相当多的related work，之前有学者利用残差向量用于图像识别，比如VLAD和Fisher Vector，之前也有学者对多层感知机添加shortcut connection，包括inception也有一些短路径，思想也和其类似。此外highway network自然而然的也会被提到，highway的思想其实也和这里的shortcut connection相似，只不过highway是可以关闭的，而这里是一直打开的，顺便作者cue了一下，highway network在层数很深的时候准确率并没有随着层数增加而明显提升(暗示我的比你强hhh)。  

## System  

首先，如果多个非线性层能够渐进的逼近一个复杂的函数，那么这些非线性层也可以渐进的逼近残差函数，即H(x)-x，不过学习的难易性会有所不同，这里的证明过程在何凯明的另一篇文献中，我在接下来会进行阅读和总结。那么随着训练的不断进行，最理想的状况就是结果不再改变，即identity mapping就是最优解，那么这个时候F(x)就全部为0，但是事实上这是不可能的，所以增加的那一部分就是对网络表现力的增强部分。接下来基本上就是把introduction里的残差块和shortcut connection重复了一遍，这里就不再赘述了。  

对于网络的结构，这里他们测试了不同的训练和对照网络，最终确定的网络结构如下：对于对照的网络，他们模仿了经典网络VGG,但是遵循两个规则：同尺寸的filter数量相同，以及特征图尺寸减半的时候过滤器数量加倍，最后的网络架构和VGG非常相似但略有不同，共34层。而残差网络的数量和维度和对照网络一模一样，每两个conv层为一个单位作为一个残差块，当输入和输出同尺寸的时候，shortcut connection可以直接使用，当不同的时候



---
本博客支持disqus实时评论功能，如有错误或者建议，欢迎在下方评论区提出，共同探讨。  
