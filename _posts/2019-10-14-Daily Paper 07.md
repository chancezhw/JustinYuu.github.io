---
layout: post
title: "Daily Paper 07"
description: "Notes"
categories: [CV-classic]
tags: [Paper]
redirect_from:
  - /2019/10/13/
---

# Daily Paper 07 - Deep Residual Networks for Image Recognition  

## Foreward  

完成了组里的任务，接下来就可以看看自己想看的paper了，在我学习吴恩达的deeplearning系列课程中，他推荐了一系列CV和NLP领域的经典文献，我在我的博客中也都有记录，但是并没有抽出时间来详细的看一下，我认为跨媒体方向至少要对CV和NLP都有基本的了解，那么我的课程学习我认为已经足够了，毕竟上了那么多课程，但是论文还没有读太多，因此在读跨媒体方向的paper之前我想先用两周的时间，每天精读一篇CV和NLP领域的经典文献，对这两个领域的开山之作有一些基本的认识。虽然研究生讲究现学现卖，但是我认为这几篇paper还是必须要读的。  

## Introduction  

这篇paper是何凯明大大在MSRA的经典作品，ResNet在2015年的ILSVRC等近乎所有计算机视觉比赛中傲视群雄，之后因其优秀的表现也被很多系统用来当做backbone。之前接触ResNet还是上吴恩达的课，现在直接看看他的论文，应该会有不一样的收获。  

深度学习网络在图像分类领域中取得了一些不错的进展，它可以整合低/中/高等级的特征，从而进行分类，而我们可以通过堆叠层数的方式来扩充特征的“等级”，事实上有研究表明网络的深度是图像分类性能的非常重要的因素，深度较高的网络可能表现会更好一些。但是问题来了，网络的性能是否等同于层的简单堆叠呢？答案可能是否定的，这是由于深层网络会产生的梯度的消失/爆炸导致的，这一问题已经被均一初始化和中间的均一化层基本解决，使得拥有数十层的深度神经网络能够通过随机梯度下降进行收敛，从而进行反向传播。  

然而，当深度网络能够开始收敛时，会出现一个新的问题，称作退化问题(degration problem)，当层数越深，准确率会逐渐饱和，之后发生骤降。他们的实验证明，退化问题与过拟合并无关系，因为训练集的表现也降低了，对一个表现良好的神经网络强行增加更多的层会导致更高的训练错误率。退化问题说明了并不是所有的神经网络都能够很简单的进行优化，这里作者的实验方案是建立一个浅层的模型，然后再建立一个相同的深层模型与之对照，深层模型的规模是浅层模型+恒等映射。恒等映射的概念不太好懂，但是cs231n中Serena进行了一些解释，简单来讲，就是增添了一些没有什么作用而只是增添层数的层，这样一来两个网络除了层数不同以外，其余功能和结构完全相同。那么我们会有一种自然的理解:层数变多的深层网络至少应该和浅层网络有着相同的表现，但是目前的研究进展并没有达到相同表现的解决方案。  

因此，这篇paper作者试图建立一种称作深度残差学习的模型来解决退化问题。与其让每一个堆叠的层都和底层输入做拟合，不如让他们都拟合一个残差映射，如果将我们需要的底层映射记为H(x)，那么我们让堆叠的非线性层适应另一个映射F(x) := H(x)-x，作者将其称之为残差映射residual mapping，那么我们需要的映射H(x)就变成了F(x)+x。作者猜测优化残差映射比直接优化底层映射本身更为方便，进而进行试验验证。极端来讲，如果非线性层只起到恒等映射的作用，即H(x)=x,那么这个时候原有的underlying mapping就是优化H(x)=x,而residual mapping就是优化F(x)=0，而作者认为后者优化更为方便  

F(x)+x，也就是我们最终需要的映射，可以通过一种叫做shortcut connection的模块实现。简单来讲，shortcut connection就是绕过一层或者多层，在作者的case里面，shortcut connection只是起到了传递x的作用，并没有增添任何新的参数和复杂度，因此可以很简单的被很多框架所使用，而不用修改求解器。作者在ImageNet上进行了实验对比，发现其深层残差网络更容易优化，但plain深层网络优化起来非常难，训练错误很多，同时残差网络可以轻松的通过增添层数而取得更好的效果，同样的结果也出现在了CIFAR-10上。他们最后搞了一个152层的深层网络……但是仍然比VGG的复杂度要低，在ImageNet训练的结果也有着很好的泛化性能，在ImageNet分类、检测、定位和COCO segmentation上都拿了第一……(这也太猛了)  

残差和shortcut connection的概念并不是何凯明自己所创，而是有相当多的related work，之前有学者利用残差向量用于图像识别，比如VLAD和Fisher Vector，之前也有学者对多层感知机添加shortcut connection，包括inception也有一些短路径，思想也和其类似。此外highway network自然而然的也会被提到，highway的思想其实也和这里的shortcut connection相似，只不过highway是可以关闭的，而这里是一直打开的，顺便作者cue了一下，highway network在层数很深的时候准确率并没有随着层数增加而明显提升(暗示我的比你强hhh)。  

## System  

首先，如果多个非线性层能够渐进的逼近一个复杂的函数，那么这些非线性层也可以渐进的逼近残差函数，即H(x)-x，不过学习的难易性会有所不同，这里的证明过程在何凯明的另一篇文献中，我在接下来会进行阅读和总结。那么随着训练的不断进行，最理想的状况就是结果不再改变，即identity mapping就是最优解，那么这个时候F(x)就全部为0，但是事实上这是不可能的，所以增加的那一部分就是对网络表现力的增强部分。接下来基本上就是把introduction里的残差块和shortcut connection重复了一遍，这里就不再赘述了。  

对于网络的结构，这里他们测试了不同的训练和对照网络，最终确定的网络结构如下：对于对照的网络，他们模仿了经典网络VGG,但是遵循两个规则：同尺寸的filter数量相同，以及特征图尺寸减半的时候过滤器数量加倍，最后的网络架构和VGG非常相似但略有不同，共34层。而残差网络的数量和维度和对照网络一模一样，每两个conv层为一个单位作为一个残差块，当输入和输出同尺寸的时候，shortcut connection可以直接使用，当不同的时候有两种选择：进行zero-padding或者用线性投影变换来将其换成相同的尺寸。  

## Experiments  

第一个实验在ImageNet2012数据集上进行，这个数据集相当大，共有1280000张图片作为训练集，1000个类，50k的开发集和100k的测试集，规模相当庞大。作为对照网络，作者测了18层和34层两种不同的网络架构，结果显示34层的网络有更高的错误率，这正是由于退化问题导致的，34层的训练误差会明显的升高，虽然18层的网络是34层的子架构。作者认为这种训练的困难并不是因为梯度消失导致的，因为对照网络采用了批量均一化，经过确认在反向传播的时候梯度都健康的存在着，并且事实上34层的对照网络仍然有比较好的正确率，这意味着整个网络某种程度上也是有效的。作者推测深度的对照网络可能有指数级的低收敛率，因此影响了训练误差，具体的原因仍不明朗。  

而同时作者测试了18层的和34层的ResNet，首先使用zero-padding来处理尺寸不同的情形，使用恒等映射从而保证没有任何参数的增加，结果显示与18层ResNet相比，34层ResNet有更低的训练误差和在开发集更好的泛化性能，这代表退化性能很好的被解决，并且作者成功的利用网络的深度来提高准确率。与34层对照网络进行比较，结果发现ResNet有效的降低了训练误差，证明了残差思想的有效性和正确性。最后比较了18层的两个网络，结果显示18层的ResNet收敛的更快，两者的准确率差不多，但是ResNet提供了更有效率的训练过程。  

接下来对比了恒等映射和投影shortcut以及zero-padding的区别，结果显示三者的ResNet都比对照网络表现要好，投影和恒等映射夹杂的组表现略好于zero-padding，全部都是投影的组表现最好，但是由于投影对退化问题并无实质性帮助，而仅仅是增加了一些额外的参数从而得到更好的训练结果，所以为了节省时间和计算的复杂度，以后的实验并不会采用这种形式，而是采用恒等映射这种并不会增加复杂度的方法。  

作者将34层的2层conv的残差块全部换成了3层conv的，把ResNet的层数提高到了50层，此外还用这种残差块建立了101层的ResNet和152层的ResNet，结果发现152层的网络复杂度仍然比VGG低（好猛），50/101/152的ResNet都比之前的34层的ResNet性能要好，并且并没有发现退化现象，结果发现错误率在不断的降低，最后得到的错误率秒杀当时所有模型。  

此外还在CIFAR10数据集上进行了类似的实验，这里我就不写细节了，最后的结果表明性能仍然优秀，此外层响应实验表明残差函数一般来讲比非残差函数更接近0，并且更深的残差网络有更小的反映大小，当层数比较多时，ResNet的每一层会更少的对结果进行修改。接下来作者试图训练一些超深层的网络，层数超过1000层的时候仍然没有训练困难（……），训练误差也小于0.1%，表现属实很优秀，不过深层网络也有深层的问题，例如1202层的网络表现就比110层的表现要差，虽然训练误差差不多，但是过深会导致过拟合，如此多的参数对于一般的数据集规模来说太多了。  

此外ResNet也可以应用在PASCAL和MS COCO等数据集上的检测问题，表现也很好，这里作者没有细讲，我也不做复述了。  

## Conclusion  

总结一下，这篇论文很长，但是大部分都是在讲自己的实现细节，对于原理的解释也就那么多，并没有太多原理上的信息，复现起来对于参数的设置应该比较方便，那么这篇paper主要讲了一个残差网络的思想和实现细节，以及设计了一个秒杀当今所有模型的神经网络，并讲了一下实现的细节。  

---
本博客支持disqus实时评论功能，如有错误或者建议，欢迎在下方评论区提出，共同探讨。  
