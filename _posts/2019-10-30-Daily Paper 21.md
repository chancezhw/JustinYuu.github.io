---
layout: post
title: "Daily Paper 21"
description: "Notes"
categories: [MMML-Self Supervised]
tags: [Paper]
redirect_from:
  - /2019/10/29/
---

# Daily Paper 20 - Cooperative Learning of Audio and Video Models from Self-Supervised Synchronization  

## Introduction  

这周小组内下了新的任务，看一些自监督的多模态机器学习方向的paper，所以暂时把VLN领域的paper放一放，先把任务完成。  

今天是第一篇，也是比较有代表性的一篇，是FAIR和达特茅斯大学共同发表在nips2018上的，其实FAIR的那位也是达特茅斯的毕业phd，所以几乎相当于是达特茅斯的paper。paper主要讲的是自监督同步的音视频模型协同学习，由于在视频中，图像和音频存在着自然的关联，作者在这篇paper中利用此关联来学习自监督的音视频时序同步的通用而有效的模型。作者表明一个校准的课程学习方案、一个对于负样本的仔细选择和对比度损失的使用是从优化的模型中获得强大的多感官表示，以实现音频-视频时序同步的重要因素。在没有调优的情况下，最终的audio features能够在已有的音频分类benchmark上得到优于或等同于现有模型表现的结果。与此同时，作者的视频子网络提供了一种非常有效的初始化方式，可以有效的提高基于视频的动作识别模块的准确率，具体来讲，与从头开始训练相比，作者的自监督预训练模型获得了UCF101上19.9%的动作识别率提升和HMDB51上的17.7%的动作识别率提升。  

虽然图像识别的发展非常迅速，对于图像的处理现在已经相当成熟了，但是对于视频的理解，深度学习方法的表现还一直不够好，在视频上训练的深度时空模型表现一直不如最好的手工特征表现。不过近几年来情况有所改善，数据集和细分类别的迅速增加使得深度学习模型的端到端训练更为有效，而对于类别的细化定义使得学习更具有辨别力的特征也成为了可能，不过这也伴随着手动注释的时间成本的升高。此外，也有人认为仅仅通过数据集的增长来改善模型表现需要将现有的benchmark扩大一个或者多个数量级，而这在短期是不现实的。因此作者提出了一种自监督学习的方式，不需要任何人工标注，从而能够更为轻松的训练任意大小的训练集，作者利用了视频中音频和视觉通道之间的自然协同性，建立了一个自学习任务，让机器来判定给定的音频和视频序列究竟同步与否，这个二元分类问题称作"Audio-Visual Temporal Synchronization(AVTS)，作者使用两个stream的网络来处理这一问题，第一个stream接受音频，第二个接受视频，在网络的后期两个stream进行融合，从而进行协同学习以改善性能。作者简要的介绍了一下AVTS和视听对应AVC的区别，AVC的训练负样本通常来自于不同的视频，比如一段视频的图像和另一个不相关视频的音频，因此模型主要通过语义来判断是否对应，而AVTS需要时间步的严格对应，因此需要用同一个视频里的不同部分作为负样本进行训练，迫使网络学习给定音频和图像的时间相关特征。时间同步是一个比语义对应更为复杂的任务，因为它需要确定音频和视觉样本的时间步严格对齐，而作者在之前的研究证明了采取课程学习能够有效的简化学习，并提升所有下游任务的功能质量。  

作者使用两个不同的流来处理视频的视觉和音频部分，学习之后这两个流就可以独立作为两个模态的特征提取器，具体的性能提升在一开始已经介绍过了，就不重复介绍了。  

## System  

这一部分在原文里的题目是Technical Approach，是整篇文章的重点，我也将重点分析这一部分。首先作者对AVTS进行了明确的定义，其次介绍了作者的模型对于AVTS的两个下游任务动作识别和音频分类的具体贡献。  

### AVTS  

首先是AVTS，假设作者获得了n个标记的视频-音频对组成的训练集D={(a1,v1,y1),...,(an,vn,yn)},an和vn表示的是第n个序列的音频样本和视频的RGB序列，标签yn代表是否同步，不管同步与否，同一组的a和v跨越的时间步数量都是相同的。  

从高层次来说，AVTS的主要目标是为了学习分类函数g(.)，最小化经验误差，使得g(an,vn) = yn尽可能多的成立。但是由于我们的首要任务是让AVTS成为视听特征的自监督学习代理，那么分类函数可以定义为由听觉子网络fa(.)和视觉子网络fv(.)共同组成的双流网络，使用函数g(fa(an),fv(vn))来融合两种模态的特征信息，以解决同步任务。  

### Loss Function  

对于二元分类来说，使用交叉熵函数作为损失函数是最正常的选择，但是作者在实际中发现，从头开始学习时很难收敛该损失函数，因此作者采用Siamese网络中使用的对比度损失函数来训练，这样得到的结果更为稳定和鲁棒，并设置一个阈值τ来完成g(fa,fv)的判断问题。作者还用了几个FC层进行微调，结果显示表现差不多，而在下游的两个任务结果显示用交叉熵函数调优后的结果并无明显改善。  

### Selection of Negative Examples  

正样本显然很好选择，但是负样本的选择就要费脑筋了，作者主要选择两种版本的负样本，第一种是来自两个不同视频的视听素材，称为easy negative，另一种是hard negative，两者都选自同一个视频，只不过时间差距要在半秒以上，原因在之前介绍AVTS和AVC的差别的时候说了，是训练模型对于时间步的敏感性，作者还试着使用super hard negative，视频和音频会有一部分的重叠，正确判断的难度也就相应的更大。当采用hard和super hard训练的时候，AVTS的准确率会相应的下降，也会降低后续的下游任务的质量，而作者会在接下来说明，采用课程学习策略能够有效的利用hard negative中的信息进行学习并改善模型的表现。  

### Curriculum Learning  

作者从头开始训练其系统，包括单独使用简单的负样本，单独使用hard negative，以及使用固定比例的简单和hard负样本。作者发现，当从一开始就引入hard negative时，目标非常难以优化，因此AVTS任务的测试结果很差。然而，如果我们仅使用简单的负样本进行初始优化，之后再使用一些更难的负样本进行微调，可以在AVTS模型的准确率和下游任务的性能方面产生更好的结果。根据经验，当使用由25％hard negative和75％easy negative组成的负样本集进行微调时，AVTS模型的准确率最高，此外下游任务的改进更为显著。  

### Architecture Design  

最后就是网络的架构，网络由两个子网络组成，视频子网络前半部分使用3D时空卷积，后面使用2D卷积，因此整个网络是基于mixed-convolution(MCx)混合卷积设计的；对于音频流，作者使用的是Chung和Zisserman的架构：首先将音频转换为MP3格式，并计算FFT滤波器特征并通过类似VGG的卷积架构。  

## Experiments  

实验的细节就不多说了，直接看结果。在几个数据集Kinetics,SoundNets,AudioSet上训练了AVTS模型，结果表示课程学习使得AVTS的准确度提升了9.4%，不过作者发现使用super hard negative在不论何时都是有害的，可能是因为其难以辨别导致训练困难。  

接下来是下游的两个任务动作识别和音频分类的表现，首先看AVTS作为动作识别预训练模型的表现，结果显示AVTS预训练模型在UCF101和HMDB51训练集上得到了准确性的显著提升，在UCF101上提升了16.7%，后者提升了13.0%。此外，由于该模型是自监督的，因此其在更大的数据集上会有更好的表现，作者在AudioSet这一比Kinetics大近八倍的数据集上预训练了AVTS模型，结果显示精度有进一步的提高，在UCF101上可以达到89.0%。  

接下来是音频特征提取的评估，作者提取conv5层的特征，并在两个声音分类数据集ESC-50和DCASE2014上测试其特征质量，结果显示在AVTS上学到的音频特征在这两种声音分类任务中都能很好地泛化，产生的性能优于或接近目前的最高水平。从结果可以看出，在这两个基准测试中从头开始直接训练的音频子网表现非常糟糕，这表明作者的方法的有效性在于自监督的学习过程而不是在网络架构中。在ESC-50上，作者的方法不仅优于其他最近的自监督方法（Net和SoundNet），而且还略微超过了人类的表现。  

此外，作者还评估了AVTS进行多模态动作识别的表现，即用视频中的音频和视觉两方面特征来预测动作，结果显示比其他自监督学习方法有更高的准确度。最后作者进行了AVTS和所有下游任务性能的比较，结果显示课程学习的AVTS系统各指标表现均优于L³-Net。  


---
本博客支持disqus实时评论功能，如有错误或者建议，欢迎在下方评论区提出，共同探讨。  
