---
layout: post
title: "Daily Paper 33: RCM+SIL for VLN"
description: "Notes"
categories: [MMML-VLN]
tags: [Paper]
redirect_from:
  - /2019/11/11/
---

# Daily Paper 33 - Reinforced Cross-Modal Matching and Self-Supervised Imitation Learning for Vision-Language Navigation  

## Introduction  

由于组内布置的自监督跨模态学习方向的paper已经看完了，我就可以继续看我的VLN领域的paper了。今天的这篇paper的来头可不小，2019CVPR的best paper，总分满分排名第一，是UCSB、微软和杜克大学共同发表的。  

VLN指的是对一个embodied agent进行导航，使其在真实的3D环境中执行自然语言指令。这篇paper主要聚焦VLN任务中比较困难的三个部分：跨媒体grounding，不适当反馈和泛化问题。首先，作者提出了一个强化跨模态匹配RCM系统，通过强化学习的方式来增强了局部和全局跨模态匹配的表现。具体来说，使用一个对应的critic来提供固有的奖励，用来激励指令和轨迹之间的全局同步，使用一个推理导航器来进行局部视野的跨模态识别。在VLN领域的benchmark数据集上进行评价，得出作者的RCM模型能够显著的在SPL上提升10%的表现，并能够得到当今最好的表现。最后，为了提高学习策略的泛化性能，作者进一步介绍了一个自监督模仿学习（SIL）方法，通过使模型模仿自己在过去做的好决定，来探索新的未曾见到过的新环境。作者证明了SIL学习策略能够逼近一个更为高效和有效的策略，这显著的降低了VLN模型在新环境和旧环境之间的表现差距。  

近年来，由于VLN在日常生活中的广泛应用，比如家用机器人和私人助理，vision-language grounded embodied agents受到了越来越多的关注。与此同时，这样的agent将其自身放置在一个第一视角下的实时学习场景下，从而进行visual和language grounding。在众多任务中，VLN视觉语言导航是一个典型的例子，它接收一系列自然语言指令，然后根据指令在自然世界中导航agent。VLN要求对于语言语义、视觉感知有充分的理解，更重要的是对于两者理解后的对齐。  

VLN有一些比较独特的挑战：1.理解视觉图片和自然语言可能会比较难，agent需要将自然语言指令在局部视觉空间中处理成为一个序列状的词语表示，并且要把指令和在全局时序空间中的视觉轨迹相对应。第二，除了严格遵守专家指定路线以外，整个VLN任务的反馈很弱，因为成功的标志是作者找到了目标物体，但是却并不关心具体的路径是什么，那么agent有可能按照指令走，也有可能按照随机路线走，这并没有一个合适的反馈来判别。此外即使agent行走的路径完完全全按照原指令进行，但是最终停止的位置略微早了一点，那么VLN系统会判定最终的定位失败，整个流程会被判定为不成功，但是这个时候路径的成功并不会被探测到，也会被标记为错误，给学习带来了误导，从而导致最终的结果偏离了最优的策略学习。第三，现有的工作泛化性能都不是太好，在熟悉和陌生的环境中的表现差距太大。  

这篇文章中，作者主要将强化学习和模仿学习的力量应用在了VLN任务中，首先他们介绍了RCM，使得模型可以通过强化学习进行局部和全局的跨模态grounding。特别的，作者设计了一个推理导航器reasoning navigator，同时通过局部视角和语言指令来学习跨模态grounding，从而使得agent可以推理应该着重注意哪些子指令。以及应该看哪里。从全局视角来看，作者给agent装备了一个matching critic，这个东西可以评估agent行进的路径，具体方法是重建原始指令的概率，这也被作者称为循环重建奖励。简单来说，这个东西的作用就是提供一个固定的奖励信号，用来使得模型不断修正其路径，使得最终的路径和指示中的路径尽可能的一致。  

那么总结一下，模型会得到两个reward，第一个是内在的固有reward，用于评价路径是否正确，另一个是外在的环境reward，用于评价是否找到目标点。作者的RCM模型显著的超越了现有所有模型的表现，并在R2R数据集上获得了STOA表现。  

此外为了增强模型的泛化性，作者提出了一个自监督模仿学习模型，对未标注的未知环境进行探索，agent试图模仿之前的好的经历来解决新环境中的问题。具体来说，在我们的框架里，导航器进行了多次的roll-outs，通过matching critic决定的好的学习策略被存储在一个回放缓冲池中用于后续导航器的模仿，因此导航器可以逐渐逼近它的最好表现，从而得到一个好的策略。  


作者又将其贡献总结了一遍，我真的不想第三次复述了，就把这重复的一段略过吧。  

## Related Works  

这篇paper介绍的详细一点，简单说下Related Works，首先是Vision-and-Language Grounding部分，近年来大家都在想如何把NLP和CV这两个巨火无比的领域合作起来，于是就搞了跨媒体这么一个领域，主要研究视觉和语言或者视觉和听觉之间的联系。大家研究的比较多的领域比如VQA、VCR、MR等等，都有一个共同点，即视频是不变的，视频就是给定的那个视频。再好比机器翻译和AVC等问题，音频也是不变的，模型只需要给什么处理什么就好了，按照同样的范式处理即可。VQA和VCR等任务只是给定的问题不同，视频场景还是不变的。那么到了VLN问题中，视频会随着agent的时间和空间位置而动态变化，这是因为agent是在不断移动的，而agent能看到的空间视觉信息只有它自己的第一视角，那么VLN任务就需要agent能够与周围的环境进行实时的交流，也相应的会更加困难。  

其次再来看Embodied Navigation Agent，对于一个移动信息系统来说，进行导航应该说是比较基础和重要的任务。Anderson首先提出了VLN问题，并给出了一个基于注意力机制的seq2seq baseline模型。Wang等人接着提出了一个hybrid模型，将model-free和model-based的强化学习方法结合在了一起，用于提高模型的泛化能力。最近Fried等人提出了一个speaker-follower模型，使用数据增强、全景动作空间和改正版集束搜索来处理VLN问题，并在R2R上得到了STOA的结果。作者提出的RCM模型虽然也是使用强化学习方法，但是还是有一些不同的，具体的不同有三点：1.将强化学习和模仿学习结合在了一起，而Fried等人的方法只用了监督学习；2.作者的推理导航器进行了跨模态grounding，而不是单一模态的时序注意力机制；3.作者的matching critic虽然在网络架构上和Fried等人的模型非常相似，但是作者使用了cycle-reconstruction固有奖励来进行RL和SIL的训练，而后者只是用了简单的数据增强。  

最后是Exploration，在改善exploration方面已经进行了许多工作，因为在exploration与exploitation之间的权衡是RL的基本挑战之一，也是博弈论的一个经典议题，其主要内容是平衡探索新事物带来的风险和利用旧事物带来的收益。agent需要利用(exploit)其学到的知识来获得最大的奖励，并探索(explore)新的领域以更好地进行策略搜索。好奇心或不确定性已被用作exploration的信号。最近，Oh等人建议exploit过去的经验来更好地exploration，并从理论上证明其有效性。我们的自监督模仿学习（SIL）方法具有相同的想法。但是，我们并不是在游戏上进行的测试，而是使用SIL在更为实际的VLN任务上验证了其有效性和效率。  

## System  

### Reinforced Cross-Modal Matching  

先来看第一个重要的模块RCM，RCM主要有两个子模块：推理导航器和matching critic。模型接收到自然语言指令的单词序列χ之后，推理导航器会进行一系列行动a1,a2,...,a<sub>T</sub>∈A，并生成了一个路径τ。导航器与环境直接进行互动，并在执行行动动作的时候感知新的视觉场景。为了提高模型的泛化性和强化策略学习，作者介绍了两个奖励函数：由环境提供的外部奖励，通过衡量导航成功的信号和每一个action的导航错误得到；以及一个内部固有的奖励，它来自作者的matching critic，主要衡量语言指令χ和导航器轨迹τ之间的对齐程度。  

#### Reasoning navigator  

接下来重点介绍一下这两个子模块。首先看跨模态推理导航器，推理导航器π<sub>θ</sub>是一个基于策略的agent，能够将输入的指令χ映射到一个动作序列A中。在每一个时间步t，导航器都会从环境中接收到一个状态s<sub>t</sub>,并需要将收到的语言指令映射到第一视角下的视觉场景中。因此作者设计了一个跨模态的推理导航器，它能够学习历史轨迹、文本指令和第一视角下的视觉注意力，从而组成了一个多模态的推理路径，从而激励在该时间步下的多模态局部运动。导航器装备了全景相机，能够看到各个视角的图像，导航器主动的将全景图像分成了m个视角下的图像，那么在t时间步下视觉状态的全景特征s<sub>t</sub>可以被表示成{v<sub>t,j</sub>}，v<sub>t,j</sub>代表t时刻下j视角下的图像预训练CNN特征。  

导航器一旦迈出第一步，那么它的视角就开始不断的变化，随着自身位置的不同，导航器的全景视角也会不同。这个时候我们就必须要求导航器需要拥有一定的记忆功能，从而将之前看到的一些视觉信息记住，这里自然而然的会想到LSTM。作者使用一个基于注意力机制的轨迹编码器LSTM：h<sub>t</sub> = LSTM(\[v<sub>t</sub>, a<sub>t-1</sub>], h<sub>t-1</sub>)。作者使用点积注意力机制，具体的公式是v<sub>t</sub> = attention(h<sub>t-1</sub>,{v<sub>t,j</sub>}) = Σsoftmax(h<sub>t-1</sub>W<sub>h</sub>(v<sub>t,j</sub>W<sub>v</sub>)<sup>T</sup>)v<sub>t,j</sub>，Wh和Wv是两个可学习投影矩阵。  

由于上述LSTM注意力机制编码器记录了历史的信息，导航器就可以利用过去的信息来识别当前的状态，并且理解哪些单词或者子指令需要在接下来完成。这就需要我们学习基于历史语境的文本上下文。作者使用一个语言编码器LSTM来将语言指令X编码为一系列语言指令{w<sub>i</sub>}，在每一个时间步都用注意力机制进行计算，公式为c<sub>t</sub><sup>text</sup> = attention(h<sub>t</sub>,{w<sub>i</sub>}<sub>i=1</sub><sup>n</sup>)，当c的值越大，说明历史轨迹和当前的视觉状态越吻合。  

作者还需要动态的理解语言指令来确定接下来看哪里，这和上一段的子任务相似，都是使用注意力机制来完成，不过这里是进行文本条件下的视觉上下文分析，公式和上面的公式非常类似，c<sub>t</sub><sup>visual</sup> = attention(c<sub>t</sub><sup>text</sup>,{v<sub>j</sub>}<sub>j=1</sub><sup>m</sup>)。  

最后，使用一个动作预测器来将上面生成的历史上下文h<sub>t</sub>，文本上下文c<sub>t</sub><sup>text</sup>和视觉上下文c<sub>t</sub><sup>visual</sup>结合在一起，从而决定接下来朝哪个方向走。该预测器使用双线性点乘来计算每一个可能的方向的具体概率p<sub>k</sub>，具体公式是p<sub>k</sub> = softmax(\[h<sub>t</sub>,c<sub>t</sub><sup>text</sup>,c<sub>t</sub><sup>visual</sup>]W<sub>c</sub>(u<sub>k</sub>W<sub>u</sub>)<sup>T</sup>)，其中u<sub>k</sub>是第k个导航方向的动作embedding，它是通过将一个表现特征向量（从该方向的图像像素块中用CNN提取出的特征向量）和一个4维的方向向量\[sinψ;cosψ;sinw;cosw](ψ和w分别代表相对heading和evaluation角度，即方向和高度)结合在一起实现的。  

#### Cross-Modal Matching Critic  

接下来看第二个子模块：跨模态Matching Critic，这一子模块的目的是正确的评价agent路径和指令路径的差异，从而得到一个内部固有的奖励信号，该信号可以表示为R<sub>intr</sub> = V<sub>β</sub>(X,τ) = V<sub>β</sub>(X,π<sub>θ</sub>(X))，而我们要做的就是去测量cycle-reconstruction reward p(X = X\|π<sub>θ</sub>(X))，这其实就是给定导航器自己走的轨迹τ=π<sub>θ</sub>(X)之后重建语言指令X的概率，那么显然的，重建的概率越高，所提供给它的导航器路径越好。这里matching critic自然无法在一开始就能完美的评价，所以作者采取用人类注释的真实指令-轨迹对进行监督学习训练。  

### Learning  

接下来是整个模块的训练过程。为了快速的逼近一个好的策略，作者使用示范行动来用最大似然估计进行监督学习，训练的损失函数L<sub>sl</sub> = -E\[log(π<sub>θ</sub>(a<sub>t</sub>\*\|s<sub>t</sub>))]，其中a<sub>t</sub>\*是模拟器提供的示范行动，使用示例行动来对agent进行warm starting是为了保证在熟悉的环境中有一个相对较好的策略，不过这也限制了模型的泛化性能，因为它只会复制专家示例的行动。那么为了学习一个更好、泛化性更强的策略，作者在后续的训练中就切换为了强化学习，并用外部和内部的奖励函数来分别从不同的视角优化策略。  

#### Extrinsic Reward  

首先看外部奖励，大家都知道强化学习经常用的一个方法是直接对评价指标进行优化，而由于VLN任务的最终目的就是成功的到达目的地，所以作者相处了两种评价指标，第一种是看停止位置和目标位置的相对距离，在agent每做一个动作后都比较agent和目标位置的即时距离，奖励为上一个时间步的距离减去这一个时间步的距离，那么这一个动作做完之后如果离得更近了，奖励就是正的，如果离得更远了就是负的，这非常好理解。作者的第二个评价指标是将“成功”作为一个额外的标准，具体而言是说如果agent的位置和目标的位置在某一个可接受的阈值距离d之内，就说明agent成功的找到了目标点，那么这个时候就可视为成功。此外为了将未来时间步的action的影响也包括在内，并为了局部贪心搜索，作者使用了一个折现累计奖励来替代了实时奖励去训练这一策略，折现的因子是0.95，公式如下：R<sub>extr</sub>(s<sub>t</sub>,a<sub>t</sub>) = r(s<sub>t</sub>,a<sub>t</sub>) + Σ(t'=t+1,T) γ<sup>t'-t</sup>r(s<sub>t'</sub>,a<sub>t'</sub>)，越往后的时间步带来的影响呈指数缩减。  

#### Intrinsic Reward  

内部的奖励在之前提到过，为R<sub>intr</sub> = p(X\|π<sub>θ</sub>(X)) = p(X\|τ)，该奖励是为了纠正路径之中偏离指令指定的路径的部分，尽可能的使整个路径达到最优。那么外部的奖励和内部的奖励合在一起，就变成了最终的RL损失函数，具体来说是L<sub>rl</sub> = -E<sub>a<sub>t</sub>~π<sub>θ</sub></sub>\[A<sub>t</sub>]，这里A<sub>t</sub> = R<sub>extr</sub> + δR<sub>intr</sub>，根据REINFORCE算法，该不可导的基于奖励的梯度函数可以被看做如下公式进行优化：▽<sub>θ</sub> = -A<sub>t</sub>▽<sub>θ</sub>logπ<sub>θ</sub>(a<sub>t</sub>\|s<sub>t</sub>)。  

### Self-Supervised Imitation Learning  

接下来就看作者的第二个子模块自监督模仿学习。在上面的RCM模块，模型是通过在已知的环境中进行训练优化，然后再在未接触过的环境中进行测试的，这相当明显的会导致泛化性能变差。而作者提出这个SIL模型也正是为了解决这一问题，算是一种补充手段吧。在这一模块中，作者允许agent在没有ground-truth注释的新环境下进行explore，这一方法是有实际意义的，原因是它可以促进模型进行终生学习（……），并增加模型对新环境的适应性。我个人感觉他这一段完完全全都是废话，一丁点有意义的东西都没有说出来。  

在未标注的环境中，我们无法像之前一样使用matching critic直接来比较标准路径和agent的实时路径的差距，所以我们需要采用一种全新的方法让agent进行exploration，这种方法显然是自监督的，因为原始环境是从未见过的，自然不会有人进行标注。那么给定一个没有标准路径和正确目标位置参考的自然语言指令，导航器会产生一系列可能的轨迹，通过matching critic比较出一个最好的轨迹，然后将这个最好的轨迹存在一个replay buffer回放缓冲池中。  

原理很简单，关键就是matching critic如何比较。之前的matching critic是以重建原始指令的概率为评价标准的，这里由于也有原始指令，所以可以直接拿过来，仍然使用原来的cycle-reconstruction奖励机制，这个时候由于有一个缓冲室replay buffer，matching critic就可以在缓冲室里面exploit，这就使得matching critic可以通过自监督学习的方式不断调优路径。最终的损失函数为L<sub>sil</sub> = -R<sub>intr</sub>logπ<sub>θ</sub>(a<sub>t</sub>\|s<sub>t</sub>)  

这里的L<sub>sil</sub>可以看做是策略梯度的损失函数，只是用非策略性的蒙特卡洛模拟收益R<sub>intr</sub>来代替策略性收益。L<sub>sil</sub>也可以被看做是监督学习损失，其中存在缓冲池的最佳路径可以被看做是ground truth，那么整个公式又可以写成L<sub>sil</sub> = -E\[logπ<sub>θ</sub>(â<sub>t</sub>\|s<sub>t</sub>)]，其中â是存在回放缓冲池中的路径。与一个matching critic配对之后，SIL方法可以和多种学习方法相结合，从而通过模仿自己之前的高光表现来逼近一个更好的策略。  

## Experiments  

### Setups  

数据集自然是R2R数据集，目前貌似也就这一个。VLN测试的标准流程是在已知的环境下训练，然后在未知的环境下测试，测试集上没有经过任何形式的exploration，这种测试方法非常合理，作者也沿用了这一方法。作者采用了5个评价指标：Path Length(PL),Navigation Error(NE),Oracle Success Rate(OSR),Success Rate(SR),Success rate weighted by inverse Path Length(SPL)。PL代表总路径长度，NE代表终点和目标点的最短距离，OSR代表agent整个路径离目标点最近的SR，SR代表终点在目标点方圆3m内的比例，SPL代表SR除以PL，同时衡量路径是否最优以及准确率是否够高，应该是当今最为明确和有效的VLN评价指标。  

作者使用未经预训练的ResNet-152 CNN网络提取图像的特征，使用预训练的GloVe词嵌入模型进行初始化和训练过程中的调优。作者使用人类的示范来训练了matching critic，在策略学习的过程中不断调整它，之后作者通过监督学习以1e-4的学习率对模型进行warm start，然后学习率除以10转为强化学习，使用Adam优化。  

### Results  

作者首先在R2R数据集上将RCM的表现和STOA进行比较，主要进行对比的baseline为以下四组：1.Random.2.seq2seq，这个模型是当前的STOA，使用student-forcing方法进行训练。3.RPA，一个结合model-free和model-based的强化学习VLN方法。4.Speaker-Follower,一个使用数据增强、全景动作空间、集束搜索的Speaker-Follower方法。结果表明RCM显著的提高了VLN任务的表现，将SPL分数从28%提升到了35%，其他指标也显著的提升了很多，在除SPL以外的评价指标上都取得了STOA表现。  

接着作者在训练过程上加了SIL，结果表明加上SIL之后在已知的环境中SPL和PL表现有所提升，其余表现略有下降，但是在unseen场景下最后的性能表现非常不错，能直接与已知环境下测试结果相比。  

最后作者进行了隔离实验，首先去掉内部奖励，结果表明模型在陌生环境下的SR显著下降，但是在已知环境下并无明显性能下降，这表明内部奖励可以作为提高泛化性的补充手段。接着又去掉了强化学习方法，只用监督学习进行学习，这相当于放弃了外部激励，结果显示性能会有一定的下降，这说明了直接对外部激励进行RL优化有利于提高强化学习的稳定性，并显著的提升整体表现。接着作者去掉了推理导航器……和基于注意力机制的seq2seq模型进行比较，两个模型完全一样，除了作者的模型有跨模态注意力机制，结果作者的模型表现更好，说明结合历史语境、语言条件的视觉语境、视觉条件的语言语境能够显著的提高整体表现。最后作者探究了SIL的效果，无SIL和有SIL的直接对比可以看出在seen和unseen条件下性能都有明显提升，这说明从之前的优秀表现中进行学习是有效的。  、

作者还进行了定性分析，选了一个成功和失败的例子进行对比，失败的原因是因为模型无法正确理解laundry room，因此在最后一步走进了错误的房间，这表明在导航任务中还需要进行更为准确的visual grounding，这也是一个可能的优化方向，找一个新模型来代替raw ResNet或许能显著改善性能凑篇paper出来。  

## Conclusion  

总结一下，这篇paper的内容还是不少的，创新性的将强化学习和自监督模仿学习应用到了VLN任务中，提出了RCM和SIL两个新的方法，从而有效的提升了VLN领域的表现。最为重要的是，极大的提升了VLN的泛化性能，而由于VLN任务的特殊性，不可能保证导航的使用环境之前已经训练和接触过，所以泛化性能足够好才是这一任务最最最重要的要求，那么这篇paper的结果才是真正的将VLN任务变成了一个可应用落地的任务。此外作者认为RCM和SIL想法能够应用在其他的embodied agent task中，提高其他任务的表现。这篇paper的两个新方法每一个方法发一篇CVPR我认为应该是没啥问题的，作者直接合在一起一篇发出来，一举拿下CVPR19的Best Paper，可谓实至名归。  

---
本博客支持disqus实时评论功能，如有错误或者建议，欢迎在下方评论区提出，共同探讨。  
