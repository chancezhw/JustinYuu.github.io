---
layout: post
title: "Sequence Model Chapter 1"
description: "Notes"
categories: [Sequence-Model]
tags: [Python]
redirect_from:
  - /2019/07/05/
---

# Sequence Model Chapter 1  

Sequence Model这门课程是整个deeplearning.ai系列课程的最后一门，主要介绍循环神经网络(RNN)和自然语言处理(nlp)领域的相关知识。复旦的NLP非常的强，但是很遗憾读研的时候方向是cv。虽然cv也不错，并且以后应该和nlp也没什么联系了，但是简单接触一下nlp和rnn也是有好处的。   
本次MOOC的内容看起来还是很多的，特别是编程作业，第一周就有三个作业，所以一天一章感觉还是不太现实，争取两天一章吧。  

## Recurrrent Neural Network  

课程直接进入正题，介绍循环神经网络RNN。所谓Sequence Model，即序列模型，指的是处理以序列形式呈现的数据集，比如演讲识别、音乐生成、DNA序列分析，感知分类，运动识别等等，通过监督学习进行训练。这些例子中有的X是序列数据，有的Y是序列数据，有的XY都是序列数据，应用的形式还是挺多样的。  

首先和其他神经网络一样，先介绍notation，也就是命名方法。这里我必须进行举例： x: Harry Potter and Hermione Granger invented a new spell，这是哈利波特小说中的一句话，即哈利波特和赫敏格兰杰发明了一个新咒语。这个时候，输出的数据就被拆分成了9个序列元素（1个单词视为一个序列元素），用x<sup><1></sup>表示第一个元素，以此类推，用x<sup><t></sup>表示第t个元素，用y<sup><t></sup>表示第y个输出。  

我们会发现，一个输入数据，即一个序列模型会有很多序列元素，那么每次输入可能会有很多个输入数据，也就是多个序列模型。为了更明确的表示第i个输入数据的第t个序列元素，我们用x<sup>(i)<t></sup>来表示。此外，我们用T<sub>x</sub>和T<sub>y</sub>来表示x和y的序列长度，T<sub>x</sub><sup>(i)</sup>表示第i个输入数据的序列长度。  

NLP中将所有的单词建立了一个词汇表，称为Vocabulary，不同的词汇表大小也不同，那么任何词汇都以One-hot向量的形式存在，其大小与词汇表相同，记词汇表中出现该词汇的位置为x，那么在One-hot向量中的x位置将值置为1，其他位置都为0，类似于一个mask。如果词语不存在词汇表中，就用一个自己定义的代表未知单词的符号代替，比如<UNK>。  

那么RNN出现的原因是什么呢？也就是传统的神经网络为什么没法处理序列模型呢？原因有3:1.参数太多了，过多的单词和One-hot的维数过大导致参数量极其巨大。2.Tx和Ty数量不固定导致输入和输出的长度不确定且不同。3.不同单词在不同位置的性质不同，有可能本来是动词，又变成了名词；本来是这个意思到了另一个地方又是另一个意思，这就导致了同一个单词的feature无法确定且无法被共享。这是普通神经网络最严重的的问题，它直接导致了普通神经网络无法很好的训练序列模型，这也就是RNN为什么必不可少的三大原因。  

那么终于到了介绍RNN的时候。RNN将第一个输入的单词输入到第一个隐藏层中，第二个单词输入到第二个隐藏层中，但是关键点在于第二个隐藏层会应用第一个隐藏层训练的结果，也就是将前一层的激活函数加入计算。总结下来，循环神经网络将前面所有的激活函数值和当前序列元素一起进行运算，也就是说每个序列元素(除去第一个)，其输入的信息都来源于前面的所有元素。对比一下以前学习的CNN和普通神经网络，不同的输入数据之间是独立的，每一个具体的输出都是由一部分特定的输入决定的，这也就是RNN和其他神经网络的区别所在。那么RNN也是有局限的，由于是从左到右依次扫描，所以他检测不到后面的词语，后面的词语也就对前面的计算没法造成影响。吴恩达举了一个例子：Teddy bears are on sale,这个时候RNN就无法判断Teddy是不是一个人名，所以这就需要更高阶的知识BRNN，这在后面会讲到。  

循环神经网络从左到右依次扫描数据，并且每一(时间)步所用的参数是共享的。我们用W<sub>ax</sub>代表输入x1到第一层隐藏层的控制参数。W的下标有两个字母，第一个指的是输出变量的名字，第二个指的是与该参数相乘的变量名称，W<sub>ax</sub>就代表该参数与x相乘，输出形式为a，那么同样W<sub>aa</sub>就代表该参数与上一层的a相乘，以该层的激活函数值a的形式输出。  

那么接下来研究RNN的正向传播。在第一个时间步，a<sup><1></sup> = g<sub>1</sub>(W<sub>aa</sub>a<sup><0></sup> + W<sub>ax</sub>x<sup><1></sup> + b<sub>a</sub>), y<sup><1></sup> = g<sub>2</sub>(W<sub>ya</sub>a<sup><1></sup> + b<sub>y</sub>)。g<sub>1</sub>一般选择tanh和RELU，g<sub>2</sub>使用softmax或者sigmoid。  
  
以上就是RNN的正向传播，但是为了简化表示，一般分块矩阵的方式来表达，将Waa和Wax水平叠加形成矩阵Wa=\[Waa:Wax]，同时将a和x竖直叠加称为一个新的矩阵，我假设为B，那么正向传播可以直接简化为a<sup>\<t></sup> = W<sub>a</sub><sup>\<t-1></sup> * B<sup>\<t-1></sup>。  

接下来介绍RNN的反向传播。对于CNN的教程，我唯一觉得美中不足的是CNN的反向传播没有在视频中进行介绍，只是在编程作业中提到了一下，但是我看的还是似懂非懂。我认为反向传播是很重要的一环，虽然框架可以帮助我直接完成这一步，但是了解其中的细节还是非常有用的。其实RNN的反向传播并不难理解，在每一个时间步，RNN都会给出一个预测值y-hat，那么将最终的损失函数定义为标准的逻辑回归损失形式，即L(y<sub>hat</sub>,y) = -ylogy<sub>hat</sub> - (1-y)log(1-y<sub>hat</sub>)，这是每层的损失函数，最终的损失函数是所有损失函数的和。以上还是正向传播的领域，在反向传播阶段，从最后的L依次反向传播到每一层L，依次反向传播到每一层的y和a，同时最后的L传播到最后的y和a，然后从右到左传播到每一层的y和每一层的a。也就是说反向传播主要是从右到左进行的，由于每一步都是一个时间步，所以看起来像是穿越时间的回溯，所以又称为穿越时间的反向传播(Backpropagation through time)。这两个视频讲的还不是很细致，因此我引用了刘建平大神的博客[RNN的前向传播和反向传播](https://www.cnblogs.com/pinard/p/6509630.html)。    

由于在RNN中Tx并不是总等于Ty，因此就出现了不同的RNN结构，比如many to many(standard RNN version),many to one(only one output in the final timestep), one to one(standard neural network), Seq2Seq(many to many while Tx != Ty, use encoder and decoder to split input timestep and output timestep)。  

语言模型（Language Model） 是自然语言处理（NLP）中最基础和最重要的任务之一，同时也是RNN非常擅长的领域。语言模型在演讲识别中可以帮助机器识别和区分两种同音的句子，通过概率来确定所识别的句子具体是什么。换句话讲，语言模型的任务就是输出输入的序列模型以某些单词表示的概率。要建立语言模型，一般需要大型的语料库(corpus)作为训练集，接着将语料库符号化，也就是将所有的单词变成one-hot向量。在这一步中一般需要在其中增加一些标记，比如标记句子结束的标记EOS，未知单词UNK，或者标记输入的标点符号等等。  

那么接下来就用RNN对序列进行可能性建模。对于单词的识别，吴恩达这里使用了softmax作为激活函数，在第t个时间步，结合前面t-1个数据，预测第t个时间步对应的单词是什么。而预测成功的关键，也就是RNN的特性，即每一步的预测都会结合前面给出的所有数据来决定。最后将所有单词的概率相乘，得到的结果就是某个特定句子的概率。  

在上门课程对CNN的学习中，我们学过如何可视化的观察CNN的每一层到底在执行什么工作，RNN也是如此。对于RNN，我们可以生成一种叫做取样产生新序列的方法，对RNN进行可视化的直观分析。在我们之前所学习的序列模型中，序列都是给定的，但是现在我们假设没有给定的序列，每一步所需要的单词都由上一步softmax输出的概率随机的取样生成。也就是说每一层的输入都是由上一层的输出概率选择而来的，这样不断的生成了一整个序列。这是单词级别的语言模型，同样的，我们也可以生成字符级别的语言模型。字符模型的优点是不存在未知字符的问题，但是缺点是序列长度太长，这导致了训练成本过高。那么这个序列有什么作用呢？如果我们在新闻稿数据集上训练，那么可以自动生成出一段模拟的新闻稿；同样的，如果在莎士比亚文集上训练，那么就可以训练出莎士比亚作品风格的诗句。  

RNN也会出现梯度消失的情况，那么如何解决RNN的梯度消失呢？我们都知道，英语中的从句可能会很长，那么如果从句位于主语和谓语之间，谓语要根据主语的单复数确定，而从句又很长。这就导致了语言中对长距离有依赖性，而RNN无法捕捉到这种依赖，所以对这种问题的处理并不好。这是因为，RNN的反向传播很难将输出影响到很浅的层面上，大多数的影响都来自于邻近的单元，这就导致距离很远的时候梯度会小到接近为0，无法捕捉，这就是RNN的梯度消失现象。这种现象很好理解，在其他的网络中，之前的数据根本就没有任何影响，而在RNN中，每一层都将前面的数据处理结果添加进去，那么这就势必导致第一层的影响会越来越小，因为他的影响会随着层数的增加而变得权重越来越小，新增加的那层的数据权重才是最大的。无语的是这节只介绍了梯度消失会出现以及出现的原因，并没有介绍怎么解决……  

除了梯度消失，RNN还会出现梯度爆炸。梯度爆炸的影响是灾难性的，它会是神经网络的参数变得非常大，从而使得整个神经网络陷入混乱。但是梯度爆炸很容易被发现，如果数值溢出或者出现NaN，那么很有可能出现了梯度爆炸。这时候可以用梯度剪切（gradiernt clipping）处理这一问题，具体的处理方式是当梯度过大直到超过某个阈值时缩放梯度(...)，这也在这系列的之前课程中学习过。  

接下来学习的是GRU(Gated Recurrent Unit,GRU),它改进了基础的RNN结构，使得RNN能够更好的捕捉长距离依赖，从而解决长距离消失问题。我必须要吐槽一下，这个视频长达17分钟，居然没有中文字幕:(。首先GRU引入一个称之为memory cell记忆单元的参数c，其作用是记忆某些值，在GRU中一般与当层的a值相同。也许在某些步之后，我们可能会再次更新c的值，那么这时候就需要设置一个新的参数c~来代表用于替换c的候选值，假设激活函数是tanh，那么c<sup>\<t></sup>~ = tanh(W<sub>c</sub>\[c<sup>\<t-1></sup>,x<sup>\<t></sup>]+b<sub>c</sub>)。这时候GRU的G就要出场了，用Γ<sub>u</sub>表示Gate，那么Γ<sub>u</sub> =σ(W<sub>u</sub>\[c<sup>\<t-1></sup>,x<sup>\<t></sup>]+b<sub>u</sub>),这里u代表update。在电工电子中我们接触过门，那个时候门电路中的门起到逻辑判断的作用，在这里也是这样。由于函数是sigmoid，大部分函数值会分化为趋近于0和1，这时候为了方便理解和操作，我们直接就认定取值只有0和1。那么通过这个函数，也就是这个gate，我们就能够控制何时更新c<sup>\<t></sup>。具体的函数形式是 c<sup>\<t></sup> = Γ<sub>u</sub> ×  c~<sup>\<t></sup> + (1- Γ<sub>u</sub>) × c<sup>\<t-1></sup>。GRU的一个好处是只有当算法确定当前的c已经不被需要了，sigmoid才会被置为1，也就是说sigmoid大部分时间都会置为0，这就导致了 Γ<sub>u</sub>始终接近0，最后导致c基本不变，也就导致了梯度很难消失。这个地方吴恩达没有细讲，但是在我的理解来看，回顾前面梯度消失的原因，正是因为长距离依赖导致神经网络需要不断的通过反向传播获得离当前层很远的层的导数值，这就导致很有可能将梯度减小成0，那么现在有c作为记忆单元，加入了一个门作为更新c与否的判准，那么大部分情况下c就无需进入非线性运算的环节，那么在反向传播的时候，由于在大多数时间步下c都没有进行过非线性运算，所以反向传播就相对来讲容易的多。比如如果有100个时间步，在没有GRU的时候做了100次非线性运算，但是如果在GRU控制下c只更新了10次，那么c就只做了10次非线性运算。值得注意的是，在GRU中c就是a，那么就直接对c进行反向传播就可以，那么10次非线性运算的反向传播肯定比100次非线性运算的反向传播梯度消失的可能性小的多，这也就是为什么GRU可以解决梯度消失问题。  

值得一提的是这里用的GRU是简化版，完整版的GRU多了一个相关门，在计算c~<sup>\<t></sup>的时候与c<sup>\<t-1></sup>相乘。这个门的值表示c~<sup>\<t></sup>和c<sup>\<t-1></sup>的相关性。所以完整的GRU里面有两个门：更新门和相关性门。  

GRU是LSTM的变体，我不太懂为什么吴恩达大大要先介绍GRU再介绍LSTM，LSTM是Long Short Term Memory的简称，它比GRU功能更强大，也更为泛化。这里放上一张很重要的图片：GRU和LSTM的对比图![GRU VS LSTM](/images/Neural Network and Deep Learning/LSTM_GRU.jpg)  
在LSTM中，c和a的值并不相同。此外，从图中可以看出LSTM将使用两种不同的门控来控制c~<sup>\<t></sup>和c<sup>\<t-1></sup>,分别是Γ<sub>u</sub>更新门和Γ<sub>f</sub>遗忘门，此外去掉了GRU的相关门，并引用了一个输出门Γ<sub>o</sub>来计算a<sup>\<t></sup>。这是我觉得描述LSTM非常形象的一张结构图:
![LSTM](/images/Neural Network and Deep Learning/LSTM.jpg)。  
多个LSTM的单元串联在一起，共同组成了LSTM网络，其中c以一条直线贯穿下去，从而保证了解决长距离依赖问题和避免梯度消失。  

GRU和LSTM各有千秋，不同的场景可以使用不同的模型，一般来讲，GRU比较简单，适合构建更大的更复杂的网络，而LSTM更为复杂和强大，更为完整，更适合作为默认的测试方式进行测试。  

在本周课程的前面介绍过BRNN，也就是双向RNN，即从两端开始同时进行的RNN模型。传统的单向RNN无法解决的是具体单词的语义需要联系后文才可以确定的情况，这时候我们就需要从双向同时进行处理。简单来讲BRNN是在RNN的正向循环单元的前提下增加了同等规模的镜像反向循环单元。这种网络定义了一个无环图，每一步都需要考虑前向激活函数和后向激活函数，那么我们就可以很明显的发现BRNN需要等一整句话全部说完才可以进行处理，这也就造成了BRNN的实时性能不够良好。此外，BRNN不一定全部都是标准RNN模块，有时也会增加一部分LSTM或者GRU等其他模块，从而构建一个真正成熟的识别系统。  

终于要结束本周漫长的视频部分了。最后一节介绍的是深度RNN，和CNN等其他神经网络不同的是，RNN是横向的时间序列，那么就导致RNN只是长度长，而不是深度深。对于RNN来说，也是可以进行深度的堆叠的，一般情况下堆叠3层就已经比较深了，三层输入序列平行输入，同时进行训练，同一个输入数据，也就是一个序列元素输入到3层RNN序列进行训练，先进入第一层，再进入第二层第三层依次训练，那么第二层的输入就同时来自于第一层的激活函数a和本层上一个时间步的激活函数a'，第三层同理。深度RNN和可以采取BRNN的形式，那么显然深度BRNN所需要的训练代价更大。    

## Programming Assignments  

---
本博客支持disqus实时评论功能，如有错误或者建议，欢迎在下方评论区提出，共同探讨。  
