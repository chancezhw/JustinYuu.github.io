---
layout: post
title: "Sequence Model Chapter 1"
description: "Notes"
categories: [Sequence-Model]
tags: [Python]
redirect_from:
  - /2019/07/05/
---

# Sequence Model Chapter 1  

Sequence Model这门课程是整个deeplearning.ai系列课程的最后一门，主要介绍循环神经网络(RNN)和自然语言处理(nlp)领域的相关知识。复旦的NLP非常的强，但是很遗憾读研的时候方向是cv。虽然cv也不错，并且以后应该和nlp也没什么联系了，但是简单接触一下nlp和rnn也是有好处的。   
本次MOOC的内容看起来还是很多的，特别是编程作业，第一周就有三个作业，所以一天一章感觉还是不太现实，争取两天一章吧。  

## Recurrrent Neural Network  

课程直接进入正题，介绍循环神经网络RNN。所谓Sequence Model，即序列模型，指的是处理以序列形式呈现的数据集，比如演讲识别、音乐生成、DNA序列分析，感知分类，运动识别等等，通过监督学习进行训练。这些例子中有的X是序列数据，有的Y是序列数据，有的XY都是序列数据，应用的形式还是挺多样的。  

首先和其他神经网络一样，先介绍notation，也就是命名方法。这里我必须进行举例： x: Harry Potter and Hermione Granger invented a new spell，这是哈利波特小说中的一句话，即哈利波特和赫敏格兰杰发明了一个新咒语。这个时候，输出的数据就被拆分成了9个序列元素（1个单词视为一个序列元素），用x<sup><1></sup>表示第一个元素，以此类推，用x<sup><t></sup>表示第t个元素，用y<sup><t></sup>表示第y个输出。  

我们会发现，一个输入数据，即一个序列模型会有很多序列元素，那么每次输入可能会有很多个输入数据，也就是多个序列模型。为了更明确的表示第i个输入数据的第t个序列元素，我们用x<sup>(i)<t></sup>来表示。此外，我们用T<sub>x</sub>和T<sub>y</sub>来表示x和y的序列长度，T<sub>x</sub><sup>(i)</sup>表示第i个输入数据的序列长度。  

NLP中将所有的单词建立了一个词汇表，称为Vocabulary，不同的词汇表大小也不同，那么任何词汇都以One-hot向量的形式存在，其大小与词汇表相同，记词汇表中出现该词汇的位置为x，那么在One-hot向量中的x位置将值置为1，其他位置都为0，类似于一个mask。如果词语不存在词汇表中，就用一个自己定义的代表未知单词的符号代替，比如<UNK>。  

那么RNN出现的原因是什么呢？也就是传统的神经网络为什么没法处理序列模型呢？原因有3:1.参数太多了，过多的单词和One-hot的维数过大导致参数量极其巨大。2.Tx和Ty数量不固定导致输入和输出的长度不确定且不同。3.不同单词在不同位置的性质不同，有可能本来是动词，又变成了名词；本来是这个意思到了另一个地方又是另一个意思，这就导致了同一个单词的feature无法确定且无法被共享。这是普通神经网络最严重的的问题，它直接导致了普通神经网络无法很好的训练序列模型，这也就是RNN为什么必不可少的三大原因。  

那么终于到了介绍RNN的时候。RNN将第一个输入的单词输入到第一个隐藏层中，第二个单词输入到第二个隐藏层中，但是关键点在于第二个隐藏层会应用第一个隐藏层训练的结果，也就是将前一层的激活函数加入计算。总结下来，循环神经网络将前面所有的激活函数值和当前序列元素一起进行运算，也就是说每个序列元素(除去第一个)，其输入的信息都来源于前面的所有元素。对比一下以前学习的CNN和普通神经网络，不同的输入数据之间是独立的，每一个具体的输出都是由一部分特定的输入决定的，这也就是RNN和其他神经网络的区别所在。那么RNN也是有局限的，由于是从左到右依次扫描，所以他检测不到后面的词语，后面的词语也就对前面的计算没法造成影响。吴恩达举了一个例子：Teddy bears are on sale,这个时候RNN就无法判断Teddy是不是一个人名，所以这就需要更高阶的知识BRNN，这在后面会讲到。  

循环神经网络从左到右依次扫描数据，并且每一(时间)步所用的参数是共享的。我们用W<sub>ax</sub>代表输入x1到第一层隐藏层的控制参数。W的下标有两个字母，第一个指的是输出变量的名字，第二个指的是与该参数相乘的变量名称，W<sub>ax</sub>就代表该参数与x相乘，输出形式为a，那么同样W<sub>aa</sub>就代表该参数与上一层的a相乘，以该层的激活函数值a的形式输出。  

那么接下来研究RNN的正向传播。在第一个时间步，a<sup><1></sup> = g<sub>1</sub>(W<sub>aa</sub>a<sup><0></sup> + W<sub>ax</sub>x<sup><1></sup> + b<sub>a</sub>), y<sup><1></sup> = g<sub>2</sub>(W<sub>ya</sub>a<sup><1></sup> + b<sub>y</sub>)。g<sub>1</sub>一般选择tanh和RELU，g<sub>2</sub>使用softmax或者sigmoid。  
  
以上就是RNN的正向传播，但是为了简化表示，一般分块矩阵的方式来表达，将Waa和Wax水平叠加形成矩阵Wa=\[Waa:Wax]，同时将a和x竖直叠加称为一个新的矩阵，我假设为B，那么正向传播可以直接简化为a<sup>\<t></sup> = W<sub>a</sub><sup>\<t-1></sup> * B<sup>\<t-1></sup>。  

接下来介绍RNN的反向传播。对于CNN的教程，我唯一觉得美中不足的是CNN的反向传播没有在视频中进行介绍，只是在编程作业中提到了一下，但是我看的还是似懂非懂。我认为反向传播是很重要的一环，虽然框架可以帮助我直接完成这一步，但是了解其中的细节还是非常有用的。其实RNN的反向传播并不难理解，在每一个时间步，RNN都会给出一个预测值y-hat，那么将最终的损失函数定义为标准的逻辑回归损失形式，即L(y<sub>hat</sub>,y) = -ylogy<sub>hat</sub> - (1-y)log(1-y<sub>hat</sub>)，这是每层的损失函数，最终的损失函数是所有损失函数的和。以上还是正向传播的领域，在反向传播阶段，从最后的L依次反向传播到每一层L，依次反向传播到每一层的y和a，同时最后的L传播到最后的y和a，然后从右到左传播到每一层的y和每一层的a。也就是说反向传播主要是从右到左进行的，由于每一步都是一个时间步，所以看起来像是穿越时间的回溯，所以又称为穿越时间的反向传播(Backpropagation through time)。这两个视频讲的还不是很细致，因此我引用了刘建平大神的博客[RNN的前向传播和反向传播](https://www.cnblogs.com/pinard/p/6509630.html)。    



---
本博客支持disqus实时评论功能，如有错误或者建议，欢迎在下方评论区提出，共同探讨。  
