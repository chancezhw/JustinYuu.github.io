---
layout: post
title: "Sequence Model Chapter 1"
description: "Notes"
categories: [Sequence-Model]
tags: [Python]
redirect_from:
  - /2019/07/05/
---

# Sequence Model Chapter 1  

Sequence Model这门课程是整个deeplearning.ai系列课程的最后一门，主要介绍循环神经网络(RNN)和自然语言处理(nlp)领域的相关知识。复旦的NLP非常的强，但是很遗憾读研的时候方向是cv。虽然cv也不错，并且以后应该和nlp也没什么联系了，但是简单接触一下nlp和rnn也是有好处的。   
本次MOOC的内容看起来还是很多的，特别是编程作业，第一周就有三个作业，所以一天一章感觉还是不太现实，争取两天一章吧。  

## Recurrrent Neural Network  

课程直接进入正题，介绍循环神经网络RNN。所谓Sequence Model，即序列模型，指的是处理以序列形式呈现的数据集，比如演讲识别、音乐生成、DNA序列分析，感知分类，运动识别等等，通过监督学习进行训练。这些例子中有的X是序列数据，有的Y是序列数据，有的XY都是序列数据，应用的形式还是挺多样的。  

首先和其他神经网络一样，先介绍notation，也就是命名方法。这里我必须进行举例： x: Harry Potter and Hermione Granger invented a new spell，这是哈利波特小说中的一句话，即哈利波特和赫敏格兰杰发明了一个新咒语。这个时候，输出的数据就被拆分成了9个序列元素（1个单词视为一个序列元素），用x<sup><1></sup>表示第一个元素，以此类推，用x<sup><t></sup>表示第t个元素，用y<sup><t></sup>表示第y个输出。  

我们会发现，一个输入数据，即一个序列模型会有很多序列元素，那么每次输入可能会有很多个输入数据，也就是多个序列模型。为了更明确的表示第i个输入数据的第t个序列元素，我们用x<sup>(i)<t></sup>来表示。此外，我们用T<sub>x</sub>和T<sub>y</sub>来表示x和y的序列长度，T<sub>x</sub><sup>(i)</sup>表示第i个输入数据的序列长度。  

NLP中将所有的单词建立了一个词汇表，称为Vocabulary，不同的词汇表大小也不同，那么任何词汇都以One-hot向量的形式存在，其大小与词汇表相同，记词汇表中出现该词汇的位置为x，那么在One-hot向量中的x位置将值置为1，其他位置都为0，类似于一个mask。如果词语不存在词汇表中，就用一个自己定义的代表未知单词的符号代替，比如<UNK>。  

那么RNN出现的原因是什么呢？也就是传统的神经网络为什么没法处理序列模型呢？原因有3:1.参数太多了，过多的单词和One-hot的维数过大导致参数量极其巨大。2.Tx和Ty数量不固定导致输入和输出的长度不确定且不同。3.不同单词在不同位置的性质不同，有可能本来是动词，又变成了名词；本来是这个意思到了另一个地方又是另一个意思，这就导致了同一个单词的feature无法确定且无法被共享。这是普通神经网络最严重的的问题，它直接导致了普通神经网络无法很好的训练序列模型，这也就是RNN为什么必不可少的三大原因。  

那么终于到了介绍RNN的时候。RNN将第一个输入的单词输入到第一个隐藏层中，第二个单词输入到第二个隐藏层中，但是关键点在于第二个隐藏层会应用第一个隐藏层训练的结果，也就是将前一层的激活函数加入计算。总结下来，循环神经网络将前面所有的激活函数值和当前序列元素一起进行运算，也就是说每个序列元素(除去第一个)，其输入的信息都来源于前面的所有元素。对比一下以前学习的CNN和普通神经网络，不同的输入数据之间是独立的，每一个具体的输出都是由一部分特定的输入决定的，这也就是RNN和其他神经网络的区别所在。那么RNN也是有局限的，由于是从左到右依次扫描，所以他检测不到后面的词语，后面的词语也就对前面的计算没法造成影响。吴恩达举了一个例子：Teddy bears are on sale,这个时候RNN就无法判断Teddy是不是一个人名，所以这就需要更高阶的知识BRNN，这在后面会讲到。  

循环神经网络从左到右依次扫描数据，并且每一(时间)步所用的参数是共享的。我们用W<sub>ax</sub>代表输入x1到第一层隐藏层的控制参数。W的下标有两个字母，第一个指的是输出变量的名字，第二个指的是与该参数相乘的变量名称，W<sub>ax</sub>就代表该参数与x相乘，输出形式为a，那么同样W<sub>aa</sub>就代表该参数与上一层的a相乘，以该层的激活函数值a的形式输出。  

那么接下来研究RNN的正向传播。在第一个时间步，a<sup><1></sup> = g<sub>1</sub>(W<sub>aa</sub>a<sup><0></sup> + W<sub>ax</sub>x<sup><1></sup> + b<sub>a</sub>), y<sup><1></sup> = g<sub>2</sub>(W<sub>ya</sub>a<sup><1></sup> + b<sub>y</sub>)。g<sub>1</sub>一般选择tanh和RELU，g<sub>2</sub>使用softmax或者sigmoid。  
  
以上就是RNN的正向传播，但是为了简化表示，一般分块矩阵的方式来表达，将Waa和Wax水平叠加形成矩阵Wa=\[Waa:Wax]，同时将a和x竖直叠加称为一个新的矩阵，我假设为B，那么正向传播可以直接简化为a<sup>\<t></sup> = W<sub>a</sub><sup>\<t-1></sup> * B<sup>\<t-1></sup>。  

接下来介绍RNN的反向传播。对于CNN的教程，我唯一觉得美中不足的是CNN的反向传播没有在视频中进行介绍，只是在编程作业中提到了一下，但是我看的还是似懂非懂。我认为反向传播是很重要的一环，虽然框架可以帮助我直接完成这一步，但是了解其中的细节还是非常有用的。其实RNN的反向传播并不难理解，在每一个时间步，RNN都会给出一个预测值y-hat，那么将最终的损失函数定义为标准的逻辑回归损失形式，即L(y<sub>hat</sub>,y) = -ylogy<sub>hat</sub> - (1-y)log(1-y<sub>hat</sub>)，这是每层的损失函数，最终的损失函数是所有损失函数的和。以上还是正向传播的领域，在反向传播阶段，从最后的L依次反向传播到每一层L，依次反向传播到每一层的y和a，同时最后的L传播到最后的y和a，然后从右到左传播到每一层的y和每一层的a。也就是说反向传播主要是从右到左进行的，由于每一步都是一个时间步，所以看起来像是穿越时间的回溯，所以又称为穿越时间的反向传播(Backpropagation through time)。这两个视频讲的还不是很细致，因此我引用了刘建平大神的博客[RNN的前向传播和反向传播](https://www.cnblogs.com/pinard/p/6509630.html)。    

由于在RNN中Tx并不是总等于Ty，因此就出现了不同的RNN结构，比如many to many(standard RNN version),many to one(only one output in the final timestep), one to one(standard neural network), Seq2Seq(many to many while Tx != Ty, use encoder and decoder to split input timestep and output timestep)。  

语言模型（Language Model） 是自然语言处理（NLP）中最基础和最重要的任务之一，同时也是RNN非常擅长的领域。语言模型在演讲识别中可以帮助机器识别和区分两种同音的句子，通过概率来确定所识别的句子具体是什么。换句话讲，语言模型的任务就是输出输入的序列模型以某些单词表示的概率。要建立语言模型，一般需要大型的语料库(corpus)作为训练集，接着将语料库符号化，也就是将所有的单词变成one-hot向量。在这一步中一般需要在其中增加一些标记，比如标记句子结束的标记EOS，未知单词UNK，或者标记输入的标点符号等等。  

那么接下来就用RNN对序列进行可能性建模。对于单词的识别，吴恩达这里使用了softmax作为激活函数，在第t个时间步，结合前面t-1个数据，预测第t个时间步对应的单词是什么。而预测成功的关键，也就是RNN的特性，即每一步的预测都会结合前面给出的所有数据来决定。最后将所有单词的概率相乘，得到的结果就是某个特定句子的概率。  

在上门课程对CNN的学习中，我们学过如何可视化的观察CNN的每一层到底在执行什么工作，RNN也是如此。对于RNN，我们可以生成一种叫做取样产生新序列的方法，对RNN进行可视化的直观分析。在我们之前所学习的序列模型中，序列都是给定的，但是现在我们假设没有给定的序列，每一步所需要的单词都由上一步softmax输出的概率随机的取样生成。也就是说每一层的输入都是由上一层的输出概率选择而来的，这样不断的生成了一整个序列。这是单词级别的语言模型，同样的，我们也可以生成字符级别的语言模型。字符模型的优点是不存在未知字符的问题，但是缺点是序列长度太长，这导致了训练成本过高。那么这个序列有什么作用呢？如果我们在新闻稿数据集上训练，那么可以自动生成出一段模拟的新闻稿；同样的，如果在莎士比亚文集上训练，那么就可以训练出莎士比亚作品风格的诗句。  

RNN也会出现梯度消失的情况，那么如何解决RNN的梯度消失呢？我们都知道，英语中的从句可能会很长，那么如果从句位于主语和谓语之间，谓语要根据主语的单复数确定，而从句又很长。这就导致了语言中对长距离有依赖性，而RNN无法捕捉到这种依赖，所以对这种问题的处理并不好。这是因为，RNN的反向传播很难将输出影响到很浅的层面上，大多数的影响都来自于邻近的单元，这就导致距离很远的时候梯度会小到接近为0，无法捕捉，这就是RNN的梯度消失现象。这种现象很好理解，在其他的网络中，之前的数据根本就没有任何影响，而在RNN中，每一层都将前面的数据处理结果添加进去，那么这就势必导致第一层的影响会越来越小，因为他的影响会随着层数的增加而变得权重越来越小，新增加的那层的数据权重才是最大的。无语的是这节只介绍了梯度消失会出现以及出现的原因，并没有介绍怎么解决……  

除了梯度消失，RNN还会出现梯度爆炸。梯度爆炸的影响是灾难性的，它会是神经网络的参数变得非常大，从而使得整个神经网络陷入混乱。但是梯度爆炸很容易被发现，如果数值溢出或者出现NaN，那么很有可能出现了梯度爆炸。这时候可以用梯度剪切（gradiernt clipping）处理这一问题，具体的处理方式是当梯度过大直到超过某个阈值时缩放梯度(...)，这也在这系列的之前课程中学习过。  

接下来学习的是GRU(Gated Recurrent Unit,GRU),它改进了基础的RNN结构，使得RNN能够更好的捕捉长距离依赖，从而解决长距离消失问题。我必须要吐槽一下，这个视频长达17分钟，居然没有中文字幕:(。




---
本博客支持disqus实时评论功能，如有错误或者建议，欢迎在下方评论区提出，共同探讨。  
