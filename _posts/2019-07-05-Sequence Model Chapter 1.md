---
layout: post
title: "Sequence Model Chapter 1"
description: "Notes"
categories: [Sequence-Model]
tags: [Python]
redirect_from:
  - /2019/07/05/
---

# Sequence Model Chapter 1  

Sequence Model这门课程是整个deeplearning.ai系列课程的最后一门，主要介绍循环神经网络(RNN)和自然语言处理(nlp)领域的相关知识。复旦的NLP非常的强，但是很遗憾读研的时候方向是cv。虽然cv也不错，并且以后应该和nlp也没什么联系了，但是简单接触一下nlp和rnn也是有好处的。   
本次MOOC的内容看起来还是很多的，特别是编程作业，第一周就有三个作业，所以一天一章感觉还是不太现实，争取两天一章吧。  

## Recurrrent Neural Network  

课程直接进入正题，介绍循环神经网络RNN。所谓Sequence Model，即序列模型，指的是处理以序列形式呈现的数据集，比如演讲识别、音乐生成、DNA序列分析，感知分类，运动识别等等，通过监督学习进行训练。这些例子中有的X是序列数据，有的Y是序列数据，有的XY都是序列数据，应用的形式还是挺多样的。  

首先和其他神经网络一样，先介绍notation，也就是命名方法。这里我必须进行举例： x: Harry Potter and Hermione Granger invented a new spell，这是哈利波特小说中的一句话，即哈利波特和赫敏格兰杰发明了一个新咒语。这个时候，输出的数据就被拆分成了9个序列元素（1个单词视为一个序列元素），用x<sup><1></sup>表示第一个元素，以此类推，用x<sup><t></sup>表示第t个元素，用y<sup><t></sup>表示第y个输出。  

我们会发现，一个输入数据，即一个序列模型会有很多序列元素，那么每次输入可能会有很多个输入数据，也就是多个序列模型。为了更明确的表示第i个输入数据的第t个序列元素，我们用x<sup>(i)<t></sup>来表示。此外，我们用T<sub>x</sub>和T<sub>y</sub>来表示x和y的序列长度，T<sub>x</sub><sup>(i)</sup>表示第i个输入数据的序列长度。  

NLP中将所有的单词建立了一个词汇表，称为Vocabulary，不同的词汇表大小也不同，那么任何词汇都以One-hot向量的形式存在，其大小与词汇表相同，记词汇表中出现该词汇的位置为x，那么在One-hot向量中的x位置将值置为1，其他位置都为0，类似于一个mask。如果词语不存在词汇表中，就用一个自己定义的代表未知单词的符号代替，比如<UNK>。  

那么RNN出现的原因是什么呢？也就是传统的神经网络为什么没法处理序列模型呢？原因有3:1.参数太多了，过多的单词和One-hot的维数过大导致参数量极其巨大。2.Tx和Ty数量不固定导致输入和输出的长度不确定且不同。3.不同单词在不同位置的性质不同，有可能本来是动词，又变成了名词；本来是这个意思到了另一个地方又是另一个意思，这就导致了同一个单词的feature无法确定且无法被共享。这是普通神经网络最严重的的问题，它直接导致了普通神经网络无法很好的训练序列模型，这也就是RNN为什么必不可少的三大原因。  

那么终于到了介绍RNN的时候。RNN将第一个输入的单词输入到第一个隐藏层中，第二个单词输入到第二个隐藏层中，但是关键点在于第二个隐藏层会应用第一个隐藏层训练的结果，也就是将前一层的激活函数加入计算。总结下来，循环神经网络将前面所有的激活函数值和当前序列元素一起进行运算，也就是说每个序列元素(除去第一个)，其输入的信息都来源于前面的所有元素。对比一下以前学习的CNN和普通神经网络，不同的输入数据之间是独立的，每一个具体的输出都是由一部分特定的输入决定的，这也就是RNN和其他神经网络的区别所在。那么RNN也是有局限的，由于是从左到右依次扫描，所以他检测不到后面的词语，后面的词语也就对前面的计算没法造成影响。吴恩达举了一个例子：Teddy bears are on sale,这个时候RNN就无法判断Teddy是不是一个人名，所以这就需要更高阶的知识BRNN，这在后面会讲到。  

循环神经网络从左到右依次扫描数据，并且每一(时间)步所用的参数是共享的。我们用W<sub>ax</sub>代表输入x1到第一层隐藏层的控制参数。W的下标有两个字母，第一个指的是输出变量的名字，第二个指的是与该参数相乘的变量名称，W<sub>ax</sub>就代表该参数与x相乘，输出形式为a，那么同样W<sub>aa</sub>就代表该参数与上一层的a相乘，以该层的激活函数值a的形式输出。  

那么接下来研究RNN的正向传播。在第一个时间步，a<sup><1></sup> = g<sub>1</sub>(W<sub>aa</sub>a<sup><0></sup> + W<sub>ax</sub>x<sup><1></sup> + b<sub>a</sub>), y<sup><1></sup> = g<sub>2</sub>(W<sub>ya</sub>a<sup><1></sup> + b<sub>y</sub>)。g<sub>1</sub>一般选择tanh和RELU，g<sub>2</sub>使用softmax或者sigmoid。  


---
本博客支持disqus实时评论功能，如有错误或者建议，欢迎在下方评论区提出，共同探讨。  
