---
layout: post
title: "Convolutional Neural Networks Chapter 1"
description: "Notes"
categories: [Convolutional Neural Networks]
tags: [Python]
redirect_from:
  - /2019/06/11/
---

# Convolutional Neural Networks Chapter 1  

这一门课程主要是学习大名鼎鼎的卷积神经网络，课程共有四周，第一周的章节名称为Foundations of Convolutional Neural Networks，顾名思义就是CNN的基础内容。

## Convolutional Neural Networks  

这一章的主要内容都放在了这一大节里，那么这一部分也就相应的笔记会多一些。首先吴恩达介绍了计算机视觉，这也是最近深度学习发展和应用的火热的领域之一。而对于像素非常高的图像来说，图像的参数规模是非常大的，这就导致很难得到足够的数据使得神经网络避免过拟合。因此这就需要运用卷积运算，这就是卷积神经网络的基础。要了解卷积运算，要先了解一个叫做边缘探测的东西，要探测边缘，需要建立一个称之为“过滤器”的矩阵，也称为“kernel(核)”，将原有数据与过滤器进行卷积运算，用* 表示。具体的卷积运算方式是从左上角开始，将与过滤器相同规模的矩阵与过滤器点乘后相加，得到一个数值，依次运算后得到结果矩阵，通过结果矩阵可以检测出图像中是否存在明显的边缘。那么不同的过滤器可以检测出不同的边缘，比如垂直和水平；也可以赋予不同位置不同的权重，比如sobel filter，赋予中间部位最重的权重，从而使得整体更稳定。人为定义的过滤器效果总是有限的，因此最好的方式是用反向传播技术来让机器自己学习过滤器的n×n个参数（n代表矩阵的规模），这将会在后面提到。  
接下来介绍的是补白，也叫填充，英文名称是Padding。由于卷积操作会使矩阵的规模变小，这样经过几次操作后我们的原有数据会被压缩的很小，不利于我们使用，这时就可以通过padding操作来缓解变小的速度。具体做法是在原矩阵的一周增添p圈空白的值，从而将n×n的规模扩大到(n+2p)×(n+2p),这样卷积操作后就可以保留原有的规模，p的具体取值可以通过计算得出，因此原有数据的规模一定可以得到保证。那么基于有无padding可以将卷积分为两种：valid和same convolutions。valid就是无padding，具体规模是n×n * f×f = n-f+1 × n-f+1。而same是有padding且填充到与原有规模相同，具体规模是(n+2p)×(n+2p) * f×f = (n+2p-f+1)×(n+2p-f+1)，若要使结果等于n×n，只需要使p=(f-1)/2即可。值得一提，计算机视觉领域中过滤器的规模绝大多数情况下是奇数。  
padding是组成基础卷积模块的一部分，而另一部分叫做带步长的卷积(Strided convolution)。带步长的卷积很好理解，就是与之前的一格一格计算相比，每隔固定的步长进行一次计算，这样重复计算的数据就会变少，但是卷积的结果规模也会变小。如果规模除以步长不为整数，那就将结果+1再取整数部分，+1是为了确保所有部分都参与了计算。值得注意的是，在数学领域和信号处理领域，卷积操作需要预先进行一步翻转，而在深度学习领域这一步被省略了，因为过滤器的值是我们自己确定的，那么就咩有必要对其进行翻转，直接确定翻转后的值就可以，所以准确的说在技术角度上，我们做的卷积操作应该被称为cross-correlation，即交叉关联，但是在深度学习领域大多直接称之为卷积。  
以上的卷积算法是在二维空间内进行的，但是对于三维空间的卷积算法还没有进行介绍。对于三维空间的卷积，显然过滤器也必须是三维的，且第三维度的值必须与原数据第三维度的数值相同。事实上两个三维的卷积之后还是会得到二维的结果，那么如果要同时检测不同类型的边缘，比如水平和数值的，那么就需要多个过滤器，数量用c'来代替。那么总结一下，若有n×n×c * f×f×c = (n-f+1) * (n-f+1）* c',c'代表过滤器的数量。值得注意的是为什么两个三维的卷积之后还会出现二维的结果，这是由于卷积核的维度和原数据的维度相同，然后对应层卷积之后将结果相加即可将三维变成二维。  
那么学过上述内容之后，便可以对单层卷积神经网络的工作原理进行总结了。首先，过滤器，也就是卷积核，其作用是用于线性操作计算，作用相当于普通神经网络中的w，那么w的参数数量就是过滤器的规模，再加上最后的一个偏置单元b，单层卷积神经网络所需要训练的参数总量为n×n×c'+1。之后的内容和普通神经网络一样，将结果导入激活函数中，CNN一般用ReLU，之后再将结果作为下一层的输入使用。  
对于整个CNN而言，一般来讲会有三种不同种类的层，分别是上面介绍的卷积层，未介绍的池化层和完全连接层。卷积神经网络通常使用池化层来减少展示量，来提高计算量、并使一些特征的检测功能更强大。对于池化层，有几种不同的池化方式，比如最大池化，将给定步长×步长区域内的最大值作为该区域的代表，然后将各区域的代表组成一组新的数据集从而使得原数据缩小。它背后的机理是对于一套数据集而言，较大的数据意味着检测到了一个特定的特征，比如垂直或者水平边缘，那么最大池化的作用就是检测所有地方中最明显的特征并将其保留下来。当然这种解释我听起来都有些牵强，所以可能并不是背后真正的机理，但是它就是有用，所以很多情况下都会使用最大池化来减少数据规模。此外还有平均池化，将给定范围内的数据取平均值即可得到平均池化结果。这两种池化方式相同的是超参数相同，分别是步长s和过滤器规模f，并不需要机器学习任何参数。一般来讲大多采用最大池化，只有在神经网络深度非常大的时候才会使用平均池化。  
经过n次卷积-池化-卷积-池化，最终得到了一个规模合适的数据集，这时将三维展开变为一维，作为一组初始权重，此时的规模一般来讲还是比较大，这时候就需要通过完全连接层在一维层面上继续压缩规模大小，经过一次或多次完全连接层，最终获得可以进入激活函数的初始数据。  
最后需要搞明白一件事情，为什么要用卷积，也就是卷积的作用在哪里。简要来讲，好处有两点：参数共享和连接的稀疏性。具体来讲，参数共享就是将一套参数，例如过滤器，用于所有数据的卷积中。稀疏连接性是指每一个输出只与图像（原数据）中的一部分数据有关，并不与所有数据都有直接的联系。  

## Programming Assignments  

本周共有两个编程作业，基本上完成了整个CNN的实现。对于CNN的反向传播我还没有太搞懂，其他的部分应该学习的差不多了。由于代码太长，因此就不和以前一样全部放在博客里，而是放在了我的github中的一个repository里面，按照每章进行分类存放。这周的链接在这里：[Convolution Neural Network Week 1](https://github.com/JustinYuu/Deeplearning-study/tree/master/Convolution%20Nerual%20Network)

## Heroes of Deep Learning  

本期采访的可谓是大牛，Yann LeCun，CNN的发明者，图灵奖获得者。采访的前半部分按照惯例，介绍了LeCun的深度学习过程和成就，奇怪的是这一周的这一部分出奇的长……这里就不再赘述了。值得敬佩的是他和Hinton一样，是从深度学习很冷门、没人研究的时候就开始研究人工智能，机器识别和深度学习，一直坚持到神经网络和深度学习大火。  
然后是对于初学者的建议部分，他觉得很好的现象是现在学习深度学习和神经网络的成本很低，很多框架可以使得人们更加轻易的学习。他对初学者的建议是要让自己变得有用。比如，贡献自己的力量给开源社区，或者去实现一些网上找不到的标准算法，并把他们贡献出来让别人去使用。我感觉LeCun对开源分享和开源社区非常看重，的确与其只是使用别人的劳动成果，不如主动分享自己的劳动果实，从而推动整个社区或者整个领域的发展。   

---
本博客支持disqus实时评论功能，如有错误或者建议，欢迎在下方评论区提出，共同探讨。  
