---
layout: post
title: "Convolutional Neural Networks Chapter 1"
description: "Notes"
categories: [Convolutional Neural Networks]
tags: [Python]
redirect_from:
  - /2019/06/11/
---

# Convolutional Neural Networks Chapter 1  

这一门课程主要是学习大名鼎鼎的卷积神经网络，课程共有四周，第一周的章节名称为Foundations of Convolutional Neural Networks，顾名思义就是CNN的基础内容。

## Convolutional Neural Networks  

这一章的主要内容都放在了这一大节里，那么这一部分也就相应的笔记会多一些。首先吴恩达介绍了计算机视觉，这也是最近深度学习发展和应用的火热的领域之一。而对于像素非常高的图像来说，图像的参数规模是非常大的，这就导致很难得到足够的数据使得神经网络避免过拟合。因此这就需要运用卷积运算，这就是卷积神经网络的基础。要了解卷积运算，要先了解一个叫做边缘探测的东西，要探测边缘，需要建立一个称之为“过滤器”的矩阵，也成为“kernel(核)”，将原有数据与过滤器进行卷积运算，用* 表示。具体的卷积运算方式是从左上角开始，将与过滤器相同规模的矩阵与过滤器点乘后相加，得到一个数值，依次运算后得到结果矩阵，通过结果矩阵可以检测出图像中是否存在明显的边缘。那么不同的过滤器可以检测出不同的边缘，比如垂直和水平；也可以赋予不同位置不同的权重，比如sobel filter，赋予中间部位最重的权重，从而使得整体更稳定。人为定义的过滤器效果总是有限的，因此最好的方式是用反向传播技术来让机器自己学习过滤器的n×n个参数（n代表矩阵的规模），这将会在后面提到。  
接下来介绍的是补白，也叫填充，英文名称是Padding。由于卷积操作会使矩阵的规模变小，这样经过几次操作后我们的原有数据会被压缩的很小，不利于我们使用，这时就可以通过padding操作来缓解变小的速度。具体做法是在原矩阵的一周增添p圈空白的值，从而将n×n的规模扩大到(n+2p)×(n+2p),这样卷积操作后就可以保留原有的规模，p的具体取值可以通过计算得出，因此原有数据的规模一定可以得到保证。那么基于有无padding可以将卷积分为两种：valid和same convolutions。valid就是无padding，具体规模是n×n * f×f = n-f+1 × n-f+1。而same是有padding且填充到与原有规模相同，具体规模是(n+2p)×(n+2p) * f×f = (n+2p-f+1)×(n+2p-f+1)，若要使结果等于n×n，只需要使p=(f-1)/2即可。值得一提，计算机视觉领域中过滤器的规模绝大多数情况下是奇数。  
padding是组成基础卷积模块的一部分，而另一部分叫做带步长的卷积(Strided convolution)。带步长的卷积很好理解，就是与之前的一格一格计算相比，每隔固定的步长进行一次计算，这样重复计算的数据就会变少，但是卷积的结果规模也会变小。如果规模除以步长不为整数，那就将结果+1再取整数部分，+1是为了确保所有部分都参与了计算。值得注意的是，在数学领域和信号处理领域，卷积操作需要预先进行一步翻转，而在深度学习领域这一步被省略了，因为过滤器的值是我们自己确定的，那么就咩有必要对其进行翻转，直接确定翻转后的值就可以，所以准确的说在技术角度上，我们做的卷积操作应该被称为cross-correlation，即交叉关联，但是在深度学习领域大多直接称之为卷积。  
以上的卷积算法是在二维空间内进行的，但是对于三维空间的卷积算法还没有进行介绍。对于三维空间的卷积，显然过滤器也必须是三维的，且第三维度的值必须与原数据第三维度的数值相同。事实上两个三维的卷积之后还是会得到二维的结果，那么如果要同时检测不同类型的边缘，比如水平和数值的，那么就需要多个过滤器，数量用c'来代替。那么总结一下，若有n×n×c * f×f×c = (n-f+1) * (n-f+1）* c',c'代表过滤器的数量。

---
本博客支持disqus实时评论功能，如有错误或者建议，欢迎在下方评论区提出，共同探讨。  
