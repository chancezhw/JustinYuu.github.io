---
layout: post
title: "Deeplearning by Udacity Chapter 3"
description: "Notes"
categories: [Deeplearning-by-Udacity]
tags: [Python]
redirect_from:
  - /2019/07/17/
---

# Deeplearning by Udacity Chapter 3  

第三章开始介绍深度学习和神经网络。  

这章秉承着这门课一贯的风格，简单的介绍了隐藏层和relu函数之后，就让你自己实现一遍用relu函数和隐藏层将简单的逻辑回归改成有隐藏层的非线性激活函数。此外还介绍了参数的数量，假设输入的数据X有N个参数，输出有K个参数，那么总共需要训练的参数为(N+1)*K个。  

介绍了RELU之后，Vincent开始介绍为什么需要引入非线性函数。吴恩达之前也讲过，目的当然是为了更好的进行分类，用更为复杂的函数更好的完成分类的任务。  

到了这章视频的中间部分，终于开始令人兴奋的数学内容环节了。我就知道他肯定逃不过讲链式求导，不然反向传播没法讲。果然不出我所料，他的反向传播过程没有任何的数学内容，我不明白他讲解链式求导的意义何在。全文只是在将我们需要正向传播+反向传播来得到更新的权重，但是这些只需要通过框架帮我们完成就可以了，并没有讲到原理。这个速成班的水分是真的大- -，不过我是为了实践来听的这门课，理论部分复习一下吴恩达大大的MOOC笔记就好~  

紧接着就来到了这门课丰富的实践环节，有了上一个笔记做基础，我有了一定的经验，并且这次编程任务需要用到TensorFlow了。这周的任务是建立一个全连接层，主要是练习内部隐藏层和relu的用法。

---
本博客支持disqus实时评论功能，如有错误或者建议，欢迎在下方评论区提出，共同探讨。  
