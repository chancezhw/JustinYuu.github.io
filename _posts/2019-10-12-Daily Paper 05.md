---
layout: post
title: "Daily Paper 05"
description: "Notes"
categories: [SR-SCD]
tags: [Paper]
redirect_from:
  - /2019/10/12/
---

# Daily Paper 05 -   Speaker Change Detection Using Features through A Neural Network Speaker Classifier  

## Introduction  

本来想这几天读几篇CV方向的经典文献的，结果突然来任务了，又多了两篇论文，那就再看一下。  

这篇是Interactive Intelligence公司在Intelligent Systems Conference 2017上发的一篇工程论文。从名称可以看出，这又是神经网络在SCD领域的一个分类，而根据摘要中的信息可以总结出，他们训练了一个神经网络分类器用于识别说话者，并探测说话者的改变，最终得到的结果是100%识别以及97%的hit rate，这个性能听起来也太牛x了。  

对于SCD问题来说作者将其分为两类，分别是retropective传统类型，以及更高要求的实时类型两种，前者一般用特定的检测算法和模型来训练，比如GMMs和HMMs等等，一般用BIC，KL等经典的量化方式来进行。而对于实时检测来说，难点是必须要通过有限的前置信息在可接受的短时间和短计算负担下内做出正确的检测，这也就使得其难度大大增加。很多学者也对实时检测的SCD进行了研究，这里我就不把研究都放上来了。  

这里作者提出了一个新系统，首先用一个前置神经网络来用原始的语音对话进行分类训练，然后通过0.5s,1s或者2s之间的相邻间隔的相似程度来判断是否发生了说话者的改变。在实际的测试中，虽然测试所使用的说话者有很多并不是训练集里已有的说话者，但是该系统仍然能够进行区分，这就使得作者可以利用该系统开发一些比较直接的距离metrics去捕捉说话者的改变，这个系统在TIMIT语料库作为数据集的表现很好，并与非神经网络的模型相比进步巨大。  

## Preparation  

作者开了一个section用来介绍数据的准备工作，这里使用了TIMIT语料库里的326名男性读者，而训练集里还有女性读者，以及有男性读者和女性读者的测试集在这里都不会使用到。对于每一个speaker来说，都会从3个类别中以5:3:2的比例选出10句话，分别称为SX，SI和SA，之后326名男性speaker会被分成200和126的两组。对于A组来说，在SX和SI类别的句子在不同的speaker之间是不同的，每个说话者的语句被组装成了大约有20秒钟的speech，作为神经网络分类器的训练集。SA中的句子是完全相同的，他们用来进行测试，所以需要消除所有可能会导致表现差异的变量。对于B组，speech是由不同的speaker读的语句拼接而成的，126个speaker的前一半，也就是前63个，会用SX和SI类别的句子组合成对话，他们的目的是为了找到SCD的最优阈值，而后面的63个也是用SX和SI拼接而成的句子，用来进行性能的测试，而由于其测试的内容正是之前寻找的阈值，所以数据集的组合方式必须完全一致，这里的实验数据组合还是相当科学的。  

在预处理阶段，要将最大的振幅scale为1，然后使用VAD把空白的部分给消灭掉，实验显示使用VAD处理过后表现会明显提升，特别是数据比较嘈杂的时候。这里使用了一个Giannakopoulo的算法来提高了VAD的性能，从而提升了VAD所捕获的音频部分的质量，具体的原理我也看不太懂，就不写了。这里的提取特征使用的是经典的MFCCs，使用了25ms的Hamming窗口，10s/hop，之后每个读者的数据将会各自均一化。此外，这些39维的MFCCs特征帧会连接起来，从而组成多个有重叠部分的更长的帧，这里大帧以10个小帧为单位组成，30ms/hop。  

## System  

接下来就是神经网络部分的架构了，总体来讲他们借鉴了吴恩达在机器学习这门课程中的建议……使用了将multi-class问题转变为K个二分问题的方式来解决，区别于一般的多分类问题的解决方式，即使用softmax和交叉熵损失函数，他们使用的是二元回归损失和分类函数，不过整体的区别也不是很大。这里还添加了正则项以解决overfitting的问题，整个神经网络只有1个隐藏层，是一个非常浅的神经网络，总体的规模是390:200:200，使用sigmoid作为激活函数，这里讲了一些很基础的反向传播的计算过程，就不详细记录了。最终会输出K维向量，使用一个argmax来找到对应的那一类，那么因为有多个帧，我们记为M，那么最终的结果就是由所有M个帧的对数和的形式得出，然后再用argmax取结果。如果M是1的话，那么最终的结果将完全取决于当前的帧，但是如果M是整个数据集的话，最终的结果将由整个数据集共同决定，那么显然M越大准确率越高，最终的实验结果表明M是整个数据集的时候，训练集和测试集的准确率都是100%，而当M是1的时候，测试集的正确率是79.65%。我们既然发现M只要足够大，总能够达到100%，但是我们又想尽可能的减少时间，那么我们就可以尝试一下实验或者计算得出准确率是100%的临界帧数，作者通过公式得出，在测试集中只需要11.59个帧就可以达到100%，时间是0.42s,这个时长还是比较短的。  

目前的神经网络的规模其实是经过实验得出的最优网络，其方式是通过建立grid进行实验，以1/10数据集规模的随机数据进行简单的训练，找出性能最好的网络结构。整个训练过程在两个循环过后准确率上升幅度不足0.1%的时候停止，所有的数据一批进入，200个speaker每个20s，i7CPU训练的情况下只需要1h，其实这么浅的网络训练起来肯定会很简单，训练成本其实不算大。  

## Experiment  

作者首先做了一个实验，用SA的句子选了5名训练集内的speaker和训练集外的speaker作对比，结果显示训练集内的speaker的检测结果还是比较靠谱的，不同frame的预测基本上都是一致的，而训练集外的speaker虽然没有那么明显，但是基本上还是会有很大一部分预测结果是相同的。那么我们之前提到过，有126位speaker被分成两组进行SCD最优阈值的查找和测试，计算两个间隔之间的距离，如果说话者相同的话，那么距离应该差距很小，或者为负值，反之则差距很大，且为正值，使用p范数作为距离计算公式，尝试了不同的p，最后发现还是欧氏距离最好用……此外还测试了其他的距离公式，比如巴氏距离，结果发现由于间隔之间的特征矩阵本身为M×K，而计算的时候将其压缩为K维向量，所以特征本身维度过小，导致协方差矩阵并不是正无穷，因此这种距离计算方式不适用，最后选了欧氏距离。  

那么确定了公式，作者就开始进行最佳阈值的寻找了，将距离d'作为样本，作者作出了在间隔为0.5s,1s,2s时距离差值与时长的变化曲线，结果显示正样本可以均匀的分布在整个间隔中，用高斯分布对正采样和负采样建模后得出，用贝叶斯决策边界作为最优阈值x\*最为合理。这里使用F1系数和错误率Pe作为SCD性能评判的标准。最后得出的最优阈值其实是训练集的最优值，但并不一定是测试集的最优值，所以当间隔在0.5s时会有超过10%的说话者改变并没有被检测到，但是实验显示间隔变长的时候这个问题就会消失。  

此外，作者还进行了进一步的改进，之前作者的模型中只是在比较特征之间的差值d'，这里他们尝试比较差值之间的差值d"，结果表明效果更好，在间隔为0.5s的时候将错误率从2.27%下降到了1.25%，这么做的原理是当当前间隔发生说话者切换的时候，下一个间隔基本上不会发生切换，因为间隔一般很短，在2s以内，那么当前间隔的d'将会显著的小于上一个间隔的d'和下一个间隔的d'，这就将偶然性进一步的降低，不过值得注意的是decision将会延迟一个间隔得出，原因也是显而易见的。  

## Conclusion  

总结一下，这篇文章提出了一个新的SCD系统，应用浅层神经网络进行说话者的分类，将这一部分作为整个系统的前缀，后接以计算欧氏距离作为基础的SCD系统，结果发现表现有着明显的提升，并且最终的结果表现相当好。作者进而提出了下一步的研究计划，即将这个算法应用到真实场景的谈话中，但是由于真实场景中的谈话会出现更多不确定的冲突，所以还需要进一步的研究。此外由于说话者在数据集内的表现更为良好，但是实时场景中无法提前获取说话者的信息进行聚类，因此还需要对实时场景的聚类进行研究，这可以参考之前的一篇低时延SCD论文，那篇论文主要解决的就是这个问题，那么我们还可以提前获取说话者的信息从而获得我们的百分百预测准确率，但是如何获取，获取哪些人的还需要进一步的研究。  

这篇文章的行文思路非常流畅，而我认为最优秀的点在于全文非常连贯，并没有常见的一大波数据在文章的后半段集合扑面而来的压迫感，在每一个部分都有条不紊的用图表来展现他们所作出的研究成果，读起来让人感觉挺舒服的。  

---
本博客支持disqus实时评论功能，如有错误或者建议，欢迎在下方评论区提出，共同探讨。  
