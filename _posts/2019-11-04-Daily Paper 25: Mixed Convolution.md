---
layout: post
title: "Daily Paper 25: Mixed Convolution"
description: "Notes"
categories: [MMML-Self_Supervised]
tags: [Paper]
redirect_from:
  - /2019/11/04/
---

# Daily Paper 25 - A Closer Look at Spatiotemporal Convolutions for Action Recognition  

## Introduction  

这篇paper是AVTS文章作者Du Tran的另一篇paper，主要介绍了在AVTS中使用的2D3D混合卷积。作者主要强调了3D和2D的混合卷积模型在残差网络模型上的优点，并尝试将三维卷积滤波器分解成独立的空间和时间分量显著的提高了精度。作者最大的创新点在于提出了一个(2+1)D时空卷积块，得到了相当于或优于现有表现的结果。  

对于静态图像领域而言，深度学习已经产生了巨大的影响，对整个领域的识别效果做出了质的提升，但是对于与之相似的视频领域，深度学习方法并没有获得如此大的性能领先。此外，基于图像的2D CNN(ResNet-152)对于视频的逐帧处理，最后的效果居然和当今最好的表现差不多，但是2D CNN是无法抓住视频的时序特征和视频中的动作变换的，这就使得在作者产生疑问：现有的视频深度学习方法是否已经真的利用了视频中的所有可利用特征了？对此有人可能会解释说视频中的时序特征并不重要，因为从图像中可以分析和预测出较为明显的时序信息。但是作者在这篇paper中对这一观点提出质疑，并通过设计三维卷积的形式，将时序这一特征纳入CNN卷积核中，建立了一个3D的ResNet网络，结果表明3D ResNet的性能明显优于2D ResNet。  

受上述结果的激发，作者又提出了两个2D和3D的中间形态，作者将第一个命名为Mixed Convolution(MC)，具体方式是只在网络前部使用3D卷积，在网络的顶层仍然使用2D卷积。这种设计的隐含逻辑在于对运动进行建模是一个低/中等级的操作，可以通过网络早期的三维卷积来实现，而顶部的二维卷积负责对这些中等级的运动特征进行空间推理，从而产生准确的动作识别；第二个被称作(2+1)D卷积块，将三维卷积显式的分解为两个单独和连续的部分：2维空间卷积和1维时序卷积，这两个卷积形式附带的非线性激活函数会使得相比于相同参数数量的3D卷积而言非线性数量增加了一倍，进而能够表示更为复杂的功能，此外将3D卷积进行分解能够有效的优化网络，能够在实践中产生更少的训练损失和测试损失。  

在视频处理领域中，SIFT3D，HOG3D，STIPs等视频表示方法都有很好的效果，目前最好的表现仍然是13年ICCV上提出的传统视频识别方法iDT，其优秀表现主要来源于其在视频分类上的强大能力。不过近年来也有一系列与三维卷积有关的深度学习方法产生，具体不在这里赘述。作者主要研究的是将时序和空间信息分离开来，应用在resnet上，从而得出最终的R(2+1)D网络，在当今的视频识别领域得到了最好的表现。  

## System  

这里作者设计了几个网络架构作为对比。第一个就是2D的ResNet，记为R2D；第二个是MCx，表示混合卷积的ResNet，x代表2D卷积的层数；第三个是rMCx，代表反向的混合卷积ResNet，x相应的就代表3D卷积的层数；第四个是全3D的ResNet，记为R3D；最后一个就是R(2+1)D，作者通过增加卷积核尺寸来扩充了参数的数量，使其最终的参数数量和之前的网络相等。此外对于2D CNN，作者采取了两种对第三维的处理方式，第一种就是简单的折叠不处理，称作R2D，第二种是在顶部的全局时空池化层上融合独立于L帧提取的信息，我们称之为f-R2D。  

## Experiment  

作者使用Kinetics和Sports-1M作为主要benchmarks，用于从头训练其深度网络，其次在较小的数据集UCF101和HMDB51上进行调优。作者考虑两种形式的R3D，第一种有18层，第二种有34层，然后分别用2D卷积，混合卷积，反向混合卷积和（2 + 1）D卷积代替3D卷积，从这些R3D模型中我们获得体系结构R2D，MCx，rMCx和R(2+1)D。作者对18层的网络性能进行了比较，结果显示R3D的模型表现明显比R2D的模型表现要好，这表明对运动建模是有必要的，此外R(2+1)D的表现显著优于其他模型，这表明将3D模型分为空间和时序进行处理更好一点。  

接下来作者将计算复杂度纳入考量，将各个网络的top-1准确率和FLOPS做了一幅图像，结果显示在同等计算复杂度下，R(2+1)D模型的表现由于R3D和MC3。此外，R(2+1)D的训练误差也低于R3D，特别是在34层的深层网络比较中差距更为显著，作者认为这是由于R(2+1)D的优化更为方便快速所致，在深层网络中体现的更为明显。  

接下来作者探究了clip的长度对网络的影响，实验表明clip越长，网络的准确性越高，但是这并不是一直增加的，而是在clip为32的时候达到峰值。此外作者还发现，当训练完毕clip为8的网络模型后，使用clip为32的网络进行调优，可以在总训练时间和精确度之间达到最好的平衡点。  

找到了最好的模型(R(2+1)D-34)后，作者就详细的评测该模型的具体性能。作者将RGB和光流输入分别用该网络训练，然后将预测分数平均融合在一起。作者使用Sport-1M数据集和Kenetics数据集上进行训练和测试，在UCF101和HMDB51 benchmarks上评估性能。  

在Sport-1M上，RGB输入的R(2+1)D模型表现最好，超过了当今表现最好的P3D模型9.1%，其在video上的73.3%的准确率成为了当今表现最好的模型。在Kinetics上表现略低于I3D(0.3%)，但是当两个模型都是从RGB输入中训练的时候R(2+1)D的表现会更好，并且其在ImageNet预训练后在光流和RGB输入中都比I3D要好。在UCF101和HMDB51上，作者在Kinetics上预训练的结果略微优于在Sports1M上的结果，最好的双流模型结果只比当今最好的I3D双流输入表现低0.7%和2.0%，并超出其余所有算法，在当前排名第二位，作者也提到这点微弱的差距可以用光流模型中，R(2+1)D模型采取Farneback的简略光流训练方法所致来解释。  

## Conclusion  

总结一下，作者提出了不同时空卷积对视频动作识别效果的实证研究。作者提出的体系结构R(2+1)D实现了在Sports1M，Kenetics，UCF101和HMDB51上的最高水平相媲美或更高的结果。作者希望该混合卷积模型对于潜在效率和时空卷积的建模灵活性的利用，能够激发其他研究者进行新的网络设计。不过作者的研究还只是局限在单一类型的网络（ResNet）和固定的(2+1)D分解方法，未来作者还将致力于为其方法搜索更合适的架构。  

---
本博客支持disqus实时评论功能，如有错误或者建议，欢迎在下方评论区提出，共同探讨。  
