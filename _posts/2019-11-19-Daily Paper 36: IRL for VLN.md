---
layout: post
title: "Daily Paper 36: IRL for VLN"
description: "Notes"
categories: [MMML-VLN]
tags: [Paper]
redirect_from:
  - /2019/11/19/
---

# Daily Paper 36 - From Language to goals: Inverse Reinforcement Learning for Vision-Based Instruction Following  

## Introduction  

今天的paper也是VLN领域的一篇经典paper，是由Google AI发表在ICLR2019上的，主要通过Inverse RL的方式来进行视频-语言导航。这篇paper的idea源于将VLN任务用RL方法来解决时奖励的不确定性，因此作者致力于使用IRL来更好的将语言指令ground到奖励函数上。  

传统VLN任务的解决方式是将语言和agent的第一视角图像合在一起直接分析，然后生成相应的动作来完成VLN任务，这一方式的限制在于policy本身需要同时解决两个具有挑战性的问题：理解如何在物理世界中plan和solve特定的task，以及理解语言指令本身。由于VLN任务需要在新环境中进行处理，那么传统方法下的VLN系统的表现就完全与模型在新系统下的泛化能力挂钩，如果语言理解和物理控制两者中有一个无法泛化，那么整个系统将会失败。  

而作者是怎么改的呢？作者将语言指令看做成了一种和最终的goal进行沟通的工具，而不是直接的将语言mapping到policy上，也就是说作者想要学习一种将由语言定义的目标转为奖励函数的转换方式，如果学会了这个，那么agent就可以通过强化学习自行学会如何在不需要零样本政策转移的情况下通过直接与环境交互来plan和perform给定的任务。这么说起来有点抽象，所以作者举了一个例子，如果给定的自然语言指令是“go to the fruit bowl”，那么一个固定的奖励函数可能就是简单的一个从agent的第一人称视角下的fruit bowl探测器。但是在传统的方法下，模型可能会在不同的房间内生成不同的plan，那么泛化性能将会大打折扣。  

那么想到在强化学习中找到合适的奖励函数，我们会自然而然的想到逆向强化学习，逆向强化学习正是通过专家决策来学习难以指定的汇报函数的一种常用方法，那么作者这里就使用了这一思想进行建模。作者认为IRL本身是一个inner-loop，这其实是正确的，因为IRL要通过专家的示例来学习回报函数，然后再用学到的回报函数进行强化学习优化模型自身，再将自己的生成样本和专家的示例进行对比，直到对比差距不断缩小才标致着回报函数和模型本身同时学习完毕。作者使用的是MaxEnt IRL这一方法，也就是基于最大熵的逆向强化学习，这一IRL方法需要对环境内的动态变化进行充分的了解，而VLN任务恰好满足这一点。因此作者可以在训练的时候通过动态规划来学习一个从观察映射的回报函数，但是在使用回报函数的时候并不需要对整个环境的运动进行充分的掌握，这恰恰是VLN任务的特点：训练的时候可以获得充足的环境变化信息，而使用的时候却不需要环境的动态知识，只需要用训练好的agent进行寻找即可。  

## Background  

今天的内容需要对IRL有一定的了解，因此我专门去补了下课。所谓的IRL可以分类成为最大边际形式化：学徒学习、MMP方法、结构化分类、神经逆向强化学习和基于概率模型的形式化：最大熵IRL、相对熵IRL、深度逆向强化学习，以及GAIL（生成对抗性模仿学习）。在最大熵IRL中，观察到的最佳轨迹概率与指数收益成正比，那么学习收益函数就等同于求最大似然估计。  

## Multi-task IRL  

language-conditioned IRL问题和标准的IRL问题相比，区别在于其目的是为了学习一个可以泛化多个任务的奖励函数。作者提出的方法是将每一个任务都看成一个独立的马尔科夫决策过程，记为ξ，再将每一个任务都关联一个语境c<sub>ξ</sub>，也就是一个独立的identifier。因此作者主要来优化以下公式：
maxE<sub>ξ</sub>\[E<sub>τ<sub>ξ</sub></sub>\[logp<sub>θ</sub>(τ<sub>ξ</sub>,c<sub>ξ</sub>)]]  

为了优化这一目标，作者首先要求所有的任务都共享相同的观察空间和动作空间，并将reward看成观察的函数而不是状态的函数，由于agent的观察内容形式和尺寸是相同的，而不同房间的布局不同会导致状态空间不同，因此将reward看成观察的函数更加有利于在所有的MDP中通用。其次，作者在全任务中共享奖励函数的同时，使用一个语言指令L<sub>ξ</sub>作为c<sub>ξ</sub>的代理。  

## Language-conditioned reward learning(LC-RL)  

接下来介绍主要的模型。总的来说作者使用了最大因果熵IRL学习语言条件奖励，并采用了一种多任务设置，用语言条件下的卷积神经网络来表示奖励。在训练的过程中需要采用动态规划的方法，这需要一定的动态知识，但是在使用和评价奖励函数的时候并不需要动态的知识，所以在测试的时候可以使用标准的model-free强化学习算法，通过新环境的推断奖励函数来学习新任务。  







---
本博客支持disqus实时评论功能，如有错误或者建议，欢迎在下方评论区提出，共同探讨。  
