---
layout: post
title: "CS231n Chapter 11 - Detection and Segmentation"
description: "Notes"
categories: [CS231n]
tags: [Python]
redirect_from:
  - /2019/08/05/
---

# CS231n Chapter 11 - Detection and Segmentation    

## Introduction  

之前我们所讨论的所有图像处理领域的问题都仅仅是图像分类，但是在计算机视觉领域，还有多种其他类型的问题，比如图像细分segmentation，定位localization，检测detection。这些任务都会使用计算机视觉和卷积神经网络。  

## Semantic Segmentation  

语义分割的意义是让计算机能够正确的对图像中的不同物体进行分类。那么我们可能会想到的方式是用label的方式对图像中可能存在的物体进行标注，然后依次对每一个像素进行检测，但是这种方法会导致计算的负担相当大，此外对于每个像素内的重叠特征无法很好的检测到，所以鉴于性能和复杂度都不能令人满意，我们不选择这种方法。  

那么我们进而会想到，与其单独提取特征，我们不如将整个图片导入一个完全的卷积神经网络，在每个卷积层之间并没有完全连接层，也就是整个网络都是卷积层，从课程PPT上我甚至没有找到池化层，那么这么一个完全的神经网络可以输出一个C×H×W的张量，C即是我们感兴趣的label数量，这样我们可以一次性的得到输入图像中的每一个位置的loss。  

在整个完全卷积网络的运行中，我们会对其进行下采样和上采样，先将输入的规模缩小为mid规模，一般是H/2和W/2，然后经过一部分处理后再将其缩小为H/4和W/4，然后再经过一部分处理后再用两段上采样将其放大为原来大小。这里牵扯到上采样和下采样，这里是我找的一篇比较好的博客：[链接](https://blog.csdn.net/stf1065716904/article/details/78450997)，课程中也进行了一些介绍，对于上采样，主要采取unpooling的方式，这里有两种具体的方式，比如复制原有的值，让新的部分区域全部是该值；另一种方式是用0来填充放大后的其他位置，这里有一个tip，我们需要保证下采样和上采样相互对应，即在pooling的时候我们要记住该区域内最大的位置，也就是保留的数值的具体位置，当上采样的时候我们将原数据保留在这个具体位置上，然后用0来填充其他值。  

上面介绍了池化的转置，这里介绍卷积的转置，卷积的转置其实就是类似于卷积的反操作，具体的操作方式是将我们的输入，也就是卷积操作后的值的每一个位置拎出来，那么每一个被拎出来的值都是通过卷积操作得出的，我们的操作是将其周围全部补0，然后与卷积核kerneo进行卷积运算，得到的结果就是转置卷积的值。那么考虑一下尺寸，假设输入是2×2，扩大后的输入是5×5，卷积核为3×3，那么得到的结果是3×3，那么卷积操作是将4×4的输入变成了2×2，而这里反卷积却是将2×2变成了3×3，可以看出卷积和反卷积无论数值还是尺寸，都不是完全互逆的，这也是卷积转置不等于信号处理领域的反卷积的原因之一。  

这里还用了一维的卷积和卷积转置操作作为示例，解释了卷积操作实质就是矩阵乘法，转置卷积其实就是矩阵转置后的乘法。  

## Classifiation+Localization  

接下来是定位问题，定位问题一般和分类问题一起进行，可以在图像中识别并标记出物体的位置。同样，我们首先可以考虑用回归问题的思想来解决这个问题，首先用分类的思想和框架来解决分类问题，之后在卷积层之后再加一个FC层用于定位框的定位，输出中心的位置和框的长宽，这里我们会有两个loss，一个是分类的softmax loss，另一个是位置的correct loss，衡量边界框坐标的正确性，之后我们将两个损失加权得到总损失。这种方式的缺点是由于两个损失函数加权得到最后的损失函数，所以我们的超参数实际上很难选取，很难找到一堆同时符合两个损失函数收敛的超参数。那么这时候就自然产生了一个疑问:为什么要用一组超参数呢？为什么不对每一个损失函数都用不同的一批超参数呢？这里的回答是其实我们也会这么做，就想进行迁移学习一样，可以分别训练两个不同的网络，然后对整体进行微调，那么我感觉这其实就是两组不同的超参数了，我还是有些糊涂的。  

这里举了一个人类姿态估计的例子，用十四个预测点来决定一个人的姿态情况，我们应用上述框架就可以很好的完成这个任务。  

## Object Detection  

最后介绍的是物体检测。物体检测和之前的分类定位的不同之处是这里我们要检测的物体数量是不确定的，所以我们不能简单的使用回归框架，将探测问题简单的分成分类+定位两个问题来解决。有一种进行物体检测的思想称为滑动窗口，给定特定的窗口大小，然后依次滑过整张图片，每次经过窗口的时候都会检测是否存在待检测对象。但是我们知道，滑动窗口带来的问题是不确定数量、规模、种类的待检测物体会导致整个物体检测流程需要进行巨大规模的location和scale，这是非常耗费时间和算力的，所以我们一般不会采取这种方法。相反，我们会使用R-CNN来代替这种方法。在之前吴恩达CNN MOOC中的物体检测[那一章](http://justin-yu.me/blog/2019/07/01/Convolutional-Neural-Networks-Chapter-3/)，首先介绍了YOLO算法，然后再简要的介绍了一下R-CNN，这里直接就介绍R-CNN了。这里采取了一个区域提案的思想，通过给定区域数量，然后用CPU迅速生成这些区域提案。接下来，应用一部分生成的区域搜索，从而得到目标可能存在区域位置，然后应用卷积神经网络进行分类，这样比直接对所有可能性进行汇总更为迅速,从CNN出来后，用SVM进行分类。这里其实很好理解，首先选出区域提案，其次导入CNN处理，最后导入SVM分类，引用回归器分别确定最终的位置。那么重点就在于选出区域提案所需要的算法，这里应用的叫做Selective Search，具体可以参考PAMI2015的一篇论文，当然还有很多不同的方法，比如EdgeBoxes，这些都是已有的算法，直接调用即可。那么R-CNN的缺点也很明显，太慢了，对于每一个区域提案都要进行一次CNN计算和SVM分类，训练和检测都很慢。所以我们有了Fast R-CNN。  

Fast R-CNN的做法借鉴了SPP（空间金字塔池化）的思想，这个思想来自何凯明大大，通过在卷积层的最后施加某种操作，使得进入FC层的所有图像尺寸全部相等，从而保证了选取的不同大小的区域最后的尺寸都是相等的，处理起来有相同的清晰度。那么这个操作就是空间金字塔池化SPP层，此外，它还有另外一个优点，即只对原图做一次卷积运算，然后候选区域在运算后的feature map上进行，所以只需要进行一次卷积，大大提高了速度。Fast R-CNN借鉴了SPP的思想，并将其精简，采用了Rol池化层，通过下采样将feature map尺寸控制到7×7，也就是说是固定了大小的SPP层。此外，Fast R-CNN直接用softmax进行分类，应用多任务损失函数，将box regression流程和region分类合并成为了一个multi-task模型，从而简化了整个流程。  

Fast R-CNN仍然存在一些问题，比如随机选择候选区域也比较耗时，这个时候我们可以继续进行优化，用神经网络的方式来自动选择候选区域，引入了一个RPN(Region Proposal Network)替代Selective Search，同时引入了anchor box锚框，确定了目标的形状和大小。我们的多任务模型需要训练四个loss，分别是RPN的分类物体，RPN的锚框坐标，最终的分类score和最终的box回归分析和坐标。此外，我们的RPN是在将图片导入CNN中得到feature map之后再进行的。  

这里是一篇总结性的[博客](https://cloud.tencent.com/developer/news/281788)，希望对大家有所帮助。  

此外用于目标检测的算法还有YOLO和SSD，YOLO是之前学过的模型，通过固定网格以及按阈值除去一些目标窗口，能够更为快速的解决目标检测的问题。SSD(Single Shot MultiBox Detector)结合了YOLO的回归和Faster R-CNN的锚框思想，使用全图各个位置的多尺度区域特征进行回归，既保持了YOLO速度快的特性，也保证了窗口预测的跟Faster R-CNN一样比较精准。  

有了目标检测这个大杀器，我们可以和上节课学的给图像添加标题结合起来，对图像里的每一个物体都添加字幕，作为一种非常详细的描述，这里就是将整个目标描述的算法和上节课的RNN-CNN结合起来。这就想搭积木一样，在CNN网络中，我们会将不同的层进行堆积，在整个网络架构之外，我们还可以将不同的网络堆积起来，从而得到更为复杂的应用。  



---
本博客支持disqus实时评论功能，如有错误或者建议，欢迎在下方评论区提出，共同探讨。  
