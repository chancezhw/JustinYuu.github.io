---
layout: post
title: "Daily Paper 20: Embodied QA"
description: "Notes"
categories: [MMML-VLN]
tags: [Paper]
redirect_from:
  - /2019/10/29/
---

# Daily Paper 20 - Embodied Question Answering  

## Introduction  

今天的这篇paper是佐治亚理工和FAIR发表在CVPR18上的，主要提出了一个新的task：Embodied QA，主要内容是给出一个具体的3D真实环境，让agent能够用其第一人称视角自行熟悉周围环境后，回答人类根据该环境提出的问题。Emobodied QA是一个经典的多模态机器学习任务，模型需要学习一系列AI技能，比如动态感知、语言理解、目标导航、常识推理和语言到行动的grounding，作者在这篇论文中提出了一个端到端的强化学习模型，并提出了EmbodiedQA这一任务的评价方式和指标。这类问题类似于视觉语言导航VLN问题，因此把这篇paper放在VLN领域一起看一下。  

作者的长期目标是使agent可以感知周围环境、交流和完成指令。对于这部分功能来说，任何微小的进步都会对人类世界产生很大的帮助。那么为了推动长期目标的发展，作者提出了一个这个大目标中的一个小任务——Embodied QA，并提出了这个问题的配套环境、评价指标和解决这个问题的强化学习模型。在作者的模型中，agent可以执行以下命令：朝左/右转，向前/后/左/右行进，这些动作的目的都是为了获取周围环境的信息，从而回答人类提出的问题。那么一个Embodied QA机器一定要有的功能是对自然语言和视觉信息的理解，除此之外还需要有以下几种功能：1.行动感知，agent必须学会将它得到的视觉信息转化成基于它对周围环境的感知而决定的正确动作。2.常识推理，由于agent并没有收到一个整体的鸟瞰图，所以agent需要在自己的主视角下像人类一样对周围的环境进行推理。3.language grouding，在VLN领域的paper中grounding这个词随处可见，不过我一直不知道如何完美的翻译，我的理解是一种“映射”，将自然语言映射为机器的某种理解，比如物体的视觉信息，空间的地理物质信息等。这里作者指的主要是将语言中的文本词汇与视觉物体形状相对应，这一问题也是语言-视觉问题中的一大难题，特别是在泛化环境中的表现非常差，使得整个模型的应用程度不高。在该模型中，语言信息grounding的并不是具体的图像，而是一个由动作组成的序列。  

从强化学习的角度理解，Emobodied QA是一个非常具有挑战性的学习问题，由于问题可能包含多种信息，而整个环境可能非常复杂，正确的结果又只有一个，那么完全正确的处理信息并得出结论是一个相当困难和复杂的任务。那么对于这一个复杂的任务，作者首先将要做的事情分类，将其分为环境、问题种类、学习范式，从而将这个稀疏的RL奖励与模仿学习和奖励塑造结合在一起。  

作者采用机器人和深度强化学习所使用的范式，在训练的时候给出所有信息，包括agent的位置、深度和环境的语义注释，并允许对无障碍物最短路径的计算。在测试的时候，agent完全按照主视角的视觉信息进行环境信息的获取，并不会得到训练时候的得到的任何辅助信息。agent的整个模块都是端到端进行训练的，从原始的视觉信息和语言信息，到目标导向的室内导航、再到回答提出的视觉问题。  

作者最后总结了一下他们的贡献，主要有：提出了Embodied QA这一新的问题；介绍了一种新的用于获取环境信息的Adaptive Computation Time导航；通过模仿学习初始化了agent并用强化学习方法进行调优，并提高了模型的泛化能力；在House3D场景下评价了模型的表现，提供了基于House3D的EQA数据集；还将House3D渲染器与Amazon Mechanical Turk（AMT）集成在一起，使测试者可以远程操作agent，并收集了基于问题的导航的专家演示，可以作为比较我们建议的算法和未来算法的benchmark。  

## System  

作者用了一页的篇幅介绍related works，由于这是一个新方向，所以相关工作都来自于不同的方向，就不复述了。此外作者还用大量的篇幅介绍他们的EQA数据集，这里我也不做过多的介绍，重点还是看作者使用的模型和实验结果。  

agent有四个模块，分别是视觉、语言、导航和回答模块，agent从原始的场景输入到目标导向的多房间室内导航、再到视频问答。整个模块大部分是使用CNN和RNN建立的，一个最重要的新技术是Graves等人设计的Adaptive Computation Time(ACT) RNN，它可以使得RNN学习在输入和输出之间究竟会有多少个计算步骤，作者使用这个RNN网络来完成方向和速度这两个判断的分割。  

视觉模块使用的是CNN网络，内含四个{5×5Conv,ReLU,BN,2×2MaxPool}模块，由于模型需要对物体的属性、语义和环境的地理属性进行编码，因此作者预训练了一个多任务的像素-像素的CNN预测框架，将上述CNN看做是一个编码网络，再训练多个解码网络，分别对原始的RGB值、语义类和每个像素的深度进行解码。语言模块使用的是一个双层LSTM，分别学习导航和回答问题的语言编码器。导航模块使用的是上述提到的ACT navigator，它将导航任务分成了planner和controller两个部分，前者负责确定执行的任务，后者负责执行任务，这种方式类似于层级强化学习，这种方式可以使得planner可以自由的决定该时间步是做决定还是强化long-term梯度流。当agent停止移动后，问答模块便启动了，agent根据其观察得到的结果，建立了T个planner帧和n个controller，共同组合了T×n个帧，用I<sub>t</sub><sup>n</sup>来表示第t个planner和第n个controller对应的帧，回答模块选取每个序列的最后五个帧，根据图像-问题的相似度，计算注意力池化的视觉编码，并将结果与一个该问题的LSTM编码器相结合，输出一个有172个可能回答的softmax向量。  

作者的训练方式采取了一种两步训练策略，首先将导航和回答模块独立的进行模仿/监督学习训练，其次使用policy gradients对整个网络结构进行联合训练。在Emobodied QA任务中，为了回答问题进行的导航其实并没有固定的正确路径一说，只要能够得到最终需要的信息并正确回答问题就可以。EQA v1数据集一个很重要的优点就是数据集中的问题都包含一个单一目标查询对象，由于目标是单一的，那么就存在agent到该目标的最短路径，那么在训练的时候就可以根据这个最短路径进行训练。而导航模块的主要任务就是让agent模仿学习最短路径，作者使用交叉熵误差训练了15个epoch。作者发现即使在模仿学习中，基于距离的训练方式还是更有效的。回答模块主要是通过最短路径下获得视觉图像和问题的正确回答训练的，作者使用标准交叉熵误差训练了50个epoch。  

分别训练结束之后，虽然各自表现的都非常好，但是两者结合起来的表现却比较差，在结合起来后，navigator有可能找不到正确的最短路径，那么对于回答模块而言，给到的用于预测的数据本身就是不准确的（非最短路径），那么这个时候与其让回答模块适应这种较差的路径，不如继续优化navigator使其生成优秀的最短路径。作者提供了两种reward signal给导航模块，第一种是导航结束后问题的回答准确率，第二种是不断接近目标物体的实时奖励，作者使用了REINFORCE这一policy gradient进行训练。  

## Experiment  

显然Embodied QA问题的最终评价指标是回答问题的准确率，但是为了更好的分析模型的性能，最为困难的导航部分的准确率其实对于评价模型性能而言更为重要。在softmax中，回答模块会给出172个可能的答案的概率，使用Mean Rank（MR）来表示真实的回答排在softmax概率的第几位，那么如果MR=1则视为回答正确。这种评价方式相对于二元评价来说可分析性更强，与其只看对与不对，分析最终的答案处于哪一阶段显然更具有实际意义。对于navigator，作者使用导航结束的地点与目标地点的距离d<sub>T</sub>，从最初到最终agent位置之间的距离d<sub>△</sub>，以及在导航过程中距离目标地点最近的距离d<sub>min</sub>。此外作者还记录了agent进入过或者停留在物体所处的房间内的问题占总问题的百分比%r，最后作者记录了agent在作出决定时走过的路径占最短路径的百分比。  

Baseline的选择也是一个问题，因为之前并没有Embodied QA这个概念，所以需要用其他问题的baseline作为评价指标。这里作者将ACTnavigator与其他网络作对比并介绍对比的原因，具体如下：  

1.Reactive CNN，该网络使用最后的n帧来预测下一步的动作，作者使用这一网络作为baseline是为了探究简单记忆帧能否很好的从训练集来泛化到训练集。  
2.Reactive CNN+Question，该网络将上述网络和一个LSTM编码器融合，编码器主要对“下一个动作是什么”这一问题进行编码。  
3.LSTM+Question，和上述两个的区别是这个的网络使用LSTM，因此有一定的记忆性。使用这一问题作为baseline的目的是为了体现planner+controller模型的有效性。  

最终的结果显示，所有的baseline的效果都不是很好，d<sub>△</sub>都是负的，这表明他们最终距离目标物体的距离更远了，这使得作者更加确信Embodied QA是一个困难的问题。此外结果表明采用具有记忆功能的LSTM更有效果，作者设计的模型ACT+Q和ACT+Q_RL表现最好。此外作者发现RL模型会存在overshoot的问题，这是因为RL模型会倾向于过度学习，这会使得agent更不倾向于学习结束时停留在目标物体房间，这一问题在普通的RL问题中可能无所谓，因此结果总是对的，只不过多学习了一部分，但是在这类问题中是有害的，因为回答模块会根据最后的几个frame进行判断，而overshoot现象恰好导致了最后几个frame的改变。这一问题可以通过在每一步动作之后施加一个小的负反馈来校正。作者还发现，最短路径并不一定适合VQA，这一问题需要后续的工作去改善。  

## Conclusion  

总结一下这篇内容量很多的paper，作者主要提出了一个新问题Embodied QA，并提出了一个新的层级模型来解构模型中的导航部分，作者用强化学习的方法初始化并训练+调优了整体模型，并给出了用于解决EmbodiedQA问题的数据集和评价指标，此外作者还收集了这一问题的人类表现，并共享了代码用于其他学者在这一问题上的后续研究。  


---
本博客支持disqus实时评论功能，如有错误或者建议，欢迎在下方评论区提出，共同探讨。  
