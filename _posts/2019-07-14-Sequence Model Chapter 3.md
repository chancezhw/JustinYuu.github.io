---
layout: post
title: "Sequence Model Chapter 3"
description: "Notes"
categories: [Sequence-Model]
tags: [Python]
redirect_from:
  - /2019/07/14/
---

# Sequence Model Chapter 3  

本周是这门课的最后一周，也是整个系列的最后一周，主要介绍音频识别，学习Seq2Seq模型。  

## Various sequence to sequence architectures  

我们之前学到，不论是图像捕捉，还是文本转换，都需要将输入的数据编码成为序列输入到神经网络中，然后再以序列的形式输出出来，那么在编码和解码的过程中，就要用到解码器和编码器。这种从序列到序列或者从图像到序列的模型就是一些经典的Seq2Seq模型。  

首先看文本转换，在一个机器翻译模型中，我们会发现和第一周学过的RNN语言模型非常相似，但是也有些不同。简单来讲，我们所做的机器翻译模型又叫做一个条件语言模型。在第一周学过的语言模型中，生成的单词或者句子是随机的，我们从softmax中随机取样作为下一层的输入，而整个模型的输入固定为a<sup>\<0></sup>,至于输入的x1,x2我们可以将其任意赋值，比如赋值为全0向量等。而在这里的机器翻译模型中，decoder部分和之前学过的语言模型基本相同，而在输入句子的时候除了固定的a<sup>/<0></sup>，由于要承担翻译句子中每一个单词的任务，所以x部分不能是全零向量，而是使用输入句子的编码。而条件语言模型在于，以前学习的语言模型计算的是简单的概率，即P(y<1>,y<2>,...y<Ty>)，而在这里计算的是条件概率，即P(y<1>,y<2>,...y<Ty>\|x<1>,x<2>,...,x<Ty>)，在翻译模型中就是指在输入法语的情况下输出不同英语句子的概率。那么实施的问题在于，你不能和之前一样随机取样，因为这样会使翻译结果时好时坏，效果并不稳定，所以我们要找到令前面的随机概率最大化的翻译结果，并将其作为最后的输出。  

找到最好的结果需要用算法实现，那么我们首先自然而然的想到了贪心算法，通过保证当前每一步结果最优来保证整个句子的联合概率最大化。但是问题在于，当我们选出第一个好的词语时，接下来的第二个词语是根据第一个单词决定的，也就是说第二个词语的最优解判定是根据当前的概率决定而不是整体的概率决定，所以这容易使整个结果陷入次优解，这也是贪心算法之所以不成功的通病。那么如果用暴力枚举呢？那么时间复杂度会飙升到V<sup>len(sentence)</sup>，显然也是不现实的，那么我们该用什么方法呢？答案就是Beam Search集束搜索算法。  

集束搜索的介绍如下，仍然使用翻译的例子。集束搜索有一个重要的超参数B集束宽，代表解码器中每个时间步候选的单词个数，假设在这里我们把集束宽设置成3，那么首先看要翻译的一个单词，选择softmax中概率最大的三个单词作为候选，接着进入第二步，分别考虑第一个单词是这三个中的一个的情况下第二个词语是什么以及其概率，接着继续在每组中选出概率最高的结果。这样我们就有了3组2个单词的短语结果，接着下面的步骤和这个类似，始终将候选的数量保持在3个，长度依次增加直到翻译结束，此时的三个结果就是概率最大的翻译结果，也就是翻译结果最好的语句。  

接下来是对集束搜索的优化算法。我们知道集束搜索的概率是通过累乘得到的，那么如果句子的单词比较多，就会导致乘积特别小，从而导致underflow，所以我们一般会将乘积取对数从而避免underflow。此外，我们唯结果论的算法会导致句子单词短的翻译结果更容易被接受，因为乘子越少结果越大。那么我们就要增加一些短语句惩罚，比如简单粗暴的按照长度取平均，也可以用一个新的超参数α来更温和的控制。α一般作为总长度的指数，如果α是1那么就和之前取平均一样，如果α是0代表不取平均，也就是没有短语句乘法，那么α取0~1之间就可以比较灵活的调整惩罚的力度。这个做法并没有理论依据作为支撑，只是因为效果很好，所以大家都这么做。这些优化方法合起来，叫做归一化对数概率目标，也叫归一化对数似然目标。  

最后总结一下如何选择B的值。首先考虑B取值的影响，如果B很大，那么我们会考虑更多的情况，结果也会更为准确，但是同时计算量和内存的要求也就更大，计算也会更缓慢，B很小的情况恰恰相反。在生产系统中，集束宽度B取值为10左右，当然这也要具体情况具体分析，当100以上的时候一般情况下就是比较大的了。在科研中，B可能会达到1000或者3000，从而得到最好的性能。B如果是1的话，那么集束搜索就会退化为贪心算法。值得一提的是，B提升的边际效益是递减的，从1到3的性能提升远远大于1000到3000。此外，不同于BFS和DFS，集束搜索是一个近似搜索算法，不能保证找到全局最优。  

既然集束搜索不能保证全局最优，那么我们就需要对结果进行错误分析。因为集束搜索是产生差错的原因之一，所以我们首先需要分析错误的出现究竟是因为RNN本身有错误还是集束搜索的结果错误。这个时候的做法一般是比较人工翻译和算法翻译的y*和y^，如果集束算法选择了y^,但是y*的概率更高，那么就说明集束搜索算法的结果错了，但是如果y*比y^更好，但是RNN预测结果相反，那么RNN模型就出现了错误。以此原理我们可以做一个表格，从而分析RNN和集束搜索出错的比例，从而发现此模型最大的问题在哪里，从而继续优化。  

---
本博客支持disqus实时评论功能，如有错误或者建议，欢迎在下方评论区提出，共同探讨。  
