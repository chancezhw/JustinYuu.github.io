---
layout: post
title: "Sequence Model Chapter 3"
description: "Notes"
categories: [Sequence-Model]
tags: [Python]
redirect_from:
  - /2019/07/14/
---

# Sequence Model Chapter 3  

本周是这门课的最后一周，也是整个系列的最后一周，主要介绍机器翻译和音频识别，学习Seq2Seq模型。  

## Various sequence to sequence architectures  

我们之前学到，不论是图像捕捉，还是文本转换，都需要将输入的数据编码成为序列输入到神经网络中，然后再以序列的形式输出出来，那么在编码和解码的过程中，就要用到解码器和编码器。这种从序列到序列或者从图像到序列的模型就是一些经典的Seq2Seq模型。  

首先看文本转换，在一个机器翻译模型中，我们会发现和第一周学过的RNN语言模型非常相似，但是也有些不同。简单来讲，我们所做的机器翻译模型又叫做一个条件语言模型。在第一周学过的语言模型中，生成的单词或者句子是随机的，我们从softmax中随机取样作为下一层的输入，而整个模型的输入固定为a<sup>\<0></sup>,至于输入的x1,x2我们可以将其任意赋值，比如赋值为全0向量等。而在这里的机器翻译模型中，decoder部分和之前学过的语言模型基本相同，而在输入句子的时候除了固定的a<sup>/<0></sup>，由于要承担翻译句子中每一个单词的任务，所以x部分不能是全零向量，而是使用输入句子的编码。而条件语言模型在于，以前学习的语言模型计算的是简单的概率，即P(y<1>,y<2>,...y<Ty>)，而在这里计算的是条件概率，即P(y<1>,y<2>,...y<Ty>\|x<1>,x<2>,...,x<Ty>)，在翻译模型中就是指在输入法语的情况下输出不同英语句子的概率。那么实施的问题在于，你不能和之前一样随机取样，因为这样会使翻译结果时好时坏，效果并不稳定，所以我们要找到令前面的随机概率最大化的翻译结果，并将其作为最后的输出。  

找到最好的结果需要用算法实现，那么我们首先自然而然的想到了贪心算法，通过保证当前每一步结果最优来保证整个句子的联合概率最大化。但是问题在于，当我们选出第一个好的词语时，接下来的第二个词语是根据第一个单词决定的，也就是说第二个词语的最优解判定是根据当前的概率决定而不是整体的概率决定，所以这容易使整个结果陷入次优解，这也是贪心算法之所以不成功的通病。那么如果用暴力枚举呢？那么时间复杂度会飙升到V<sup>len(sentence)</sup>，显然也是不现实的，那么我们该用什么方法呢？答案就是Beam Search集束搜索算法。  

集束搜索的介绍如下，仍然使用翻译的例子。集束搜索有一个重要的超参数B集束宽，代表解码器中每个时间步候选的单词个数，假设在这里我们把集束宽设置成3，那么首先看要翻译的一个单词，选择softmax中概率最大的三个单词作为候选，接着进入第二步，分别考虑第一个单词是这三个中的一个的情况下第二个词语是什么以及其概率，接着继续在每组中选出概率最高的结果。这样我们就有了3组2个单词的短语结果，接着下面的步骤和这个类似，始终将候选的数量保持在3个，长度依次增加直到翻译结束，此时的三个结果就是概率最大的翻译结果，也就是翻译结果最好的语句。  

接下来是对集束搜索的优化算法。我们知道集束搜索的概率是通过累乘得到的，那么如果句子的单词比较多，就会导致乘积特别小，从而导致underflow，所以我们一般会将乘积取对数从而避免underflow。此外，我们唯结果论的算法会导致句子单词短的翻译结果更容易被接受，因为乘子越少结果越大。那么我们就要增加一些短语句惩罚，比如简单粗暴的按照长度取平均，也可以用一个新的超参数α来更温和的控制。α一般作为总长度的指数，如果α是1那么就和之前取平均一样，如果α是0代表不取平均，也就是没有短语句乘法，那么α取0~1之间就可以比较灵活的调整惩罚的力度。这个做法并没有理论依据作为支撑，只是因为效果很好，所以大家都这么做。这些优化方法合起来，叫做归一化对数概率目标，也叫归一化对数似然目标。  

最后总结一下如何选择B的值。首先考虑B取值的影响，如果B很大，那么我们会考虑更多的情况，结果也会更为准确，但是同时计算量和内存的要求也就更大，计算也会更缓慢，B很小的情况恰恰相反。在生产系统中，集束宽度B取值为10左右，当然这也要具体情况具体分析，当100以上的时候一般情况下就是比较大的了。在科研中，B可能会达到1000或者3000，从而得到最好的性能。B如果是1的话，那么集束搜索就会退化为贪心算法。值得一提的是，B提升的边际效益是递减的，从1到3的性能提升远远大于1000到3000。此外，不同于BFS和DFS，集束搜索是一个近似搜索算法，不能保证找到全局最优。  

既然集束搜索不能保证全局最优，那么我们就需要对结果进行错误分析。因为集束搜索是产生差错的原因之一，所以我们首先需要分析错误的出现究竟是因为RNN本身有错误还是集束搜索的结果错误。这个时候的做法一般是比较人工翻译和算法翻译的y* 和y^，如果集束算法选择了y^,但是y* 的概率更高，那么就说明集束搜索算法的结果错了，但是如果y* 比y^更好，但是RNN预测结果相反，那么RNN模型就出现了错误。以此原理我们可以做一个表格，从而分析RNN和集束搜索出错的比例，从而发现此模型最大的问题在哪里，从而继续优化。  

为了衡量机器翻译结果的准确性，可以采用BLEU(bilingual evaluation understudy)指数进行评估，指数的值是机器翻译和人工翻译相同的单词数除以总单词数。  

注意力模型是很重要的模型。对于长序列来说，比如翻译句子中的长难句，人工翻译不会一次性的将整个长句子看完再翻译，因为记不住。机器也是这样，随着句子长度的上升，bleu score会逐渐下降，即准确率逐渐降低。在注意力模型中，我们使用BRNN作为编码器，另外一个单向RNN作为解码器，这个时候我们对句子的每个部分，即编码器的每个输出都增加一个注意力权重，从而能够让神经网络更注重当前的，也就是注意力权重高的部分。具体的操作如下：对于BRNN每个单元的前向和后向激活函数，我们将其合并起来记为a，对于解码器的第t个时间步，其输入来自编码器的注意力加权c，以及前一个时间步的激活函数s，输出y。c的值由所有层的注意力权重乘以该层的激活函数a加和得到。也就是说和普通的RNN或者LSTM相比，输入的激活函数值乘以了新增的注意力权重。此外，所有的注意力权重加和为1。  

a<sup>\<t,t'></sup>表示y<sup>\<t></sup>对a<sup>\<t'></sup>的注意力权重，其值a<sup>\<t,t'></sup>=exp(e<sup>\<t,t'></sup>)/Σ(t'=1,Tx)exp(e<sup>\<t,t'></sup>),而e<sup>\<t,t'></sup>的值需要用一个小的神经网络计算，输入为s<sup>\<t></sup>和a<sup>\<t'></sup>。采用神经网络的原因是虽然知道与这两者有关，但是不知道具体的函数形式，因此要采用一个神经网络进行学习。这里找了一篇介绍注意力模型很好的博客：[自然语言处理中的Attention Model：是什么及为什么-张俊林](https://blog.csdn.net/malefactor/article/details/50550211)  

## Speech recognition  

Seq2Seq的另一个重要应用是语音识别，给定一个语音片段x，将其自动转换为文本y。我们可以用之前学的注意力模型来训练进行语音识别，也可以用一种新的技术CTC(Connectionist temporal classification)，即连结主义时间分类。具体来讲，我们会使用一个输入和输出大小相同的RNN，实际情况中一般会使用双向的LSTM或者BRNN，输入的时间步一般会比输出的时间步大很多，所以CTC允许RNN的输出中含有大量的空白符_和空格<space>，以及大量的重复字符。等到输出完毕后，将重复的字符和空白符删除，即可得到最后的结果。  
  
触发词检测是一种可以在小数据集训练下有良好表现的算法，这套系统主要用于语音唤醒。具体来讲，我们将用RNN来计算语音的特征，定义目标标签Y，当识别到触发词时将目标标签设为1，如果没识别到触发词就将当前时间步的目标标签设为0。但是有个缺点是这样1和0的数量将会非常不平衡，所以有一个小技巧是可以在输出1的时候多输出几次，或者输出一个固定的时间段，从而让0和1的比率稍微平衡一点。  

## Programming assignments  

这次的编程作业是用注意力模型来实现机器翻译，以及用触发词检测来进行语音识别。这里是我的源代码链接：[Code](https://github.com/JustinYuu/Deeplearning-study/tree/master/Sequence%20Model)  

## Conclusion  

终于结束了！这一系列共有5门课程，从最初的神经网络开始，到如何改进神经网络，如何在自己的项目中得到更好的效果，以及介绍了很重要的CNN和RNN。正如吴恩达所说，deeplearning is a superpower!深度学习可以在各个领域有良好的应用，善用AI，善用深度学习，我们可以让整个世界变得更加美好。这系列的课程我断断续续用了五个月左右来完成，这个系列的课程非常优秀，难度合理，内容量适中，学习完这个课程后我对深度学习这个领域也算是有了基本的了解，期待在我的研究生生涯中我在这个领域能够有更好的进展。  

---
本博客支持disqus实时评论功能，如有错误或者建议，欢迎在下方评论区提出，共同探讨。  
