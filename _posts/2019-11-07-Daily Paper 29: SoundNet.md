---
layout: post
title: "Daily Paper 29: SoundNet"
description: "Notes"
categories: [MMML-Self_Supervised]
tags: [Paper]
redirect_from:
  - /2019/11/07/
---

# Daily Paper 29 - SoundNet: Learning Sound Representations from Unlabeled Video  

## Introduction  

这篇paper是nips2016上的，主要通过利用野外收集的大量未标记声音数据来学习丰富的自然声音表示。利用视觉和声音之间的自然同步来学习使用200万个未标记视频的声学表示。未标记的视频具有以下优点：它可以在大规模上经济地获取，但包含关于自然声音的有用信号。作者提出了一种学生-教师培训程序，它使用未标记的视频作为桥梁，将辨别的视觉知识从完善的视觉识别模型转换为声音模态。与声学场景/对象分类的标准基准测试相比，作者的声音表现相较于最先进的结果具有显着的性能改进。可视化表明声音网络中会自动出现一些高级语义，即使它是在没有标注的情况下进行训练的。  

作者主要提出了一个深度卷积网络，从原始的音波中直接进行学习，这是通过视频到声音的转变训练实现的。虽然该网络是用视觉信息训练的，但是其并没有对于视觉信息的依赖性，在作者的实验中，作者发现他们通过大规模无标注声音数据训练的网络表示能够在当前三大声学场景分类数据集上获得当前最好的表现。作者的主要贡献是开发了一个自然声音的大规模且语义丰富的表示，作者坚信这个大规模的自然声音表示能够对很多实时应用产生重大的影响，此外作者还首次提出了视频和音频联合训练，学习丰富的声音表征。  

作者应用了迁移学习的思想，建立了一个老师-学生模型，使用一个dark knowledge transfer，将有识别力的知识从一个训练好的复杂模型压缩(转移)到简单模型中，同时不降低太多准确性。这里的teacher是视觉模态下的大规模训练模型，student是听觉模态的拟训练网络。此外作者利用了声音和视觉之间的自然对齐，在无声音标注的前提下学习了自然声音的深度表示。  

## System  

首先作者做的一大贡献就是搞了一个大数据集，把一大堆未标注的视频直接放了进去。作者主要从Flickr上下载的，因为这些视频是自然未经专业剪辑的短片段，将日常生活中的各种情形的不同声音全部包含在内。作者下载了约200万个视频，总长度大约超过一年，每个视频的长度在几秒到几分钟不等。此外，作者还对这些原始的视频进行了处理，将所有的声音转成了MP3格式，并将采样率下调到了22Hz，并将音频转为单声道，这样可以更方便的在大数据集上对其进行操作。作者还将音波缩放在了-256到256之间，不过并没有减去均值，因为均值几乎等于0。  

接下来看作者的网络架构，网络使用一个深层的卷积神经网络，由于音频是一个序列状的输入，那么如果使用CNN的话需要使用一系列一维卷积层来处理声音。这里CNN能够应用在音频中是有原因的，首先因为这个网络需要用一套参数来适应不同的变换，因此使用CNN可以提高学习效率；其次卷积网络可以很方便的堆叠层，使得通过一系列低级别的探测器的组合能够探测到高级别的概念。不过这里和图片不一样的是，声音的输入长度是不同的，因为每一段声音的时间步数量都不同，因此作者需要让其网路处理不同输入的大小，这里使用的是全卷积网络。由于卷积网络对不同输入的相同位置是不变的，因此作者根据输入的长度来卷积每一层，在作者的架构中只有卷积和池化层。虽然最终的表示会自适应输入的长度，但是这也导致了作者的网络输出也必须适应不同的长度，这里作者如果使用一个全局池化层来讲不同尺寸的输出下采样到一个固定维度的向量，会导致某些对高等级表示有用的信息被丢弃掉，因此应该另谋他法。作者想到既然最终我们要用不同长度的视频来训练这个网络，干脆直接用一个卷积层作为输出层，直接输出视频中多个时间步上的输出直接应用好了。由于训练的数据量相当大，因此在不发生过拟合的前提下使用深度的网络架构是可行的，这里作者使用了5层和8层的网络做实验……  

之前提到过，一段视频中的视觉和声觉部分存在着自然的对齐，而作者正是要利用这个关系来学习声音表示，所以下面要考虑如何将大规模数据集上训练得到的视觉特征转移到声音中。作者希望在学习中使用教师视觉网络的后验概率g来训练学生声音网络f，使其能够识别给定音频的概念。由于我们希望能够从object和scene网络中同时转移知识，将两者代替监督学习中的标签训练学生网络，因此使用KL散度作为损失函数，这使得教师网络g的输出可以被解释为一个类分布。由于KL散度是可微的，因此作者使用反向传播和随机梯度下降来优化该网络。  

虽然作者想让SoundNet分类视觉类别，但是作者想让它识别的类别可能不会再视觉模型中出现，比如打鼾声，所以其实需要使用一个不同的策略去给声音附加一个语义。作者忽略了网络的输出层，使用内部表示直接作为训练分类器的特征，并对一些感兴趣的概念使用了一小部分的标注声音数据，并挑了网络的一个层作为特征训练了一个线性SVM。对于多分类任务，作者使用的是one-vs-all策略，使用交叉验证集去选择margin正则化超参数。为了提高网络的鲁棒性，作者使用了一个标准数据增强步骤，将每一个训练样本分成重叠的若干等长度声音片段，对每一个片段都提取特征用于训练。在inference中跨所有的窗口来平均预测结果。  

对于baseline，除了使用一些著名的baseline以外，作者还自己训练了一个卷积自编码机作为额外的baseline。自编码机使用4个卷积层作为encoder，使用4个分数步长卷积作为decoder，encoder的参数和SoundNet的前四个卷积层相同。  

作者首先进行了声学场景分类实验，在DCASE Challenge、ESC-50和ESC-10数据集上进行了测试，结果显示在DCASE上作者的准确率高于其他方法10%，在ESC-50和ESC-10上也高于其他方法10%，并更加接近人类表现，在50和10上分别差7%和3%。  

其次作者进行了ablation analysis，比较了不同的loss、Teacher Net、网络深度对网络带来的影响，结果显示使用KL散度、8层teacher网络、8层SoundNet网络得到的准确率最高。此外作者还与监督学习的方法进行了对比，监督学习的具体内容是使用标注的音频数据而不使用视觉网络迁移学习，结果显示差距很小。作者接着比较了数据集和teacher网络种类的影响，结果显示VGG对于DCASE更好，AlexNet对于ESC5O更好。  

最后作者进行了多模态识别，使用AMT标注的一系列新视频用来测试。首先用t-SNE看一下视听特征的语义相关性，结果显示仅测试声音特征也含有相当数量的语义信息。作者还定量对比了一下视听特征，对视听特征各自训练了一个多类SVM，接下来联合起来进行多模态识别，结果显示视听结合后的表现显著优于声觉特征的表现，略优于视觉特征的表现，虽然提升只有2%，但还是说明听觉特征起到了一定的辅助作用。  

## Conclusion  

总结一下，作者主要提出了一个新的网络SoundNet，通过对已建立的视觉网络进行迁移学习，和使用大量未标注视频进行训练，得到了一个能学习到良好的听觉表示的模型。此外，该实验证明了跨模态学习能够获得更为优秀的性能，指出了这一领域广泛的研究前景。  

---
本博客支持disqus实时评论功能，如有错误或者建议，欢迎在下方评论区提出，共同探讨。  
