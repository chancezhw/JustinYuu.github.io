---
layout: post
title: "Daily Paper 18"
description: "Notes"
categories: [multimodal-VLN]
tags: [Paper]
redirect_from:
  - /2019/10/26/
---

# Daily Paper 18 - Are You Looking？ Grounding to Multiple Modalities in Vision-and-Language Navigation  

## Introduction  

这篇paper是UCB和波士顿联合发表在ACL2019上的文章，主要研究的就是多模态领域下的VLN视觉-语言导航问题，一作是一位清华本科的UCB在读phd。VLN即通过视觉和语言两种模态的信号处理来对机器人或者其他机器进行导航。VLN显然需要用到一系列指令来完成，比如“右转，在门口停下”，这种指令会将指令这一语言本身通过多种模态与机器人周围的环境进行连接，比如在门口停止也许会与视觉领域的物体识别相联系，朝右转可能与路线的空间结构相关。作者主要研究了自然语言在两种VLN模型下的应用场景，此外作者还发现视觉特征可能会损害这些模型的发挥，这是通过将模型的视觉特征去除掉之后，只保留路线结构会在Room-to-Room数据集上有更好的新环境表现。为了更好的利用以上多种模态，作者将这些grounding procedure,也就是最初的instruction分解为一系列不同模态的专家系统，并在预测的时候将这些系统合在一起共同使用，从而提高了目前的VLN任务的表现。  

由于这是VLN领域我读的的第一篇paper，所以把related work比较详细的讲一讲，就当是对这个领域的入门学习了。VLN是一个相对较新的问题，是由Anderson等人于CVPR18中的一篇论文提出的，这篇论文在接下来的几天内会提到。VLN要求通过一个代理agent，按照人类写下的一段文本指示在现实世界中进行导航,agent需要在考虑指示和它所看到的周边环境的前提下进行导航，直至达到指定位置。最近的几篇paper介绍了VLN领域的一系列进展，主要是准确率的提升，不过他们都没有说清楚他们的模型大幅增加的准确率究竟是来自于哪一个模态，进一步说，准确率的提升究竟是因为模型对指令的理解更深了，还是导航过程中的路线结构简化了呢？  

首先，作者通过训练两个不带视觉特征的最新模型的版本来分析VLN在多大程度上将指令语言建模为视觉外观和路线结构，这里使用的是R2R数据集。作者最终发现虽然将语言指令建模为路线结构非常有用，但是拥有视觉特征的模型并不能完成对于泛化场景的指令视觉化。令人惊讶的是，没有视觉特征作为训练数据的模型对于未见到过的新环境，相较于有视觉特征训练的模型表现相当甚至更好，也就是说视觉特征反而起到了负作用，这本身是很反常理的。  

作者对该现象的解释是低等级的、基于像素的CNN特征可能导致了视觉特征训练模型的泛化失败。那为了解决这一问题，作者提出了一个高等级的、基于物体的视觉表示方法，使用一个预训练的目标检测系统的符号化输出将语言指令用一种泛化性更强的方式来表示为视觉信息。具体来讲，如果指令里有一个桌子，那么导航过程中可能会将桌子对应到agent视觉中的具体的那一张桌子的图像，而这里作者采用目标检测的方法，将桌子符号化，然后将指令里的桌子对应到目标检测系统中桌子的符号化输出中，这样就可以更好的泛化新环境。  

最后，作者将这个指令的映射过程分成了几个不同的系统，通过训练几个独立的视觉和非视觉的agent，让每一个agent都聚焦于不同的模态，在最终将其预测的结果合在一起，得到最终的预测结果。作者的这种多模态agent的混合预测模式效果优于传统的单agent模式，也优于单一模态的多agent预测结果，具体来讲，提高了当前最好表现10%，进步还是很可观的。  

VLN的related work主要融合了两种不同的工作，第一种是在现实环境下跟随自然语言导航指示；第二种是基于视觉的现实世界中的导航任务。18年至今有一系列研究旨在提高VLN的success rate成功率，不过好的表现究竟来自模型的何处仍然是未知的。在有些VLN模型中，不对视觉这一模态建模也能获得很好的表现，一些研究表明训练集中的视觉信息有可能会起到负作用。有些研究还调查了VLN任务的单模态表现，发现没有视觉建模的VLN模型有很好的表现，总而言之，大家对于VLN的视觉特征应用的还不够好。  

## Experiment  

作者首先实验了在R2R数据集上的两个主流框架SF和SM，这两个模型都是基于encoder-decoder模型的，作者对这两个网络在加入视觉特征和不加入视觉特征的情况进行了横向和纵向对比，结果显示加入视觉特征的模型虽然在见过的环境里表现更好，但是在新环境中表现更差，即泛化性更差，因此有理由相信视觉特征对VLN模型的泛化性有反作用。  

在这两个模型中，视觉特征都是通过一个预训练的ResNet-152得到的，而由于训练数据过少，只有61个不同的环境，所以极易造成过拟合，对于一个特定的物体来说，模型更倾向于理解成训练集里见到过的实物本身，这对于新环境中的泛化影响是非常大的，因为新环境中几乎不可能有和训练集的环境中一样的特定物体存在。而作者就在考虑，与其与训练集中的实物对应，不如应用目标检测的方式，对应一个符号化的物体，使用一个大规模的预训练目标检测网络，从而有效的避免过拟合和泛化性能差的问题。作者使用Faster R-CNN来进行目标检测， 使用GloVe对物体和其属性进行编码，使用注意力机制来获得物体表示，使用物体检测获得的表示来代替ResNet的CNN特征或者将ResNet CNN特征和物体检测连接起来。结果显示SF模型的ResNet+Obj的效果最好，SM的Obj效果最好。  

最后，作者建立了一个mixture-of-experts模型，将视觉和非视觉模型合在一起使用，对于SF模型，效果最好的是RN+Obj和RN+Obj的融合……对于SM模型，最好的是RN+Obj和no vis.的融合，作者还训练了一个三者融合模型，将RN、Obj、no vis.合在一起，效果比以前的都要好。作者最后进行了联合训练，只建立一个agent，使用一套encoder进行训练，最终得到了最好的表现，在val-unseen上得到了51.9的SR，优于之前的所有模型。  

## Conclusion  

这篇paper主要的创新点就是探究了视觉特征对VLN模型效果的负影响，以及提出了解决这种负影响的可行解决措施，最终通过联合训练多模态模型获得了最好的表现。作者最后发现，用目标检测代替传统的视觉特征ResNet CNNfeature会显著的提高网络性能，此外在一个agent里面训练多模态的模型效果会低于多个单模态agent的融合，最后发现联合训练一个agent和一套encoder效果会比mixture-of-experts的训练结果要更好。  

---
本博客支持disqus实时评论功能，如有错误或者建议，欢迎在下方评论区提出，共同探讨。  
