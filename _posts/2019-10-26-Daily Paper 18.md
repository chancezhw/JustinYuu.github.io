---
layout: post
title: "Daily Paper 18"
description: "Notes"
categories: [multimodal-VLN]
tags: [Paper]
redirect_from:
  - /2019/10/26/
---

# Daily Paper 18 - Are You Looking？ Grounding to Multiple Modalities in Vision-and-Language Navigation  

## Introduction  

这篇paper是UCB和波士顿联合发表在ACL2019上的文章，主要研究的就是多模态领域下的VLN视觉-语言导航问题，一作是一位清华本科的UCB在读phd。VLN即通过视觉和语言两种模态的信号处理来对机器人或者其他机器进行导航。VLN显然需要用到一系列指令来完成，比如“右转，在门口停下”，这种指令会将指令这一语言本身通过多种模态与机器人周围的环境进行连接，比如在门口停止也许会与视觉领域的物体识别相联系，朝右转可能与路线的空间结构相关。作者主要研究了自然语言在两种VLN模型下的应用场景，此外作者还发现视觉特征可能会损害这些模型的发挥，这是通过将模型的视觉特征去除掉之后，只保留路线结构会在Room-to-Room数据集上有更好的新环境表现。为了更好的利用以上多种模态，作者将这些grounding procedure,也就是最初的instruction分解为一系列不同模态的专家系统，并在预测的时候将这些系统合在一起共同使用，从而提高了目前的VLN任务的表现。  

由于这是VLN领域我读的的第一篇paper，所以把related work比较详细的讲一讲，就当是对这个领域的入门学习了。VLN是一个相对较新的问题，是由Anderson等人于CVPR18中的一篇论文提出的，这篇论文在接下来的几天内会提到。VLN要求通过一个代理agent，按照人类写下的一段文本指示在现实世界中进行导航,agent需要在考虑指示和它所看到的周边环境的前提下进行导航，直至达到指定位置。最近的几篇paper介绍了VLN领域的一系列进展，主要是准确率的提升，不过他们都没有说清楚他们的模型大幅增加的准确率究竟是来自于哪一个模态，进一步说，准确率的提升究竟是因为模型对指令的理解更深了，还是导航过程中的路线结构简化了呢？  

首先，作者通过训练两个不带视觉特征的最新模型的版本来分析VLN在多大程度上将指令语言建模为视觉外观和路线结构，这里使用的是R2R数据集。作者最终发现虽然将语言指令建模为路线结构非常有用，但是拥有视觉特征的模型并不能完成对于泛化场景的指令视觉化。令人惊讶的是，没有视觉特征作为训练数据的模型对于未见到过的新环境，相较于有视觉特征训练的模型表现相当甚至更好，也就是说视觉特征反而起到了负作用，这本身是很反常理的。  

作者对该现象的解释是低等级的、基于像素的CNN特征可能导致了视觉特征训练模型的泛化失败。那为了解决这一问题，作者提出了一个高等级的、基于物体的视觉表示方法，使用一个预训练的目标检测系统的符号化输出将语言指令用一种泛化性更强的方式来表示为视觉信息。具体来讲，如果指令里有一个桌子，那么导航过程中可能会将桌子对应到agent视觉中的具体的那一张桌子的图像，而这里作者采用目标检测的方法，将桌子符号化，然后将指令里的桌子对应到目标检测系统中桌子的符号化输出中，这样就可以更好的泛化新环境。  

最后，作者将这个指令的映射过程分成了几个不同的系统，通过训练几个独立的视觉和非视觉的agent，让每一个agent都聚焦于不同的模态，在最终将其预测的结果合在一起，得到最终的预测结果。作者的这种多模态agent的混合预测模式效果优于传统的单agent模式，也优于单一模态的多agent预测结果，具体来讲，提高了当前最好表现10%，进步还是很可观的。  

VLN的related work主要融合了两种不同的工作，第一种是在现实环境下跟随自然语言导航指示；第二种是基于视觉的现实世界中的导航任务。



---
本博客支持disqus实时评论功能，如有错误或者建议，欢迎在下方评论区提出，共同探讨。  
