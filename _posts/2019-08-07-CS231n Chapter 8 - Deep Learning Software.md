---
layout: post
title: "CS231n Chapter 8 - Deep Learning Software"
description: "Notes"
categories: [CS231n]
tags: [Python]
redirect_from:
  - /2019/08/05/
---

# CS231n Chapter 8 - Deep Learning Software      

## Introduction  

这节课我们主要研究一下深度学习所使用的软件和框架，包括CPU和GPU之间的区别，以及不同的框架之间的联系和异同。  

## CPU vs GPU  

这一部分更像是一节硬件课，这里Justin直接把他的机箱图片放了上来。CPU就是central processing unit中央处理器，它放置在风扇下面，只使用了一小部分空间。但是GPU不同，graphic processing unit图像处理器，俗称显卡，空间巨大，占据了机箱的很大一部分空间。那么对于显卡的介绍，首先要引入一场世纪大辩论：NVIDIA VS AMD,可能在打游戏和性价比方面两者各有优劣，但是在深度学习方面，NVIDIA有得天独厚的优势，可以说占据主导地位。  

接下来介绍了一下当前（2017）的顶级CPU和GPU，当然当时Titan和1070时代和现在RTX的天下已经有很大的不同了，不过相同的是GPU和CPU的核心区别是巨大的，CPU的核一般都在20以内，但是GPU的核心能够达到上千级别。核心的数量差距导致了计算性能存在巨大差距，此外，CPU和GPU的强项也有所不同，CPU适合进行具有复杂的计算步骤和复杂的数据依赖的任务，而GPU适合进行大规模的并行计算，而我们深度学习恰好需要大规模的并行简单计算，所以GPU更适合深度学习。  

要想进行GPU编程,我们需要学习一些GPU编程的语言或者框架，CUDA是一个NVIDIA平台的GPU编程语言，通过写类C语言的代码，可以直接在GPU上运行，此外CUDA还有一些高级API，比如cuDNN,cuFFT,cuBLAS等，但是值得一提的是CUDA代码非常复杂和困难，所以我们一般不会直接进行CUDA代码的编写，而是直接调用一些NVIDIA的官方包来直接使用。此外OpenCL是一个类似CUDA的支持多平台的GPU编译器，但是其速度偏慢。  

接下来就用一些主流的深层网络进行CPU和GPU速度的对比，显然这是全方位吊打，速度差距一般在60-70倍左右。值得注意的是，我们在用GPU训练的时候要注意，由于GPU的训练速度很快，所以很容易出现CPU和GPU之间的通信速度限制导致训练速度出现瓶颈，所以我们一般的做法是尽量将所有数据读在内存里，并尽量用固态硬盘来代替机械硬盘，此外用多CPU线程来预先获取训练数据。  

## Deep Learning Frameworks  

深度学习的框架在我看来，就和泼了大粪的庄稼，几天就出来一大堆，主流的框架也是一年一变，所以这里2017年的课程做的对比我感觉其实在现在看来意义不大，不过简单来讲，这两年pytorch和TensorFlow基本上占据了框架的主流，pytorch在近两年发展的越来越好，但是TensorFlow在2019年发布了2.0版本，配合高级框架keras，使用的人也越来越多。  

简单来讲，深度学习的框架作用有三点：方便的建立我们的大型计算图，简单的计算/更新梯度，在GPU上能够方便的运行。所谓的计算图，就类似于一个流程图，该流程图描述了具体的操作步骤和细节，然后我们可以用代码来实现这个计算图。其实我们完全可以用numpy完成前向传播的过程，但是numpy的局限性在于，无法进行反向传播，也无法进行GPU编程，所以我们需要用Tensorflow等框架来辅助我们完成这些操作。一般情况下，我们的操作就是自己完成前向传播和网络结构搭建，然后让框架来进行反向传播和训练预测。  

接下来以TensorFlow 1.x为例进行了一个两层神经网络的运算。这里2.0版本的TensorFlow已经丢弃了placeholder和session，所以学习的意义不大，我就不再纪录具体操作了。  



---
本博客支持disqus实时评论功能，如有错误或者建议，欢迎在下方评论区提出，共同探讨。  
