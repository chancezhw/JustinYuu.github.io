---
layout: post
title: "CS231n Chapter 9 - CNN Architectures"
description: "Notes"
categories: [CS231n]
tags: [Python]
redirect_from:
  - /2019/08/05/
---

# CS231n Chapter 9 - CNN Architectures  

## Introduction  

这周将会学习一大堆CNN的架构，课程将使用AlexNet、VGG、GoogLeNet、ResNet作为案例进行讲解学习，并同时介绍一些其他的CNN架构，比如NiN,Wide ResNet,ResNeXT,Stochastic Depth,DenseNet,FractalNet,SqueezeNet等等。  

## AlexNet  

首先是AlexNet，这是2012年Hinton老爷子的高徒Krizhevsky等人建立的，一举多得当年ImageNet的分类冠军。AlexNet的输入规模是227×227×3，第一层是第一卷积层CONV1,11×11的过滤层，步幅为4，数量为96，卷积运算后输出的规模为55×55×96（(227-11)/4+1），该层的总参数数量为(11* 11 * 3) * 96 = 35K个。第二层是第一池化层POOL1，池化filter的size是3×3，步幅为2，输出的体积为27×27×96，这一层的参数为0.以后的层和这两个大同小异，就不具体计算了，最后经过全连接层后输出的最后尺寸是1000neurons，也就是1000×1的One-hot向量。  

AlexNet第一次采用ReLU，并使用了归一化层，其使用了大量的data augmentation，其dropout的参数为0.5，batch size为128，动量参数为0.9，learning rate为0.01，当验证集准确率趋于平缓时减少10%，这些都是超参数的细节，就不多赘述了。值得一提的是，AlexNet当时是在GTX580 gpu上训练的，当时的显卡只有3G的显存，他们将所有的网络分布在两个GPU上，神经元平均分布在两个GPU中间。  

## VGGNet

VGGNet使用了小的filters，更深的网络架构，相比于AlexNet的8层，他能到达16-19层。其卷积层和池化层的filter尺寸比较小，stride也比较小。我们使用较小的卷积层的原因是3个3×3小卷积层的堆积能够和一个7×7大卷积核的卷积层有相同的有效感知领域。那么什么是有效感知领域呢？或者简单点说，什么是感知领域呢？这里我找了一篇写的比较好的博客放在这里：[链接](https://iphysresearch.github.io/posts/receptive_field.html),原来所谓receptive field，就是上层filter对于某一层图像的影响领域，那么定量的分析，一个3×3的卷积核能够影响的下一级区域为3×3，下二层能够影响到的区域是5×5，下三层是7×7.那么我们有了相同的有效感知领域之外，多层卷积核能够有更深、更为非线性化的结果出现，这对于我们的神经网络来说无疑是一大利好，除此之外，我们这样还会有更少的参数，从而减少我们的学习量。  

这里根据整个VGG网络架构和参数表格我们可以看出，随着网络的不断加深，图像的容量，也就是占用的空间实际上是越来越少的，这要得益于我们的卷积和池化，但是所需的参数是越来越多的，因为channel不断增加。VGG在14年的ILSVRC中在分类领域获得了第二名，定位领域获得了第一名，它还有VGG16和VGG19等变体。  

## GoogLeNet  

接下来介绍GoogLeNet，它是一个更深的网络，共有22层。它最为特别的是使用了一个名为inception的模块，通过应用该模块，只用了5百万参数就能达到更深的效果，这个参数数量比AlexNet少12倍。这个网络在14年的ILSVRC分类赛中打败了VGG夺得第一名。  

那么inception是什么？inception应用了局部拓扑的概念，设计了一个小型的局部网络，而这个局部网络位于整个神经网络层中，所以我们又称为network in network NiN,某个inception模块堆叠在其他inception模块上方，形成整个网络的总架构。我们首先来看局部网络内部，在inception v1中，一个local network里面有一个1×1卷积层，一个3×3卷积层，1个5×5卷积层，还有一个3×3最大池化层，这些层是并列的，共同处理上一层输出的值，之后共同导入到一个concatenation层中，作为该局部网络的输出。  

我们会想到如此深和复杂的网络，其计算复杂度可能会比较高。我们分析inception局部网络模块内部，我们会发现我们保持了尺寸不变的情况下，将深度做了极大的扩充，如果输入的尺寸是28×28×256，那么输出的尺寸将会是28×28×672，总操作数将会达到854百万次，这是一个非常巨大的数字，所做的操作数量也是极为巨大的。值得一提的是，所谓的1×1卷积，其作用是增加非线性化，以及降低深度，所以我们可以将目前的inception做一个变体，将除了1×1卷积层之外的所有层之前都添加一个1×1卷积层，从而降低深度，减少计算复杂度。我们通过计算发现，改良后的计算总操作数将会达到358百万次，减少了一半还要多，很有效的降低了计算复杂度。

---
本博客支持disqus实时评论功能，如有错误或者建议，欢迎在下方评论区提出，共同探讨。  
