---
layout: post
title: "CS231n Chapter 9 - CNN Architectures"
description: "Notes"
categories: [CS231n]
tags: [Python]
redirect_from:
  - /2019/08/05/
---

# CS231n Chapter 9 - CNN Architectures  

## Introduction  

这周将会学习一大堆CNN的架构，课程将使用AlexNet、VGG、GoogLeNet、ResNet作为案例进行讲解学习，并同时介绍一些其他的CNN架构，比如NiN,Wide ResNet,ResNeXT,Stochastic Depth,DenseNet,FractalNet,SqueezeNet等等。  

## AlexNet  

首先是AlexNet，这是2012年Hinton老爷子的高徒Krizhevsky等人建立的，一举多得当年ImageNet的分类冠军。AlexNet的输入规模是227×227×3，第一层是第一卷积层CONV1,11×11的过滤层，步幅为4，数量为96，卷积运算后输出的规模为55×55×96（(227-11)/4+1），该层的总参数数量为(11* 11 * 3) * 96 = 35K个。第二层是第一池化层POOL1，池化filter的size是3×3，步幅为2，输出的体积为27×27×96，这一层的参数为0.以后的层和这两个大同小异，就不具体计算了，最后经过全连接层后输出的最后尺寸是1000neurons，也就是1000×1的One-hot向量。  

AlexNet第一次采用ReLU，并使用了归一化层，其使用了大量的data augmentation，其dropout的参数为0.5，batch size为128，动量参数为0.9，learning rate为0.01，当验证集准确率趋于平缓时减少10%，这些都是超参数的细节，就不多赘述了。值得一提的是，AlexNet当时是在GTX580 gpu上训练的，当时的显卡只有3G的显存，他们将所有的网络分布在两个GPU上，神经元平均分布在两个GPU中间。  

## VGGNet

VGGNet使用了小的filters，更深的网络架构，相比于AlexNet的8层，他能到达16-19层。其卷积层和池化层的filter尺寸比较小，stride也比较小。我们使用较小的卷积层的原因是3个3×3小卷积层的堆积能够和一个7×7大卷积核的卷积层有相同的有效感知领域。那么什么是有效感知领域呢？或者简单点说，什么是感知领域呢？这里我找了一篇写的比较好的博客放在这里：[链接](https://iphysresearch.github.io/posts/receptive_field.html),原来所谓receptive field，就是上层filter对于某一层图像的影响领域，那么定量的分析，一个3×3的卷积核能够影响的下一级区域为3×3，下二层能够影响到的区域是5×5，下三层是7×7.那么我们有了相同的有效感知领域之外，多层卷积核能够有更深、更为非线性化的结果出现，这对于我们的神经网络来说无疑是一大利好，除此之外，我们这样还会有更少的参数，从而减少我们的学习量。  

这里根据整个VGG网络架构和参数表格我们可以看出，随着网络的不断加深，图像的容量，也就是占用的空间实际上是越来越少的，这要得益于我们的卷积和池化，但是所需的参数是越来越多的，因为channel不断增加。VGG在14年的ILSVRC中在分类领域获得了第二名，定位领域获得了第一名，它还有VGG16和VGG19等变体。  

## GoogLeNet  

接下来介绍GoogLeNet，它是一个更深的网络，共有22层。它最为特别的是使用了一个名为inception的模块，通过应用该模块，只用了5百万参数就能达到更深的效果，这个参数数量比AlexNet少12倍。这个网络在14年的ILSVRC分类赛中打败了VGG夺得第一名。  

那么inception是什么？inception应用了局部拓扑的概念，设计了一个小型的局部网络，而这个局部网络位于整个神经网络层中，所以我们又称为network in network NiN,某个inception模块堆叠在其他inception模块上方，形成整个网络的总架构。我们首先来看局部网络内部，在inception v1中，一个local network里面有一个1×1卷积层，一个3×3卷积层，1个5×5卷积层，还有一个3×3最大池化层，这些层是并列的，共同处理上一层输出的值，之后共同导入到一个concatenation层中，作为该局部网络的输出。  

我们会想到如此深和复杂的网络，其计算复杂度可能会比较高。我们分析inception局部网络模块内部，我们会发现我们保持了尺寸不变的情况下，将深度做了极大的扩充，如果输入的尺寸是28×28×256，那么输出的尺寸将会是28×28×672，总操作数将会达到854百万次，这是一个非常巨大的数字，所做的操作数量也是极为巨大的。值得一提的是，所谓的1×1卷积，其作用是增加非线性化，以及降低深度，所以我们可以将目前的inception做一个变体，将除了1×1卷积层之外的所有层之前都添加一个1×1卷积层，从而降低深度，减少计算复杂度。我们通过计算发现，改良后的计算总操作数将会达到358百万次，减少了一半还要多，很有效的降低了计算复杂度。  

## ResNet  

何凯明大大的残差网络压轴出场，这个2015年诞生的超级深度神经网络横扫了当年主流计算机视觉大赛的分类和检测冠军。该神经网络拥有152层，远远超过了其他神经网络的深度。为什么如此深的网络构建如此困难呢？难道不是简单的堆叠就可以了吗？答案是否定的，当我们将深度增加到一定程度的时候，我们会发现深层的神经网络表现比略浅一点的要差，但是如此差的表现并不是因为过拟合，那么原因究竟是什么呢？何凯明等人提出一个假设：这其实是一个优化问题，深层模型更难去优化。深层网络应该至少能够达到和浅层网络相同好的程度，我们只需要将浅层网络搬过来，然后增加一些额外的层作为映射就可以了，这应该是表现完全一样的。这里Serena不知道怎么了讲的比较乱，仔细的梳理一下，ResNet的不同之处是相较于普通的神经网络直接导进层里计算，或者相较于之前提到的照搬浅层网络然后只添加映射的不作为方案，ResNet采取了一种折中的方案，将两者结合在一起:即既取一部分导进去计算，又取一部分直接放到下下层的输入那里，也就是说不经过该层计算，两部分在下一层的输出，也就是下下层的输入位置处加到一起，即为该层最终的计算结果。  

其深度能达到如此深的程度，就是因为将普通的输入分出了残差residual，在ResNet残差网络中，我们一般在开头添加一个隐藏层，在结尾一般没有FC全连接层，只有一个为了输出softmax向量而存在的全连接层。此外不要添加dropout层，并在每一个CONV层之后进行批量均一化。  

ResNet的效果非常好，之前提到它横扫了2015年的图像识别大赛各个冠军，事实上在每一项上它都比第二名好10%以上，ImageNet的location甚至优于第二名27%，最NB的是它居然比人的准确性还好……当然人的分类水平来自于李飞飞实验室的前大弟子Andrej Karpathy，他自己训练了自己一周，得到了5%左右的错误率……  

## Other architectures to know  

下面是一些Serena认为需要了解的网络模型或者思想，这里进行简要介绍。  

首先是Network in Network(NiN)，GoogLeNet的inception就应用了NiN的思想，NiN的思想起源于2014年，主要通过Micronetwork来进行计算，以获得更为抽象的特征，它也是GoogLeNet和ResNet“瓶颈层”的起源。  

其次是ResNet的一些变体，这里只是简要提一下，2016年何凯明大大继续提出了Identity Mappings in Deep Residual Networks，这里他创建了一个更为直接的路径来在整个网络中进行前向传播，并获得了更好的性能。此外2016年Wide Residual Networks使用了更宽的残差块，增加了宽度以代替深度，从而得到了更好的计算便利性。2016年何凯明大大又发了一篇ResNeXt，在每个残差块中并行多个路径，这种思想毫无疑问借鉴了inception模块的思想，相当于inception和resnet的结合体。最后，随机深度是一种很好的改进ResNet的方法，其动机是为了减少梯度消失的问题，其方法类似于dropout，在训练时随机丢弃一部分层，在测试时应用所有的层。  

此外，还有一些Beyond ResNet的神经网络也有很好的表现，首先，Larsson等人在2017年创建了一种称作FractalNet的超级深度网络，丢弃了残差层，从浅到深的并行训练，也得到了很好的效果。还有Huang在2017年创立的DenseNet，这个网络看起来比较乱，每一层都和其他的每一层链接，通过建立一个极为密集的层间联系，从而缓解梯度消失问题。还有一种SqueezeNet，通过添加一种Squeeze层，从而在保持原有网络准确率的基础上大幅减少参数数量，以AlexNet为例，能够将参数值减小50倍，并将整个模型尺寸控制在0.5Mb以内，这其实是一个很了不起的成就。  

这周的内容就结束啦，下周居然介绍RNN，这门课有点东西的。  

---
本博客支持disqus实时评论功能，如有错误或者建议，欢迎在下方评论区提出，共同探讨。  
