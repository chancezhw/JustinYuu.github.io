---
layout: post
title: "CS231n Chapter 6 - Training Neural Networks, part I"
description: "Notes"
categories: [CS231n]
tags: [Python]
redirect_from:
  - /2019/07/31/
---

# CS231n Chapter 6 - Training Neural Networks, part I    

## Introduction  

训练神经网络这一部分将会通过两部分组成，第一部分主要介绍一些激活函数、数据的预处理、权重初始化、批量归一化，以及维护学习的过程和超参数优化。  

## Activation Functions  

常用的激活函数其实就那么几种，再介绍一遍好了。首先就是sigmoid函数，σ(x) = 1/(1+e^(-x))，它将函数值维持在0到1之间。但是这个函数也有一些明确的缺点，比如说输入值过大或过小的时候梯度就会消失，从而不利于反向传播。此外，sigmoid的输出并不是以0为中心的，而是恒为正值，这将会导致w的梯度将会一直是恒正或者一直是恒负，这就会导致梯度更新的低效率，用图像表示出来可以发现它会出现锯齿形的晃动，较为繁琐。我的理解是恒正和恒负相当于限制了具体的更新方向，其实也不仅仅是恒正和恒负，而是恒为某一正值和某一负值，这就导致我们的梯度更新必定会在这两个方向上不断的更新，那么如果这两个方向都不是最优的方向，结果就是锯齿形的zig-zag下降。最后的一点小问题是sigmoid需要进行指数运算，这会导致运算的成本大大增加。  

接下来是tanh函数，这个函数的值域为\[-1,1],所以不会出现上述的那种问题，但是仍然会出现的问题是梯度消失的问题还没有被解决。  

正所谓大道至简，这么多复杂的激活函数没有解决的问题，就让一个小小的RELU来解决了，这货的右半部分压根就是一个线性函数，只是左半轴部分是0，但是它却有独特的优点，比如说右半部分的梯度不会消失，计算相当简单，实际计算中收敛速度极快，甚至更符合神经学原理……在我个人的认知上我还是不太懂为啥这个半线性函数是有用的，不过它就是有用的，anyway。  不过Relu也是有缺点的，比如不是0为中心的输出，以及一个dead ReLU的情况产生：当输入一直是负数或者0的时候，输出也将一直是0，那么这个ReLU函数将一直不会更新，这一部分我很感兴趣，这里是Quora上的一篇讨论，我把链接放到[这里](https://www.quora.com/What-is-the-dying-ReLU-problem-in-neural-networks)，那么这个问题可以用一个ReLU的改进方案Leaky-ReLU来代替，将负半轴的值用一个很小的负值来代替，即可避免这个问题。  

此外，还有另外一种ReLU叫做Parametric Rectifier,其公式是f(x) = max(αx,x)，负坐标的倾斜程度由参数α确定，该参数参与反向传播，这种ReLU简称PReLU。我们还有另一种ELU，负半轴为α(exp(x)-1)，这个ReLU的好处是对于一些噪音有更好的鲁棒性，以及结果更接近于0中心，但是其缺点是需要进行指数操作，从而导致计算负担增加。  

这里还介绍了一个称之为Maxout Neuron的max函数，公式为max(w1x+b1,w2x+b2)，这个函数实质上同时生成了leaky ReLU和ReLU，这个函数不会die，也不会饱和导致梯度消失，但是它用了double的参数，从而导致计算负担增加。  

总结一下，对于激活函数的选用，使用ReLU比较好，但是要注意学习率的选用，也可以使用ReLU的上述变种，如果可以的话也可以试试tanh，但是不要抱太大希望……最后，不要用sigmoid……  

## Data Preprocessing  

接下来是对数据的预处理，比较容易理解的就是均一化和零均值化。回到之前我们学到的那个zig-zag的例子，出现zig-zag的原因就是因为原数据均为正或者均为负，所以零均值化原数据显得格外重要，零均值化在吴恩达Machine Learning MOOC中也称作 feature scaling,具体方式是(原数值-平均值)/标准差，从而将所有的数据都限定在\[-1,1]这个区间内，并将均值设置为0。和其他机器学习方法相比，我们的图像处理领域数据预处理反而不太常用标准化以及更深的预处理，比如PCA。此外，对于训练数据和测试数据我们都需要做相同的预处理，从而保证两者的训练结果可比较。那么零均值化的过程中我们可以减去总的图像均值，比如AlexNet，也可以减去每个channel的均值，比如VGGNet。  

## Weight Initialization  

对于权重的初始化，如果将全部的权重初始化为0，那么可能出现的问题是所有的权重都会做同样的事情，从而导致输出相同的结果，得到相同的梯度，以相同的方式更新，那么所有的神经元都会一样，所以我们一般采用随机初始化权重的方式，初始化的具体做法一般使用高斯权重。那么如果我们仍然想让初始值接近于0，我们可以采取将随机值scale到一个很小的数值的方法，比如W= 0.01 * np.random.randn(D,H),这个方法对于小型网络来说是OK的，但是对于更深层的网络来说就没那么好用了，对于深层网络来说，层方差会迅速的缩小为0，从而导致所有的激活值为0，这也很好理解，由于权重的值很小，导致梯度的值非常小，甚至出现梯度消失。那么如果将系属于由0.01变大，比如变成1，那么结果将会是几乎所有的神经元都会完全饱和，取值为1或者1(假设激活函数为tanh)，梯度将因此会全为0，导致权重得不到更新。那么这就陷入了一个困境，我到底该如何初始化我的权重？2010年Glorot等人创立了一个新的初始化方法，称作Xavier初始化，公式为W = np.random.randn(fan_in,fan_out)/np.sqrt(fan_in)，Xavier初始化的思想就是将每一层的输入都能控制在一定范围内，且使我们的权重w分布和数据x的分布满足一定的分布，在官方笔记中有Xavier初始化的相关推导，可以看一下。  

但是Xavier初始化的问题在于如果使用在ReLU等非线性函数上，由于ReLU在负半轴全部是0，导致我们的方差会大大减小，从而导致最后的结果又会突然的坍缩为0，那么2015年的一篇论文对其进行了改良，将分母的fan_in除以2，就可以解决这个问题……的确很神奇。  

## Batch Normalization  

批量均一化是一个很常见基础而又很有效的方法。批量均一化的方案就是将原数据减去平均值再除以标准差。BN一般插入在全连接层和卷积层后面，非线性激活层前面。其具体的计算方式如下：首先计算mini-batch的均值，然后计算该batch的方差，接着将其均一化，最后用参数γ和β来对结果进行线性变换以得到我们所需要的范围，从而得到最后的结果。  

批量均一化能够很好的改善通过网络的梯度流，并能够允许更高的学习率。此外，它还能减小对于初始化的强依赖型，并且它作为一种正则化形式，能够轻微的减轻对dropout的依赖，在2019年现在的视角来看，BN在很多情况下基本上是可以完全代替dropout的。  

在批量均一化的过程中，我们需要用到标准差和平均值，训练过程中我们会单独计算他们，但是在测试的时候我们一般不在均一化的过程中计算这两个数值，而是在训练的时候估计，例如使用运行平均值。  

## Babysitting the Learning Process  

这一节主要研究如何在训练的时候控制超参数，即学习调参。先来走一遍流程，首先预处理数据，其次选择我们需要的神经网络架构，包括隐藏层的种类和数目等，在这个过程中，我们最好再次确认一下损失值是合理的，检查一下输出值，和我们的class数目对比一下，确保loss不至于太大和太小。  

之后进行训练，首先选取一小部分数据，之后将正则化调整为0，然后尽可能的降低loss值，提高准确率到尽可能为1，采用随机梯度下降法，对每一个数据都进行梯度下降，尽可能的过拟合这些数据，这一步的目的是确保我们可以将小部分的数据训练到准确率很高，从而确保我们的算法是健全的。接下来就可以进行训练了，首先确定学习率，如果loss不变，说明学习率太小。如果loss太大到NaN，那么说明学习率太大。  

## Hypterparameter Optimization  

对于超参数的优化，我们还可以采用交叉验证集，这在之前已经介绍过了。在第一阶段，我们只是进行几轮epoch的训练来明确我们的超参数具体起到什么作用。在接下来的第二阶段我们才开始更长时间的运行，来进行不断的优化。整体的思路就是从corase到fine的搜索超参数数值，那么我们最好是用一个loop加上log记录来搜索，从而明确的得出什么参数值是表现最好的，先将loop的range放大，进行corase阶段的搜多，然后将range缩小，进行fine阶段的搜索。这里Serena将调参比作DJ调整音乐，其实还是挺像的:)  

此外，还可以将不同超参数下损失函数的曲线作图比较，从而得到特定的超参数下损失函数随着迭代次数的变化趋势，通过分析不同阶段的变化从而得出究竟是哪里的训练效果还不够好。我们也可以把train和val两个曲线作在一个图像里，如果gap很大，说明出现了过拟合，如果没有gap，说明我们可以适当提高模型的复杂度。我们还可以改变权重更新和权重大小的比值，我们希望比值大概在0.001左右，这个数值刚刚好，使我们的权重更新不至于改变太多，也不至于没有太大的效果。  

---
本博客支持disqus实时评论功能，如有错误或者建议，欢迎在下方评论区提出，共同探讨。  
