---
layout: post
title: "Daily Paper 24"
description: "Notes"
categories: [MMML-Self_Supervised]
tags: [Paper]
redirect_from:
  - /2019/11/02/
---

# Daily Paper 24 - Look, listen and learn  

## Introduction  

今天的这篇paper是昨天VGG组在ECCV2018上发表的另一篇，主要提出了L³-Net，来研究音频和视频之间的对应关系，并提出了一个新颖的AVC问题。通过这篇paper和昨天的对比，可以看出VGG组在AVC问题中的方案优化过程，并可以与再之前的两篇AVTS文章作对比。作者还将该模型应用在了两个下游任务物体定位和细化识别任务中，得到了很好的表现。  

由于我看的顺序有点问题，这篇paper其实是最初提出AVC的文章，因此应该作为第一篇读，由于这篇提出了一个新的问题，因此introduction部分作者写的比较详细，但是因为我读了之前几篇，对于AVC问题非常熟悉了，所以就简单的写一下。  

AVC即Audio-Visual Correspondence，直译为音频-视频关联，指的是给定一段视频和音频，判断两者是否相关联，这相对于AVTS来说要求更低，因为无需时序对齐。作者的目标是设计一个系统，通过简单地查看和聆听大量未标记的视频，能够以完全无监督的方式学习视觉和音频语义信息。为了实现这一点，作者引入了一种新颖的音视频对应（AVC）学习任务，用于从头开始训练两个（视频和音频）网络。作者在下面的部分中详细的描述了学习了哪些语义信息，并评估了音频和视频网络的性能。作者发现这项任务可以产生相当细化的视觉和音频差别，例如可以区分不同的乐器。在定量性能方面，音频网络超过了最近使用visual super vision训练的用于音频识别的音频网络，视觉网络的表现和监督学习的纯视觉网络性能相当。此外，作者还展示了使用激活值可视化在视频帧中定位音频事件的来源（并且还可以定位声源的相应区域）这一下游任务。  

## System  

首先来看作者的AVC模型。AVC其实是一个二元判断问题，模型只需要输出是和否即可，那么用于训练的正样本就是同一时间同一视频的视频和音频，负样本来自不同的视频，而作者期望模型只通过学习视觉和听觉概念上的各种语义概念来进行判断，这和昨天的完全相同。  

网络架构分为音频子网络和视频子网络两部分，除输入数据尺寸不一样之外卷积网络的架构完全相同，都是由4个卷积模块和4个池化模块组成，卷积模块包括两个3×3卷积层。两个子网络的最终输出concat连接起来，然后通过两个FC层降维，输出一个2×1的softmax向量。其余训练细节和参数就不复述了。  

## Experiments  

对于结果的评价，主要分为AVC系统本身的表现评估和模型学习的视听特征表现评估，后者主要通过在视频和音频分类的迁移学习中评价，最后作者还定性分析了网络学到的特征。  

作者主要在两个数据Flickr-SoundNet和Kinetics-Sounds数据集上训练网络，前者是一个无标注的自监督学习数据集，用于网络的主要训练，而后者是一个标注数据集，用于定量评估网络的性能。  

首先看AVC任务的评估，对于评估的baseline，作者采取自己在相同的子网络下训练监督学习模型，然后将两个子网络以两种不同的方式进行组合，生成了两个baseline。用于监督学习baseline的训练网络架构和原模型的子网络架构完全相同。第一个baseline是较为直接的baseline，直接将两个子网生成结果的相似度作为AVC分数，具体方式是计算softmax输出的34维向量的标量积，如果结果大于某个阈值则判定为相关，这个baseline也就是昨天那篇paper的子网连接方式。第二个baseline是完全和AVC模型网络相同的baseline，不过是用受监督学习的方式来训练，从而比较该自监督模型的自监督特征带来的影响。结果显示L³-Net的性能在Kinetics监督训练数据集上的表现为74%，等同于第二个baseline的表现，比第一个baseline表现高9%，在Flickr-SoundNet数据集上的表现为78%，略高所有baseline。同时作者指出，由于本身任务就比较难，人类的表现也只比L³-Net的表现高了几个百分点，可见该模型的效果还是很好的。  

接下来是对音频特征进行性能评估，评估的benchmarks是ESC-50和DCASE，结果显示ESC50和DCASE的两个基准均比当前最先进的SoundNet，分别降低了5.1％和5％。对于ESC-50，作者将先前最佳结果与人类表现之间的差距缩小了72％，而对于DCASE，作者将误差降低了42％。  

此外还需要对视频特征进行性能评估，评估方式是将L³-Net的视频子网络从图像中提取的特征在ILSVRC12上进行评估，结果达到了32.3%的top-1准确率，超过了当前绝大部分的监督学习方法的性能。  

最后进行定性的分析，主要方式是对测试集的结果进行可视化，来看一看子网络究竟学到了什么，结果显示L³-Net可以很好的区分不同的视觉物体和音频事件。  

---
本博客支持disqus实时评论功能，如有错误或者建议，欢迎在下方评论区提出，共同探讨。  
