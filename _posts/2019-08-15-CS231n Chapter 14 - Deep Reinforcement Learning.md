---
layout: post
title: "CS231n Chapter 14 - Deep Reinforcement Learning"
description: "Notes"
categories: [CS231n]
tags: [Python]
redirect_from:
  - /2019/08/15/
---

# CS231n Chapter 14 - Deep Reinforcement Learning    

## Introduction  

这周主要介绍强化学习。至此为止我们主要学习的是监督学习，给定特定的标签x和y，对模型进行训练，此外在上节课我们还学习了非监督学习，通过多种方法可以生成新的模型。那么这里我们要学习叫做强化学习，用于描述和解决智能体（agent）在与环境的交互过程中通过学习策略以达成回报最大化或实现特定目标的问题。  

那么今天主要有四个部分，首先介绍什么是强化学习，其次介绍马尔科夫决策过程，再接下来介绍两个算法，Q-Learning和Policy Gradients。  

## What is Reinforcement Learning?  

强化学习有很多典型的例子，比如车竿问题，如何在一辆运动的车上平衡一个杆子，那么角度、角速度、位置、纵向的速度都是要考虑的状态。另外一种经典的RL例子叫做robot locomotion机器人运动，其目的就是让机器人向前走，还有一个Atari game的游戏例子，通过强化学习来赢得该游戏的最高分。最著名的强化学习的例子就是alphago了，在2016年alphago战胜了李世石，这是强化学习的一个非常成功的应用。  

## Markov Decision Process  

接下来进入数学领域的知识：马尔科夫决策过程。所谓的马尔科夫属性，就是当前的状态完全表征了当前的世界，那么这个属性由(S,A,R,P,γ)五个参数定义，S代表可能的状态集，A代表可能的操作集，R代表奖励给定对(state,pair)的分布，P代表过渡概率，即在下一个状态给定对上的分布，γ是一个参数，代表对奖励的重视系数。MDP是一个比较复杂的概念，这里是维基百科[链接](https://en.wikipedia.org/wiki/Markov_decision_process)，可以详细的看一下。  

这里介绍一个简单的MDP任务：Grid World，在一个网格中，根据我们的状态，决定下一步是向哪个方向走，直到走到我们提前设定的终点。那么我们可以对每一个过渡设定一个负奖励，我们可以在每一个状态下采取行动，即向四个方向之一随机走一步。我们可以用随机方法和我们的优化政策作对比，我们的action数量明显的要小于随机方法。  

那么我们要做的就是最大化reward的总和，这种政策就是我们的优化政策π<sup>*</sup>，那么我们如何处理初始状态和转移概率等随机性呢？我们的方案就是使用数学期望来平均这些随机性，我们可以定义一个价值函数，用来代表特定状态的好坏，并同时定义一个Q-value函数来表示状态-动作对的好坏，我们将MDP的五个参数按照样本轨迹/路径进行初始化。  

我们可以尝试用Bellman equation来定义并求解Q-value函数，Q<sup>*</sup>是给定的状态-动作能实现的最大期望累计奖励，Q<sup>*</sup>(s,a) = max E\[sigma γ<sup>t</sup>r<sub>t</sub>\|s<sub>0</sub> = s,a<sub>0</sub>=a,π]，同时Q<sup>*</sup>满足Bellman equation。我们将使用Bellman方程作为迭代更新，第i次更新都会求得一个新的Qi值，然后当i趋向于无穷大时Qi将会收敛到Q<sup>*</sup>，但是这会有一个问题，我们要计算每一个状态-行动对的Q(s,a)，计算量在某些情况下将会非常大，大到我们无法遍历整个状态空间来求最大值，所以这个方案实质上不可行。那么我们的想法就是和之前的VAE一样对Q进行估计，这里我们使用的是神经网络。  

## Q-learning  

Q-learning就是用于解决优化政策的神经网络及其对应算法的总称。Q-learning简单来讲，就是使用了一个函数逼近器来估计动作值函数Q-value function，我们可以记为Q(s,a;θ)≈Q*(s,a)，θ就是使用的神经网络的权重，如果函数逼近器是一个深度神经网络，那么我们称这个算法为deep q-learning。这里没有softmax层，我们的输出直接就会接近到Q*(s,a)的值，由于一个前向传播就可以计算当前状态下所有action的Q-value，所以效率非常高，接下来就进行反向传播，计算损失函数值关于网络权重θ的梯度并更新它。  

但是这也是有缺点的，由于样本都是相关联的，所以在学习的时候会存在一些不方便，具体表现在每一个状态下的样本会参与多个样本的Q-网络参数的训练，这就造成了训练的冗余和循环。那么我们解决这个问题的方法叫做experience replay经验回放，不断的更新一个replay memory表格，记录了transitions的五个参数(st,at,rt,st+1)，将其存储为experience，然后相较于原来从连续的样本中抽取minibatch，这里直接从replay memory中抽取进行Q-网络的训练。这么做还有利于让每一个transition有利于对多个权重更新有共同的促进作用，从而增加数据处理的效率。  

这里给出了玩Atari游戏的示例，在打砖块的游戏中，一开始它只是试着去寻找球，但是很难接到，但是两个小时的训练之后，它已经可以轻松的接球了，最NB的是过了4小时的训练之后，它能够意识到打出一道缝，让球弹到上面自己在砖块的上部弹来弹去是最方便的，强化学习能够达到这样的智能程度令我很惊讶。  
Q-learning还是有缺点的，比如Q函数实在是太复杂了，那么如果我们要进行一些复杂的强化学习，比如让机器人学会自己动，那么我们需要学习非常准确而又复杂而又数量巨大的状态-动作对，这无疑是非常困难的，那么我们就不能采取这个方法来学习策略。那么进而我们有另外的想法，既然训练Q网络，学习Q函数比较困难，而我们的最终目的又是学习到策略本身，那么我们能不能直接学习策略呢？或者说，我们设计一个算法，从而从无数种可能的策略中挑选出一个来，作为最终的优化策略？  

## Policy Gradients  

我们采取一种叫做政策梯度的方法来完成这个思想。我们首先定义一个政策类，对于类中的每一个政策，我们都定义其value值，然后我们的目标是找到最优策略，从而使得该策略的value J(θ)最大。那么我们的做法是采用一种称作强化算法的算法来解决求J(θ)极大值的问题。在数学领域中，J(θ)可以写作每一个transition τ下轨迹奖励r和p的乘积在dτ方向上的积分，那么J关于θ的梯度就是r和p关于θ的梯度的乘积在τ方向上的积分，而关键是这个积分是不可解的，因为θ是未知的，而p与θ又是相关的，所以这就称为了一个循环的悖论。那么我们的解决措施是用一个数学上的小技巧，将积分同时乘除p，而p的梯度除以p等于logp的梯度，那么p的梯度就等于p乘以logp的梯度，之后我们就可以通过蒙特卡洛采样来估计我们的梯度值。这可以使我们在不知道转换概率的情况下计算这些数量，最终的logp的梯度等于logπ<sub>θ</sub>(at\|st)在t>=0下的和，而J(θ)的梯度就等于r\*logπ<sub>θ</sub>(at\|st)在t>=0下的和。  

那么我们的直觉就是当奖励r(τ)高的时候，提高这种动作出现的概率，当奖励小的时候，降低这种动作出现的概率，但是这种方式是近似的，因为我们会认为如果trajectory轨迹是好的，那么动作就是好的，但是我们的价值函数其实是轨迹中所有动作的平均，也就是说并不是轨迹的所有动作都是好的，所以这种方法经常会出现过拟合。那么我们就要想办法进行方差的降低。事实上，价值函数本身并不是最有用的，真正起作用的是价值函数和期望值的差值，如果当前值在期望值之上，那么说明我们希望要增大该动作出现的频率，而如果当前值在期望值之下，那么就要减小该动作出现的频率，所以我们引入了一个baseline基线的概念，将价值函数更新为原有的价值函数和基线的差，那么基线怎么选择呢？我们可以用简单的方法，那么就是平均之前出现的所有轨迹函数值，但是这总是不准确的，我们深入的想一想，所谓的奖励和基线，其实当前的奖励就是状态-动作对的奖励函数，也就是之前提到的Q-value函数，而基线就是状态的奖励函数，也就是价值函数，那么就回到了我们上面学习的Q-value和value function，所以我们要寻找基线，只需要不断的一起迭代训练我们的Q价值函数和策略就可以了，这种算法叫做Actor-Critic Algorithm，政策policy即使actor，而Q函数即是critic，actor负责决定做哪些行动，而critic负责判断行动的好坏。  

接下来介绍了一些增强学习的例子，首先是Recurrent Attention Model(RAM)，这里要做的事情是进行图像分类，输入一副含有数字的图像，动作是下一个图块的坐标，奖励函数是最后的分类结果正确与否。这里我们可以使用这种RL+RNN的思想来完成该任务，其优点是我们可以重点关注图像的某些特殊区域来获取局部的特征信息，并且我们是以图块的形式读入的，尺寸相较于整个的图片来说很小，减少了计算量。  

这里还有另一个例子，就是我们熟悉的AlphaGo，它其实是一个强化学习和监督学习的结合，它将深度强化学习和很多旧的方法，包括蒙特卡洛搜索树结合。2017年这节课授课的时候，刚好是alphago的升级版和柯洁的比赛，当时alphago1比0获胜，我们现在知道已经3比0获胜了。alphago的整体流程是首先用监督学习的方式来从职业围棋手的交战记录里训练，然后使用policy gradient来自己训练自己，它也同时学习价值网络，也就是actor-critic中的critic，最后在一个蒙特卡洛搜索树中将policy和value值结合在一起。  

后面还有两个lecture，我就不做笔记了，自己过一遍就好了，那么到这里cs231n所有的内容就结束啦，assignment 3我节选了几个做，因为不论是pytorch还是tensorflow都是17年的老版本了，所以做的意义不是很大，我会把做的放到我的github[链接](https://github.com/JustinYuu/Deeplearning-study/tree/master/cs231n/Assignment%203)里，那么到此为止DL的MOOC坑基本上已经填完了，还有tensorflow的最后一门MOOC没有看，抽空补上，以后上这种MOOC的次数将会越来越少，更多的是把一些项目或者科研过程中遇到的问题放在这里记录一下。  

---
本博客支持disqus实时评论功能，如有错误或者建议，欢迎在下方评论区提出，共同探讨。  
