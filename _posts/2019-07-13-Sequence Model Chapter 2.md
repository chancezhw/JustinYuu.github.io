---
layout: post
title: "Sequence Model Chapter 2"
description: "Notes"
categories: [Sequence-Model]
tags: [Python]
redirect_from:
  - /2019/07/13/
---

# Sequence Model Chapter 2  

这周主要介绍词嵌入，主要通过原理、算法、案例三个方面展开。  

## Introduction to Word Embeddings  

在one-hot向量中，不同的单词之间没有相关性，不同的向量中1所在的位置不同，并且值为1的位置只有一个。这个时候为了表示每个单词所蕴含的特征，以更好的找出不同单词之间的关系，我们会采用特征表示的方法，对每个单词的不同特征进行评价，从而可以让机器学习到近义词、反义词等词性之间的联系。  

事实上我们是将低维的词语变成了高维的带有不同特征的词语，这样一种将低维的词语映射到一个高维空间中，换言之把一个维数为所有词的数量的高维空间嵌入到一个维数低得多的连续向量空间中的做法称之为词嵌入。词嵌入是自然语言处理中语言模型和表征学习技术的总称，也是NLP领域最为重要的技术之一。  

词嵌入可以应用到迁移学习中。在NLP的训练中，一般会将大规模的文本作为训练集进行训练，因此我们训练的神经网络会对训练集中出现的所有单词进行特征标记。吴恩达举了一个命名实体的例子，机器将自动识别人名。例如: Sally Johnson is an orange farmer，这个时候由于神经网络知道orange是水果，farmer是职业，因此会判断Sally Johnson是一个人名。而当orange farmer换成其他类似词语组成的职业，比如apple farmer时，由于词嵌入的学习成果迁移到这部分命名识别的任务中，可以很轻松的识别出apple和orange是一类的词语，那么之前相同位置的词语也必将是人名。总结一下，词嵌入的迁移学习步骤如下：首先从大规模的语料库中进行词嵌入，再将结果迁移嵌入到我们需要的任务中，这个时候的训练集量级一般小了很多。最后可以投入新的数据继续进行词嵌入，这一步是可选的。  

在之前学习Siamese网络的时候，我们学习过将人脸进行编码，这里将词语进行嵌入，感觉是有一定的联系的。这里吴恩达详细介绍了一下两者的区别。在人脸识别中，我们的成果可以识别任何人脸，也就是说应用的场景是无限的，但是在词嵌入中，我们只会对在训练集训练到的有限的数据进行嵌入并特征识别。但是人脸识别虽然可以识别无限种人脸，但是人脸的具体特征却是有限的，这和词嵌入有异曲同工之妙。  

analogy reasoning,类比推理，是词嵌入的另一个用途。对于人类而说，Man对应King，而Woman对应Queen是一个自然而然的过程，但是对于机器而言，要让它懂得这层联系，就需要通过类比推理来进行。我们通过把man和woman的特征向量相减，可以得到差异主要体现在gender这一特征层面上，把king和queen的特征向量相减，发现差异还是体现在gender上。那么对于机器而言，知道了man对应king的前提下，要查找woman对应的词语，只需要将man和woman的特征向量相减，在将语料库中的单词逐一与king相减，如果最后的结果大致相同，那么就可以说明该词汇与king对应。那么显然，正常情况下对应的单词应该是queen，这就是机器进行类比推理的过程。这一过程用数学的形式表示，就是寻找sim(e<sub>w</sub>,e<sub>king</sub>-e<sub>man</sub>+e<sub>woman</sub>)的最大值，其中sim是相似度函数，取值一般是两者之间的余弦相似度或者欧氏距离。  

当我们训练学习词嵌入时，我们事实上学习的是一个嵌入矩阵(Embedding matrix)E，这将是一个D×V的矩阵，D代表特征维度，V代表词汇表的大小。在词汇表中我们之前了解到，是用one-hot向量表示的，这个one—hot向量o<sub>i</sub>的规模是V×1，那么我们如果将这两个矩阵相乘，也就是E·o<sub>i</sub>，最后得到的向量就是规模为D×1的向量，我们称之为词向量e<sub>i</sub>。这个词向量由于是由one-hot作为乘子，那么大部分值都是0，只有在第i个值才会是非零值，那么这个词向量就是这个词在E中所有的特征值组成的一个列。换言之，这个乘法的作用就是把规模很大的矩阵E中，提取出了我们需要的那个词的特征向量(那一列)，也就是这个单词在词汇中的嵌入值。值得注意的是，虽然写起来很方便，但是在实际应用中我们可能会用一个专门的函数来从E中查找e<sub>i</sub>，以增加效率。  

## Learning Word Embeddings: Word2vec & Glove  

本节学习两个词嵌入的算法：Word2vec和Glove。与我个人的理解相反，在深度学习词嵌入的历史上，算法事实上是越来越简单的，虽然算法不断变简洁，但是效果仍然很好，这很难得，也很神奇。这节的介绍顺序也是从复杂开始，依次变简单。  

举个栗子，如果我们要通过一句话的前面若干个单词来预测最后一个单词是什么，那么根据上节所学，我们首先要把前面若干个单词的one-hot向量找出来，然后依次乘以E得到前面若干个词在嵌入矩阵中的列向量，之后将这些向量导入神经网络，通过softmax输出一个V（词汇表大小）×1的向量，然后让其目标输出设为训练集中的最后一个单词。接下来进行大量的数据训练，从而训练出一个适当的嵌入矩阵E。  

一般情况下，我们会设置一个固定大小的历史窗口，窗口的大小是一个超参数，需要自行设置、假设我们将窗口设置为4，那么我们总是用前面的4个单词来预测下面一个单词。所以即使这个句子有7个单词，我们也会用第3-6个单词来预测第七个单词。这种做法可以保证输入大小总是固定的，这样可以让我们来处理任意大小的输入。  

接下来我们可以泛化这个算法，再举个栗子： I want a glass of orange juice to go along with my cereal。假设我们要预测juice这个词，在我们之前的设置中，我们要通过之前的4个单词来预测，也就是a glass of orange，这对构建一个特定的语言模型来说很正常，但是如果要学习语言模型本身，也就是为了学习词嵌入矩阵，那么我们可以选择其他位置，比如左边的4个单词+右边的4个单词，或者只是选取目标单词前面那一个单词of，甚至只是选取附近的一个单词，比如glass。很神奇的是就算只选择了附近的一个单词，训练的效果居然也非常好，这种做法叫做skip gram，在后面会学到。  


---
本博客支持disqus实时评论功能，如有错误或者建议，欢迎在下方评论区提出，共同探讨。  
