---
layout: post
title: "Sequence Model Chapter 2"
description: "Notes"
categories: [Sequence-Model]
tags: [Python]
redirect_from:
  - /2019/07/13/
---

# Sequence Model Chapter 2  

这周主要介绍词嵌入，主要通过原理、算法、案例三个方面展开。  

## Introduction to Word Embeddings  

在one-hot向量中，不同的单词之间没有相关性，不同的向量中1所在的位置不同，并且值为1的位置只有一个。这个时候为了表示每个单词所蕴含的特征，以更好的找出不同单词之间的关系，我们会采用特征表示的方法，对每个单词的不同特征进行评价，从而可以让机器学习到近义词、反义词等词性之间的联系。  

事实上我们是将低维的词语变成了高维的带有不同特征的词语，这样一种将低维的词语映射到一个高维空间中，换言之把一个维数为所有词的数量的高维空间嵌入到一个维数低得多的连续向量空间中的做法称之为词嵌入。词嵌入是自然语言处理中语言模型和表征学习技术的总称，也是NLP领域最为重要的技术之一。  

词嵌入可以应用到迁移学习中。在NLP的训练中，一般会将大规模的文本作为训练集进行训练，因此我们训练的神经网络会对训练集中出现的所有单词进行特征标记。吴恩达举了一个命名实体的例子，机器将自动识别人名。例如: Sally Johnson is an orange farmer，这个时候由于神经网络知道orange是水果，farmer是职业，因此会判断Sally Johnson是一个人名。而当orange farmer换成其他类似词语组成的职业，比如apple farmer时，由于词嵌入的学习成果迁移到这部分命名识别的任务中，可以很轻松的识别出apple和orange是一类的词语，那么之前相同位置的词语也必将是人名。总结一下，词嵌入的迁移学习步骤如下：首先从大规模的语料库中进行词嵌入，再将结果迁移嵌入到我们需要的任务中，这个时候的训练集量级一般小了很多。最后可以投入新的数据继续进行词嵌入，这一步是可选的。  

在之前学习Siamese网络的时候，我们学习过将人脸进行编码，这里将词语进行嵌入，感觉是有一定的联系的。这里吴恩达详细介绍了一下两者的区别。在人脸识别中，我们的成果可以识别任何人脸，也就是说应用的场景是无限的，但是在词嵌入中，我们只会对在训练集训练到的有限的数据进行嵌入并特征识别。但是人脸识别虽然可以识别无限种人脸，但是人脸的具体特征却是有限的，这和词嵌入有异曲同工之妙。  

analogy reasoning,类比推理，是词嵌入的另一个用途。对于人类而说，Man对应King，而Woman对应Queen是一个自然而然的过程，但是对于机器而言，要让它懂得这层联系，就需要通过类比推理来进行。我们通过把man和woman的特征向量相减，可以得到差异主要体现在gender这一特征层面上，把king和queen的特征向量相减，发现差异还是体现在gender上。那么对于机器而言，知道了man对应king的前提下，要查找woman对应的词语，只需要将man和woman的特征向量相减，在将语料库中的单词逐一与king相减，如果最后的结果大致相同，那么就可以说明该词汇与king对应。那么显然，正常情况下对应的单词应该是queen，这就是机器进行类比推理的过程。这一过程用数学的形式表示，就是寻找sim(e<sub>w</sub>,e<sub>king</sub>-e<sub>man</sub>+e<sub>woman</sub>)的最大值，其中sim是相似度函数，取值一般是两者之间的余弦相似度或者欧氏距离。  

当我们训练学习词嵌入时，我们事实上学习的是一个嵌入矩阵(Embedding matrix)E，这将是一个D×V的矩阵，D代表特征维度，V代表词汇表的大小。在词汇表中我们之前了解到，是用one-hot向量表示的，这个one—hot向量o<sub>i</sub>的规模是V×1，那么我们如果将这两个矩阵相乘，也就是E·o<sub>i</sub>，最后得到的向量就是规模为D×1的向量，我们称之为词向量e<sub>i</sub>。这个词向量由于是由one-hot作为乘子，那么大部分值都是0，只有在第i个值才会是非零值，那么这个词向量就是这个词在E中所有的特征值组成的一个列。换言之，这个乘法的作用就是把规模很大的矩阵E中，提取出了我们需要的那个词的特征向量(那一列)，也就是这个单词在词汇中的嵌入值。值得注意的是，虽然写起来很方便，但是在实际应用中我们可能会用一个专门的函数来从E中查找e<sub>i</sub>，以增加效率。  

## Learning Word Embeddings: Word2vec & Glove  

本节学习两个词嵌入的算法：Word2vec和Glove。与我个人的理解相反，在深度学习词嵌入的历史上，算法事实上是越来越简单的，虽然算法不断变简洁，但是效果仍然很好，这很难得，也很神奇。这节的介绍顺序也是从复杂开始，依次变简单。  

举个栗子，如果我们要通过一句话的前面若干个单词来预测最后一个单词是什么，那么根据上节所学，我们首先要把前面若干个单词的one-hot向量找出来，然后依次乘以E得到前面若干个词在嵌入矩阵中的列向量，之后将这些向量导入神经网络，通过softmax输出一个V（词汇表大小）×1的向量，然后让其目标输出设为训练集中的最后一个单词。接下来进行大量的数据训练，从而训练出一个适当的嵌入矩阵E。  

一般情况下，我们会设置一个固定大小的历史窗口，窗口的大小是一个超参数，需要自行设置、假设我们将窗口设置为4，那么我们总是用前面的4个单词来预测下面一个单词。所以即使这个句子有7个单词，我们也会用第3-6个单词来预测第七个单词。这种做法可以保证输入大小总是固定的，这样可以让我们来处理任意大小的输入。  

接下来我们可以泛化这个算法，再举个栗子： I want a glass of orange juice to go along with my cereal。假设我们要预测juice这个词，在我们之前的设置中，我们要通过之前的4个单词来预测，也就是a glass of orange，这对构建一个特定的语言模型来说很正常，但是如果要学习语言模型本身，也就是为了学习词嵌入矩阵，那么我们可以选择其他位置，比如左边的4个单词+右边的4个单词，或者只是选取目标单词前面那一个单词of，甚至只是选取附近的一个单词，比如glass。很神奇的是就算只选择了附近的一个单词，训练的效果居然也非常好，这种做法叫做skip gram，在后面会学到。  

接下来介绍第一种模型Word2vec，这是一种简单高效的算法。Word2vec需要用到我们刚刚接触的skip gram算法。我们知道skip gram是选择一个特定的单词，那么特定的单词的选择范围也有一个特定的区间window，在该区间内随机选取一个单词作为target，与之前选择的context单词组成一个context-target对。通过训练做到给定context预测在window区间内部的target。我们可以很明显的感觉出，这一任务其实很难做到，因为在一个window范围内，可能会有很多种可能的单词存在，但其实我们的目标并不在于这个监督学习任务本身，而是想使用这个学习问题来学习构造一个好的词嵌入模型。  

Skip gram的模型很简单，首先输入context单词的one-hot向量，乘以E得到嵌入词向量e，然后输入到softmax层，输出一个V维向量，从而得到词汇表内每个单词出现的概率。但是这个简单的模型有一个缺点，即softmax的计算量太大，每次都要对整个单词表内的所有单词计算求和，这就会导致词汇表很大的时候计算速度会非常慢。对于这个现象的解决方式是采用Hierarchical Softmax Classifier，通过二叉树状的分层分类器，按照类别对单词进行不同层次的分类，形成一个多层的二叉树。分层完毕后，只需要找到单词具体的分支然后将分支内的计算求和即可，这会将O(n)级别的空间复杂度变成O(logn)。在实际情况中，会出现经常出现的单词排在树根附近，而不经常出现的分支排在叶子附近，这样可以降低查找的时间，这是数据结构的基础内容，就不赘述了。Context的抽样是随机抽取的，但是保证其随机抽取需要用一些方法，来避免均匀分布抽样带来的常用单词抽取频率过大的问题。在实际情况中，我们会使用一系列不同的分布来进行抽样，从而达到常用单词和非常用单词都会较为平均的被选取。值得一提的是，在应用中一般使用Huffman树进行输出层优化。  

Word2vec模型在论文原文中提到了两种实现方式，一种是skip-gram，而另一种叫做CBOW-连续词袋模型（Continuous Bag-Of-Words Model），通过获取中间词两边的词语来预测中间的单词，这种预测方式和skip gram完全相反。这里贴一篇博客，里面对Word2vec的两种模型进行了详细的推导：[Word2Vec概述与基于Hierarchical Softmax的CBOW和Skip-gram模型公式推导](https://blog.csdn.net/liuyuemaicha/article/details/52611219)，以及刘建平大大的博客[word2vec原理(二) 基于Hierarchical Softmax的模型](https://www.cnblogs.com/pinard/p/7243513.html)  

除了skip-gram模型之外，我们还有另外一种模型：Negative Sampling负采样法，这也是Word2vec的模型之一。Skip-gram使用霍夫曼树来降低softmax的计算负担，但是如果训练样本的中心词是一个非常偏僻的词，那么就要在霍夫曼树中查找很久，因此我们采用负采样这一方法来让模型变的更加简单。首先我们定义一个新的监督学习问题：给定一个单词对，预测两个单词是否为context-target对。首先生成一个正样本，生成方式是首先选择一个context，在给定的window范围内随机选择一个target。之后选择同样的context，生成负样本，对应的target从词汇表中随机选择。随机选择的target我们统一标注为负样本，即使该组合有可能成为一个正样本。具体的正负样本比例为1：k，k的值一般在小数据集下取值为5-20，大数据集下为2-5。  

由上文可知我们构造的监督学习的任务是给定一个单词，判断是否与context组成context-target对，那么监督学习的x就是context-word对，y是target的bool标签。换个角度理解，我们的任务也可以看做是区分两种单词对：context-target和随机选取的context-word。给定特定的notation如下：用c代表context，y代表target，t代表word。那么我们的model就是给定c和t的前提下，输出y=1的概率。那么这个模型的优势在于，给定的context是固定的，那么对应的词向量也就是固定的，我们只需要用sigmoid函数来计算词向量与不同的单词组成的函数值是1还是0，那么我们完全没有必要把词汇表中所有的单词都拿来比对一番，由上文可知我们选取了特定的值k，每次我们只需要进行k次操作即可，大大减轻了我们计算的负担。  

那么负样本的选择方法是怎样的呢？如果采用在语料库中进行经验采样，那么就会出现频率高的单词采样多的情景，那么如果完全不考虑出现频率，将所有单词进行平均采样的话，又没有考虑单词出现的实际频率。所以负样本这篇论文中写道，根据经验，按照词频的3/4次幂进行抽样，这种介于均匀分布和经验分布之间的启发式采样效果最好。  

最后一节学习了GloVe算法(global vectors for word representation),它的势头没有Word2vec和skip-gram那么猛，但是非常简单，也受到一些人的欢迎。简而言之，这个算法用X<sub>ij</sub>来代表单词i出现在单词j上下文的次数，上下文一般定义为一个window范围内，比如window=10，所以说X<sub>ij</sub>和X<sub>ji</sub>是相对称的，表示i和j一起出现的关联程度。我们定义模型为ΣiΣjf(X<sub>ij</sub>)(θ<sub>i</sub>X<sup>t</sup>e<sub>j</sub>+b<sub>i</sub>+b<sub>j</sub>-log(X<sub>ji</sub>))<sup>2</sup>。  
常用词的权重不能过大，罕见词的权重也不能过小，可以用多种启发式的方法来确定权重函数，这可以在GloVe的论文中看到。此外，在上述公式中，θ和e是对称的，而且他们在公式中起到了相同的作用，因此我们将θ和e的平均值作为最终的词向量e。  

此外，吴恩达提到在词嵌入的时候，我们的特征属性不一定是可理解的。详细来讲，在我们之前的例子中，我们会用gender，royal，age等特征值来评判一个词汇，但是在实际运算中，神经网络所用的特征属性并不一定刚好就是我们所设计的这几个，有可能是我们不能够理解但是却最终有用的。这是因为在正交变换和转置变换中，很容易将本身是可理解的矩阵值转化为平行的、转置的、可逆的值，所以很难保证经过一系列矩阵变换和运算后，词向量的值还与我们一开始设定的特征属性对应，但是它仍然是有效的。  

## Application using Word Embeddings  

本节介绍词嵌入的两个应用：情感分类和偏见词嵌入。  

情感分类是指通过一段文字来判断该文字透露出的情感是正面的还是负面的。情感分类的一个挑战是数据集可能不够大，但是应用词嵌入可以在较小的训练集下达到较好的分类效果。我们的监督学习方法是给定一句话，输出情感的正向级别/负向级别。那么对应的模型很简单：将所有的输入单词通过嵌入矩阵E转换为词向量e，之后将所有的向量相加后取平均，通过softmax层输出1-5的评分。但是这个模型对于含有正向评价词语非常多的负面评价效果很差，比如"Completely lacking in good taste,good service,and good ambience"。这个时候就需要我们用RNN进行改进，将词向量输入到上周学过的many-to-one RNN网络，然后通过softmax进行分类。词嵌入可以使得该算法在遇到训练时没有遇到过的单词的时候也能做出正确的判断。  

人们的认知中可能会出现一系列偏见，比如种族偏见、性别偏见、性取向偏见等，我们要尽量让自己的算法没有这种不好的偏见存在，从而更好的帮助人类做出决策。比如说，男人对应程序猿，女人对应家庭主妇，这就是一个不好的示例；男人对应医生，女人对应护士，这也是一个不好的示例。这是由于词嵌入会反映性别、种族、年龄、性取向以及其他的偏见，这些偏见是由训练文本造成的，所以我们要尽量消除这种影响严重的偏见。  



---
本博客支持disqus实时评论功能，如有错误或者建议，欢迎在下方评论区提出，共同探讨。  
