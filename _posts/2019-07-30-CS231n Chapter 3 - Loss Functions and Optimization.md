---
layout: post
title: "CS231n Chapter 3 - Loss Functions and Optimization"
description: "Notes"
categories: [CS231n]
tags: [Python]
redirect_from:
  - /2019/07/30/
---

# CS231n Chapter 3 - Loss Functions and Optimization      

## Introduction  

在上节课中，我们已经引入了损失函数的概念，在课程给出的笔记2中也明确的说了损失函数即为我们的算法给出的预测值和正确值之间的差距大小，而我们的目标就是要尽可能的降低损失函数值的大小。那么这堂课的主要内容就是探究一下损失函数的不同形式以及损失函数的一些优化方法。  

## Linear classification II  

上周的线性分类准确的讲还没有完全讲完，这里将上节课的例子缩小了一下，将识别的10个类别变成了3个，这里Justin试图用这个例子向我们展示最后的损失函数的样子，即将每一个小的损失函数加起来取平均值，不仅仅是线性分类，这也是几乎所有损失函数的格式。如果看了上周给出的第二个笔记，我们会发现在第二个笔记的后半部分多了一些关于SVM的内容，这些内容就是这里所要将的线性分类的第二部分。  

这里介绍的算法乘坐Multiclass SVM，它的损失函数称为Hinge loss，铰链损失。我们要定义一个margin值，保证正确类别的得分与其他类别的得分差值大于等于这个margin，预测才是有效的，否则损失函数不为0。这被称为SVM的原因这就是保证了有一个足够大的margin。这里提出了一系列问题，很多问题都很简单，但是我对最后一个问题很感兴趣，这里的损失函数是将所有的损失相加后取平均（或者不取，whatever），但是问题是如果将损失取平方和相加会不会产生不同的影响呢？答案是会的，我们现在做的是线性分类，如果要取平方和那么就是非线性的运算，得出的结果是不同的。但是这里学生提问说到，为什么要考虑使用平方和函数呢？那么这里一次方和二次方的区别比较明显，就是二次方更明显的放大了损失，提高了损失带来的负面影响，而一次方比较缓和的减轻了损失带来的影响，各有各的用处，需要根据情况来选择。  

这里会产生另外一个比较严重的问题，W是unique的吗？也就是说会不会存在多个相同的W同时使损失函数相同？答案是肯定的，显然将W放大任意倍得到的结果都是相同的，因为本来就大于margin了，放大之后肯定还是大于margi n的，甚至会更大于margin，所以我们的W是不唯一的。这里就引出了正则化，对W增加一些限制来移除一些不确定性，这里介绍了奥卡姆剃刀，即“如无必要，勿增实体”，正则化正是奥卡姆剃刀这一思想的有效利用，将我们的模型尽可能的简单化，也就是避免过拟合。那么这里有一些超参数，比如lambda需要我们自行调整设置，lambda的作用是作为一个软约束，将更为高阶的多项式模型选用带来的损失函数值变得更大，从而迫使模型变得更为简单。当然这是从感性的角度来解释的，要从理性的角度来解释，要参考本篇博客之前Machine Learning MOOC中关于正则化的公式推导。一般情况下，我们的正则化方法有L2正则化，L1正则化，弹性网络，最大范数正则化和Dropout。  

在Multiclass领域中，我们刚刚学到了Multiclass SVM，事实上我们还有一种应用更为广泛的分类器：Softmax。这里我们将输出的向量值经过指数运算，然后将其百分比作为输出值输出出来，然后取负对数作为损失函数值，这也就是一个交叉熵运算。在这里我们没有设置一个固定的margin，我们只是将输出的值匹配了一种和为1的概率，这其实就是一个似然估计，将输出值的概率输出出来，然后选取最大的一个概率作为最有可能的label值。  

那么我们就可以将softmax和multiclass SVM做一个对比，明显的区别在于损失函数：对于SVM，我们使用hinge loss，看看差值和margin的对比；对于Softmax，我们采用交叉熵函数，将结果用概率的形式表示。

---
本博客支持disqus实时评论功能，如有错误或者建议，欢迎在下方评论区提出，共同探讨。  
