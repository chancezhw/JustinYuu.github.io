---
layout: post
title: "CS231n Chapter 3 - Loss Functions and Optimization"
description: "Notes"
categories: [CS231n]
tags: [Python]
redirect_from:
  - /2019/07/30/
---

# CS231n Chapter 3 - Loss Functions and Optimization      

## Introduction  

在上节课中，我们已经引入了损失函数的概念，在课程给出的笔记2中也明确的说了损失函数即为我们的算法给出的预测值和正确值之间的差距大小，而我们的目标就是要尽可能的降低损失函数值的大小。那么这堂课的主要内容就是探究一下损失函数的不同形式以及损失函数的一些优化方法。  

## Linear classification II  

上周的线性分类准确的讲还没有完全讲完，这里将上节课的例子缩小了一下，将识别的10个类别变成了3个，这里Justin试图用这个例子向我们展示最后的损失函数的样子，即将每一个小的损失函数加起来取平均值，不仅仅是线性分类，这也是几乎所有损失函数的格式。如果看了上周给出的第二个笔记，我们会发现在第二个笔记的后半部分多了一些关于SVM的内容，这些内容就是这里所要将的线性分类的第二部分。  

这里介绍的算法乘坐Multiclass SVM，它的损失函数称为Hinge loss，铰链损失。我们要定义一个margin值，保证正确类别的得分与其他类别的得分差值大于等于这个margin，预测才是有效的，否则损失函数不为0。这被称为SVM的原因这就是保证了有一个足够大的margin。这里提出了一系列问题，很多问题都很简单，但是我对最后一个问题很感兴趣，这里的损失函数是将所有的损失相加后取平均（或者不取，whatever），但是问题是如果将损失取平方和相加会不会产生不同的影响呢？答案是会的，我们现在做的是线性分类，如果要取平方和那么就是非线性的运算，得出的结果是不同的。但是这里学生提问说到，为什么要考虑使用平方和函数呢？那么这里一次方和二次方的区别比较明显，就是二次方更明显的放大了损失，提高了损失带来的负面影响，而一次方比较缓和的减轻了损失带来的影响，各有各的用处，需要根据情况来选择。  

这里会产生另外一个比较严重的问题，W是unique的吗？也就是说会不会存在多个相同的W同时使损失函数相同？答案是肯定的，显然将W放大任意倍得到的结果都是相同的，因为本来就大于margin了，放大之后肯定还是大于margi n的，甚至会更大于margin，所以我们的W是不唯一的。这里就引出了正则化，对W增加一些限制来移除一些不确定性，这里介绍了奥卡姆剃刀，即“如无必要，勿增实体”，正则化正是奥卡姆剃刀这一思想的有效利用，将我们的模型尽可能的简单化，也就是避免过拟合。那么这里有一些超参数，比如lambda需要我们自行调整设置，lambda的作用是作为一个软约束，将更为高阶的多项式模型选用带来的损失函数值变得更大，从而迫使模型变得更为简单。当然这是从感性的角度来解释的，要从理性的角度来解释，要参考本篇博客之前Machine Learning MOOC中关于正则化的公式推导。一般情况下，我们的正则化方法有L2正则化，L1正则化，弹性网络，最大范数正则化和Dropout。  

在Multiclass领域中，我们刚刚学到了Multiclass SVM，事实上我们还有一种应用更为广泛的分类器：Softmax。这里我们将输出的向量值经过指数运算，然后将其百分比作为输出值输出出来，然后取负对数作为损失函数值，这也就是一个交叉熵运算。在这里我们没有设置一个固定的margin，我们只是将输出的值匹配了一种和为1的概率，这其实就是一个似然估计，将输出值的概率输出出来，然后选取最大的一个概率作为最有可能的label值。  

那么我们就可以将softmax和multiclass SVM做一个对比，明显的区别在于损失函数：对于SVM，我们使用hinge loss，看看差值和margin的对比；对于Softmax，我们采用交叉熵函数，将结果用概率的形式表示。  

## Optimization, stochastic gradient descent  

接下来考虑对损失函数的优化，首先我们会想到全部随机W的值，那么表现当然会不太好，实际测试中准确率只有15%左右。另一个想法是跟随梯度下降最快的方向，也就是梯度下降算法，始终在当前导数最小的方向下降，那么就会以最快的速度降低损失函数的值。梯度分为数值梯度和解析梯度，其实这就是两种不同的计算梯度的方法，对于数值梯度而言，在每一个维度上都在原数值上加一个小的h，然后计算这个维度/方向上的偏导，最后组合在一起得到梯度。由于h必须比较小，所以时间会很慢，计算起来会比较麻烦，但是准确度非常高。而解析梯度就是直接用微积分的求导方式来计算的，省去了每一个维度上的导数定义计算环节，这种方法更为快速。这里我们经常会选取一部分参数用于数值梯度的计算，将计算结果和解析梯度的结果作为对比，从而检验我们的解析梯度是否正确。  

接下来需要介绍的是随机梯度下降SGD，在实际情况中批量梯度下降，也就是将所有样本的梯度全部更新一遍，是非常准确但又非常耗时的，所以我们一般采用随机梯度下降，即每次迭代随机选择一个样本进行更新的方式。但是这种方式会存在一些问题，比如随机选择一个样本导致可能陷入局部最优，那么我们就采取了一个相对折中的方法，称作小批量随机梯度下降，定义一个超参数batch，在每次迭代中都随机选择batch个样本进行梯度的更新，从而结合SGD和BGD的优点。  

## Image Features  

最后还提到了一些捕捉图像特征的方法。与其他数据不同，在图像处理领域，我们不是直接将图像的所有特征或者原始数据直接输入到我们的神经网络中，而是把不同的特征提取出来然后连接到一起。此外，我们还可以采取特征变换/像素变换的方式来将图像的特征变得更加线性可分，比如将直角坐标系变成极坐标。还有一个颜色直方图的trick，将每个像素的颜色映射到一个颜色bucket中，最后根据直方图的高度来确定图片中的何种颜色占据主导地位。  

这里是本周的两个官方笔记的链接：[Note1](https://cs231n.github.io/linear-classify/),[Note2](https://cs231n.github.io/optimization-1/)  

---
本博客支持disqus实时评论功能，如有错误或者建议，欢迎在下方评论区提出，共同探讨。  
