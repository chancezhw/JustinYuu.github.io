---
layout: post
title: "CS231n Chapter 10 - Recurrent Neural Networks"
description: "Notes"
categories: [CS231n]
tags: [Python]
redirect_from:
  - /2019/08/05/
---

# CS231n Chapter 10 - Recurrent Neural Networks  

## Introduction  

循环神经网络用于处理序列状的数据，例如一段话、一个句子、一段音频等。基于序列的种类，RNN有多种形式，比如1 to 1，1 to many,many to 1,many to many等等，这里举了中法翻译的例子，这和DeepLearning.ai的课程引例是一样的。对于非序列状的数据，我们其实也可以进行序列化的处理，比如对图像进行序列化处理，通过某些操作和循环神经网络，我们可以让图像随着时间的推进不断的生成新的部分。那么这一节课的主要内容就是介绍RNN的原理及变体。  

## Recurrent Neural Network  

RNN的原理可以用一个公式来表示，这个公式在之前的MOOC中没有提及到，所以我在这里记录下来，公式是：h<sub>t</sub> = f<sub>w</sub>(h<sub>t-1</sub>,x<sub>t</sub>)，h<sub>t</sub>代表t时间步内的状态，也就是将要呈现的新状态，f<sub>w</sub>代表与参数W相关的一些函数，里面的自变量有两个，分别是上一时间步的旧状态h<sub>t-1</sub>，和这一事件步的输入向量x<sub>t</sub>。那么从这个公式中我们就可以明显的看出，与之前的ANN和CNN不同的是，我们的输出不仅仅与输入的数据有关，还与之前的状态有关，两者共同决定当前时间步的输出，那么我们再往前推一步，之前时间步的状态又与之前的数据和之前时间步之前的时间步有关，究其根本还是与之前的输出有关，所以每一时间步的输出都会直接或者间接的影响到后面所有时间步的输出，所以称之为Recurrent Neural Network，这个概念还是很好理解的。  

值得注意的是每一步的函数f和参数W都是完全一样的，之后我们会用另一个权重W<sub>ht</sub>来与结果h相乘，将其转变为输出y，这里我们一般初始化参数为0。这里会出现两种典型的情况，one to many和Many to one，many to one就是在每一个时间步都会输入新数据，而one to many就是在每一个时间步输出新数据，many to many是这两种情况的结合，我们将其分为两个阶段。第一阶段我们称之为encoder，我们的encoder模型类似于many to one，每一个时间步都有数据输入，输出一个最终的隐藏状态。第二个阶段我们称之为decoder，我们的decoder模型使用另外一套权重W，类似于one to many，在每一个时间步都产生输出。  

这里补充了一下实时输出的细节，我们一般是通过softmax生成概率向量，然后我们并不是采取概率最高的结果作为最终的结果，而是通过随机采样抽取一个作为输出的值，那么自然问题就出现了，为什么不选概率最高的例子？在这里因为大家概率都不高所以我们采取采样更好一些，在实际情况中使用argmax选取会更稳定。但是值得一提的是，稳定不代表优秀，有时候多样性的输出反而是好事，比如随机采样可以使我们在相似的输出时避免得到完全重复的结果。这里还有同学提问为什么测试的时候输入的时候要使用one-hot向量来代替输出的softmax向量，不得不说stanford的学生提问都很刁钻，每一个没有讲清楚的地方都问到了。这里原因有2，第一个原因是测试和训练的时候模型的表现是非常不同的，第二个原因是由于词汇表可能会非常大，所以使用one-hot这种较为稀疏的向量会使得计算更为简便。  

接下来讲一下反向传播，这里用常规方法进行反向传播的制约因素在于由于时间步可能会相当长，因此完全反向传播会耗费大量的时间和空间，导致计算负担难以承受。所以我们采取truncated反向传播的方式，选择一个特定的长度，我们的反向传播在该长度下进行，也就说将整个序列分为多个子序列，在经过每一个序列的时候进行反向传播，不过整个阶段正向传播是完全一样的，对整个序列长度进行正向传播。这个做法的理论依据是太过遥远的时间步对现在时间步的影响是非常有限的，所以为了计算的简洁性我们可以将其忽略不计。  

这里演示了几个RNN自动生成的例子，比如自动生成莎士比亚十四行诗，或者自动生成一些代数拓扑教科书，这个真的是笑死我了hhh，生成的的确和正常的一样啥都看不懂……它居然还会自己作图和省略证明……我佛了。  

此外Justin他们还用Linux源码来训练，得到了一些类C语言的自动生成结果，这些代码会自己调用if等简单函数，自动缩进，甚至会自己编写注释……不过问题在于，它懂得声明变量，但是却不总是使用它声明的变量，甚至使用一些没有声名的变量……  

回到正题，在这门计算机视觉课程中介绍RNN的意义是什么呢？原来RNN可以和CNN进行结合，比如image captioning，可以给图片添加标题或者描述。具体的做法是通过若干CNN层和最后的FC层之后，我们不和以前一样直接输出softmax向量，而是将结果向量作为RNN网络的开始，由于有CNN的介入，我们的RNN网络将有3个W矩阵，分别是之前的W<sub>xh</sub>、W<sub>hh</sub>和现在新增的W<sub>ih</sub>，将其乘以v，也就是我们新增的每一个时间步的图像信息。我们通过这个CNN+RNN的网络可以从图像中识别含义，并用语句来表示，比如生成图像的标题或者描述。这里也展示了一些正确和错误的案例，in general，图像中的元素已经学习过的时候表现一般很好，但是存在未学习的元素的时候描述一般比较差，但是总体看来给图片起标题还是比较靠谱的。  

为了优化上述算法，有一个改进级别的算法叫做Image Captioning with Attention，该算法的方法是用向量网络来替代向量，作为CNN网络的输出结果。此外随着时间步的进行，不但在每个时间步周采样词汇，同时也产生了一个算法想捕捉的元素在原图像（向量网络）中的分布，所以在每个时间步，都会出现两个输出，一个是在词汇表的分布，以softmax向量的形式得出，另一个是在原图像（向量网络）中的分布，在图像中的分布返回到向量网络中，得到一个单独的摘要向量，内容是应该着重注意的图像位置，该摘要向量作为下一个时间步的一个输入导入到下一个时间步中，其作用是调整算法着重注意的图像位置。所以在Attention模型中，每一个时间步都有三个输入和两个输出。  

RNN也是有深度的，我们可以将多层RNN并列在一起，组成一个多层RNN网络，但是RNN由于其独特的时序性，反向传播需要back很多个时间步，这就导致其深度不可能特别深。  

下面我们研究RNN的反向传播。首先看RNN的前向传播，一般情况下激活函数是tanh，那么h<sub>t</sub> = tanh(W<sub>hh</sub>h<sub>t-1</sub>+W<sub>xh</sub>x<sub>t</sub>)，我们也可以将W和h,x写成向量相乘的形式，这就是单层下RNN的前向传播。那么后向传播的时候，我们会收到上一层计算完毕的L对ht的梯度，然后我们所要做的就是计算L对h<sub>t-1</sub>的梯度，首先，梯度需要通过tanh，然后通过矩阵相乘。这里我们会发现对于h的梯度计算，结果会含有权重矩阵W里的大量参数，而W的参数是相同的，再加上经过多重的tanh，那么多个RNN cell会出现乘以多个相同数值的情况，这会出现梯度要么爆炸要么消失的尴尬情况（相同的值恰好为1的概率太小太小了），所以这就是RNN经常出现的梯度爆炸和梯度消失问题，这和吴恩达deeplearning MOOC中讲的原因略有不同，不过显然更好理解。  

为了解决这个问题，一个称作LSTM的算法诞生了。LSTM，全称为Long Short Term Memory，其运用遗忘门、输入门、输出门、更新门ifog四个门来控制每一个单元的输入和输出，前三个门使用sigmoid函数，而后一个门使用tanh函数。具体的公式在我当时的笔记里有，这里是[链接](http://justin-yu.me/blog/2019/07/05/Sequence-Model-Chapter-1/)。  

LSTM的梯度可以平行朝后传播，这一段梯度传播流是不间断的，就想在高速公路上一样，这类似于ResNet，梯度也是通过一段快速的道路迅速反向传播。这两种都是应用了一个叫做Highway Network的思想，这本身就是一篇论文，可以查阅一下。  

LSTM还有一些变种，比如GRU，在我上面链接的那篇博客中也有介绍。  

---
本博客支持disqus实时评论功能，如有错误或者建议，欢迎在下方评论区提出，共同探讨。  
