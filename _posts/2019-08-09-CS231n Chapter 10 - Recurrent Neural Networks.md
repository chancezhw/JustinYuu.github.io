---
layout: post
title: "CS231n Chapter 10 - Recurrent Neural Networks"
description: "Notes"
categories: [CS231n]
tags: [Python]
redirect_from:
  - /2019/08/05/
---

# CS231n Chapter 10 - Recurrent Neural Networks  

## Introduction  

循环神经网络用于处理序列状的数据，例如一段话、一个句子、一段音频等。基于序列的种类，RNN有多种形式，比如1 to 1，1 to many,many to 1,many to many等等，这里举了中法翻译的例子，这和DeepLearning.ai的课程引例是一样的。对于非序列状的数据，我们其实也可以进行序列化的处理，比如对图像进行序列化处理，通过某些操作和循环神经网络，我们可以让图像随着时间的推进不断的生成新的部分。那么这一节课的主要内容就是介绍RNN的原理及变体。  

## Recurrent Neural Network  

RNN的原理可以用一个公式来表示，这个公式在之前的MOOC中没有提及到，所以我在这里记录下来，公式是：h<sub>t</sub> = f<sub>w</sub>(h<sub>t-1</sub>,x<sub>t</sub>)，h<sub>t</sub>代表t时间步内的状态，也就是将要呈现的新状态，f<sub>w</sub>代表与参数W相关的一些函数，里面的自变量有两个，分别是上一时间步的旧状态h<sub>t-1</sub>，和这一事件步的输入向量x<sub>t</sub>。那么从这个公式中我们就可以明显的看出，与之前的ANN和CNN不同的是，我们的输出不仅仅与输入的数据有关，还与之前的状态有关，两者共同决定当前时间步的输出，那么我们再往前推一步，之前时间步的状态又与之前的数据和之前时间步之前的时间步有关，究其根本还是与之前的输出有关，所以每一时间步的输出都会直接或者间接的影响到后面所有时间步的输出，所以称之为Recurrent Neural Network，这个概念还是很好理解的。  

值得注意的是每一步的函数f和参数W都是完全一样的，之后我们会用另一个权重W<sub>ht</sub>来与结果h相乘，将其转变为输出y，这里我们一般初始化参数为0。这里会出现两种典型的情况，one to many和Many to one，many to one就是在每一个时间步都会输入新数据，而one to many就是在每一个时间步输出新数据，many to many是这两种情况的结合，我们将其分为两个阶段。第一阶段我们称之为encoder，我们的encoder模型类似于many to one，每一个时间步都有数据输入，输出一个最终的隐藏状态。第二个阶段我们称之为decoder，我们的decoder模型使用另外一套权重W，类似于one to many，在每一个时间步都产生输出。  

这里补充了一下实时输出的细节，我们一般是通过softmax生成概率向量，然后我们并不是采取概率最高的结果作为最终的结果，而是通过随机采样抽取一个作为输出的值，那么自然问题就出现了，为什么不选概率最高的例子？在这里因为大家概率都不高所以我们采取采样更好一些，在实际情况中使用argmax选取会更稳定。但是值得一提的是，稳定不代表优秀，有时候多样性的输出反而是好事，比如随机采样可以使我们在相似的输出时避免得到完全重复的结果。这里还有同学提问为什么测试的时候输入的时候要使用one-hot向量来代替输出的softmax向量，不得不说stanford的学生提问都很刁钻，每一个没有讲清楚的地方都问到了。这里原因有2，第一个原因是测试和训练的时候模型的表现是非常不同的，第二个原因是由于词汇表可能会非常大，所以使用one-hot这种较为稀疏的向量会使得计算更为简便。  

接下来讲一下反向传播，这里用常规方法进行反向传播的制约因素在于由于时间步可能会相当长，因此完全反向传播会耗费大量的时间和空间，导致计算负担难以承受。所以我们采取truncated反向传播的方式，选择一个特定的长度，我们的反向传播在该长度下进行，也就说将整个序列分为多个子序列，在经过每一个序列的时候进行反向传播，不过整个阶段正向传播是完全一样的，对整个序列长度进行正向传播。这个做法的理论依据是太过遥远的时间步对现在时间步的影响是非常有限的，所以为了计算的简洁性我们可以将其忽略不计。  

---
本博客支持disqus实时评论功能，如有错误或者建议，欢迎在下方评论区提出，共同探讨。  
