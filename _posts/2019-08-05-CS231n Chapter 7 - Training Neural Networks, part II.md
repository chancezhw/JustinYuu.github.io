---
layout: post
title: "CS231n Chapter 7 - Training Neural Networks, part II"
description: "Notes"
categories: [CS231n]
tags: [Python]
redirect_from:
  - /2019/08/05/
---

# CS231n Chapter 7 - Training Neural Networks, part II    

## Introduction  

在开始这节课内容之前，Justin带我们回忆了一下上节课的内容，有一点我没有记录到，就是在超参数的搜索方面，如果超参数数量比较多，那么与其网格化的搜索，不如进行随机搜索，这样得到的效果反而会好一些，也更为省时、节省计算开支，这一点在吴恩达MOOC中也有讲到([链接](http://justin-yu.me/blog/2019/04/08/Improving-Deep-Neural-Networks-Hyperparameter-tuning,-Regularization-and-Optimization-Chapter-3/))，不过具体的随机搜索方法没有讲清楚，这里是一篇知乎回答[链接](https://www.zhihu.com/question/57394983)可供参考，简单来讲，grid search的每一步搜索并不能保证是有效的，而random search的每一步都采样在超参数的分布空间中，因此每一步都是有效的，所以效率更高。  

这一周将会沿着上一周的内容，进一步学习一些更为有效的优化方法，以及介绍正则化和迁移学习这两个非常重要的trick。  

## Fancier optimiaztion  

首先看一看SGD随机梯度下降的缺点，首先可以想到的就是优化过程中的当前下降方向不稳定，从而导致曲折下降的zig-zag状况。这还不是最致命的，最严重的问题在于如果损失函数遇到了一个局部最优值或者一个鞍点，那么导数就会变成0，那么梯度下降就会停止。事实上局部最优其实很难达到，因为在高维空间中，局部最优点需要在所有的维度都达到最优，那这是很难发生的，但是鞍点不同，鞍点可能在某些方向上升，但在某些方向是下降的，和局部最优点的相同点是它在所有维度的梯度也都等于0，用数学语言表示，局部最优点的Hessian矩阵是正定的(行列式等于0)，但是鞍点的Hessian矩阵是非正定的。在高维空间中，鞍点通常被相同误差值的平面Plateau所包围，这个平面上的梯度都接近于0，所以梯度下降的过程中会长时间卡在这个区域无法动弹。那么我们可以比较容易理解到，鞍点存在的情况比局部最优点多得多，并且局部最优点其实已经很好了，很多时候都可以满足我们的需要，鞍点众多也正是牛顿法整体效率低于梯度下降法的原因。  

Momentum是一种可以和SGD一起进行的梯度优化方法，动量法通过计算梯度的指数加权平均来作为梯度从而更新权重，那么带来直观的感受就是，如果当前梯度的更新方向与累积的历史方向一致，那么当前的梯度将会得到加强，从而这一步下降的幅度更大；反之当前步数下降的幅度会更小。关于Momentum的公式，这里有两种形式，一种是cs231n和原论文给出的公式：v<sub>t+1</sub> = μv<sub>t</sub> - ε▽f(θ<sub>t</sub>)， θ<sub>t+1</sub> = θ<sub>t</sub> + v<sub>t+1</sub>，其中epsilon是学习率，μ是momentum项系数。那么另一种就是吴恩达在授课的时候给出的公式，这也用于TensorFlow，keras等框架中，具体的公式为 V<sub>dw</sub> = βV<sub>dw</sub> + (1-β)dW,  V<sub>db</sub> = βV<sub>db</sub> + (1-β)db， W = W - αV<sub>dw</sub>, b = b - αV<sub>db</sub>，我觉得这一个公式比较清晰明确，β一般取值为0.5,0.9,0.99，其大小表示之前的梯度对当前方向的影响。  

此外还有一种Momentum的变种：Nesterov Momentum，这里的区别就是增加了一个校正参数，总体的公式是 V<sub>dW</sub> = βV<sub>dW</sub> - αdW， V<sub>db</sub> = βV<sub>db</sub> - αdb， W = W + βV<sub>dW</sub> - αdW, b = b + βV<sub>db</sub> - αdb。另一种优化方式称为AdaGrad，全称为自适应梯度，其思想是在一轮更新权重的时候不同的参数采用不同的学习率，其具体的公式是Gw = Gw + (dW)², W = W - α/(√Gw + ε) ，Gb和b同理，这里ε一般取1e-7，而α一般取0.01。这里AdaGrad会出现问题：由于训练开始的时候就不断的累计梯度的平方，从而导致学习率骤减，所以其效果也并不稳定。这时候就出现了RMSprop，一种Hinton提出的和AdaGrad非常相似但是效果更好的优化方案，设置一个decay_rate β，S<sub>dw</sub> = βS<sub>dw</sub> + (1-ρ)(dW)²， S<sub>db</sub> = ρS<sub>db</sub>+(1-ρ)(db)²，W和b的更新公式和AdaGrad一样。  

既然Momentum和RMSprop都很好，那么我们不如将两者结合在一起形成一个新的算法，称作Adam。我们首先要将Vdw,Sdw,Vdb,Sdb初始化为0，然后用小批量梯度下降计算出dW和db，然后分别用动量梯度下降和RMSprop计算一遍，分别除以(1-β1/β2)，然后进行梯度下降，不同点是用V除以S的平方根，也就是说将两个算法结合了起来。至于Adam这个名字的由来，并不是发现者的名字叫做Adam，而是因为Adam代表Adaptive Moment Estimation（自适应估计）。Adam将两个非常有效的算法结合在了一起，所以Adam事实上相当有效，在很多网络上都有良好的表现。  

我们会发现，在所有的优化算法中，共同出现的一个超参数叫做Learning rate学习率，事实上对于学习率，我们不必要一直坚持使用一个固定的值，而是使用逐渐衰减的学习率表现更好，这称作learning rate decay，也就是字面意思学习率衰减。值得注意的是这是一个二阶优化项，一开始我们没有必要设置学习率衰减，随着第一次优化的逐渐进行我们可尝试着进行学习率衰减操作从而进行二阶优化。详细的说一下一阶优化和二阶优化，我们的一阶优化用来优化我们真正的损失函数，方法是使用梯度形式的线性逼近来逐步降低近似值，但是我们会发现在一些变化非常大的地方，一阶优化很难完美的符合损失函数的变化，这个时候我们就要进行二阶优化，试着以泰勒的二次项展开形式来代替一次项展开形式，也就是线性变换。那么我们的二次函数可以使用梯度和海森矩阵去形成二次近似值估计，从而不断的降低我们的近似值。对于二阶优化，我们其实不需要学习率，我们只是尽量在每一步都尽可能的减小学习率，所以我们会使用牛顿法来降低损失函数的值。一个比较常见的二阶优化算法叫做L-BFGS，具有收敛速度快，内存开销小的特点，在大规模数据计算和训练中有良好的应用，但是L-BFGS在mini-batch算法里面效果不是很好，其在full batch update的表现会更好。  





---
本博客支持disqus实时评论功能，如有错误或者建议，欢迎在下方评论区提出，共同探讨。  
