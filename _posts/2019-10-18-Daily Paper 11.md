---
layout: post
title: "Daily Paper 11"
description: "Notes"
categories: [CV-classic]
tags: [Paper]
redirect_from:
  - /2019/10/18/
---

# Daily Paper 10 - DeepFace: Closing the gap to human level performance  

## Introduction  

这篇paper是FAIR和以色列的特拉维夫大学共同发表在CVPR2014上的，主要内容是使用孪生网络建立了一个DeeoFace模型，并获得了很好的人脸识别表现。  

在当前的人脸识别中，通常经过四个步骤：检测-对齐-表示-分类，作者在对齐和表示这两个步骤里提出了新的方法，通过显式的3D建模来实现分段仿射变换，并从一个9层的卷积神经网络中派生出人脸的表示。所谓的仿射变换，指的是对一个图形进行缩放、平移、旋转、反射、错切等变换或其组合。该模型有超过1.2亿个参数，因此作者在一个在当时最大的人脸数据集上进行训练，并在LFW上得到了很好的表现。  

无约束人脸识别问题一直是最近被广泛研究的问题之一，无约束人脸识别更困难，但应用场景也更为丰富，目前机器和人类表现的差距还很大。而作者提出的deepface模型基本上消除了两者之间的差距，其表现已经接近人类准确率，并且可以经过微调便能够很好的适应目前的人脸识别系统。此外，该系统产生的人脸表示非常紧凑，和其他系统上万的特征完全相反。  

该系统使用了深度学习框架来代替了主流的人工设计特征，深度学习特别适合大样本的训练集，近些年在计算机视觉、演讲和语音建模等领域都取得了很大的进展。对于图像具体来说，提取到人脸特征的关键在于快速的3D对齐，网络结构基于以下假设：一旦对齐完成，人脸局部的每个区块在像素级别就固定了，因此才可能从原始像素的RGB值中进行学习。  

总结一下，作者做了以下贡献：开发了一个有效的DNN架构，实现了通过带标签的大型数据库进行训练和学习获得人脸表达的方法，并能很好的泛化到其他数据库中；开发了一个有效的基于3D人脸模型的人脸对齐系统；在LFW数据库上得到了近乎人类的性能，将YouTube Faces DB上将错误率降低了50%。  

## System  

### Face Alignment  

目前存在的人脸对齐通过提供一个标准化的输入来改进人脸识别算法的性能，但是在无约束的场景中，仍然会被很多因素所影响，从而在带有身份的面部形态识别中表现不佳。但是已经有一些复杂的对齐方法来改进性能，比如说1.应用分析型人脸3D模型。2.从外部的数据集中寻找相似的基准点设置。3.使用无监督方法来从像素中寻找相似度转换。但是到目前为止，还没有一种实质上能够完全解决无约束人脸验证的对齐问题的方法，最近在无约束领域中，3D建模逐渐失宠，但是由于识别的物体都是3D的，作者坚信还是要用3D建模来解决这一问题。在这篇paper中，作者使用基于基准点的3D建模方法，把人脸转为3D正脸，具体方式是使用支持向量回归器SVR训练从图像描述器中预测point configuration，图像描述器基于LBP直方图，但是其他特征也能被考虑到。通过一个诱导相似矩阵将图片转变为一个新的图片，作者可以在一个新的特征空间继续运行基准点探测器，进而矫正定位。接着检测出6个基准点，分别是双眼中心、鼻尖和嘴巴，接着拟合一个对图像的仿射变换（缩放、旋转和平移），生成一个2D对齐裁切。而2D对齐裁切无法解决无限制场景下非常重要的平面外旋转，因此作者使用了一个通用的3D模型，并建立了一个3D仿射相机，用来将2D对齐裁切出来的图像弯曲成3D模型形状，即将2D模型转换为3D模型。这一步需要使用第二个SVR来定位额外的67个2D基准点，作者将67个2D基准点手动放置在3D模型中，从而得到了这67个点在2D和3D模型的完整相似度，具体的数学推导我也看不太懂，也没找到有人看懂后写的博客，就跳过这一部分了。而Frontalization部分则是以根据67个基准点引出的德劳内三角剖分算法（Delaunay triangulation）从x2d(source)到x˜3d(target)的分段仿射T所实现。  

简单来讲，作者所做的就是LBP+SVR的方法检测出人脸的6个基准点，通过拟合一个对基准点的转换（缩放，旋转，平移）对图像进行裁剪，对图像定位67个基准点，并进行三角剖分，用一个3D人脸库USF Human-ID得到一个平均3D人脸模型，然后学习一个3D人脸模型和原2D人脸之间的映射P，并可视化三角块，然后通过相关的映射，把原2D人脸中的基准点转换成3D模型产生的基准点，最后得到一个3D的人脸。  

### Face Representation  

至于人脸的表示，作者训练了一个深度神经网络用于人脸图像的特征表示。神经网络的架构如下：首先通过预处理阶段，输入3通道的人脸进行3D校正，接着归一化到152×152的像素大小；接着通过包含32个11×11×3的filter的卷积层C1，得到32张特征图，此时的尺寸为32×142×142×3，接着通过最大池化层M2，滑窗为3×3，步长为2，在三个通道上分别进行池化，最后通过另一个包含16个9×9×16的三维卷积层C3，得到16×63×63×32的特征图。这三层是为了提取低水平的特征，比如简单的边缘特征和纹理特征，最大池化会使卷积网络对于边缘的变换和小的标记误差更加鲁棒，不过池化也会使得网络丢失一些图像的信息，所以作者只在第一层进行最大池化。  

紧接着的三个层是局部卷积层，对于特征图上的每个位置，都学习不同的卷积核，因为人脸的不同部位特征不同，比如眉毛眼睛之间的区域比鼻子和嘴巴之间的区域，有着不更高的区分能力，因此在一个特征图中，卷积核的参数不共享。这种局部的卷积层虽然不会增加特征提取的负担，但是却增加了所要训练的参数数量，因此要求我们需要更多的数据进行处理。  

最后是两个FC层，用来捕捉人脸图像中距离较远的区域之间特征的相关性，比如眼睛的位置和形状和嘴巴的位置和形状之间的关联性。第一个FC层的输出提取出来作为人脸特征，可以和LBP特征进行对比，结果还是有很大区别的；第二个FC层的输出被传递到Softmax层作为分类，使用交叉熵损失函数进行训练和优化，使用SGD进行反向传播。作者在F7，也就是第一个FC层进行了dropout，此外这个网络高度稀疏，由于ReLU的使用，75%的顶层元素特征都是0……由于训练集合很大，在训练的过程中未发现明显的过拟合现象。  

此外，作者还对F7的输出特征进行了归一化，方式是将原始值除以训练集中所有样本的最大值，从而使得所有的值都在0到1之间，这么做的原因是为了降低特征对光照变化的敏感度(？？？)，由于使用了ReLU，该系统对于图像亮度的改变并不是不变的。  

### Verification Metric  

所谓的verification，就是输入两个图片判断是不是属于同一类或者同一个人。由于在监督学习中，测试数据和训练数据的不一致会导致其泛化能力很差，此外在小数据集上进行训练的模型泛化到其他数据集上表现也会很差。作者试图训练一种在各种数据集上都泛化很好的无监督学习的metirc，这里是简单的将两个归一化的向量做内积，作者也进行了监督学习的metric测量，使用的是χ²相似度和孪生网络。  

作者使用加权的χ²相似度，公式的权重参数由一个线性SVM学习得到。而作者也同时测试了一个端到端的度量学习方法孪生网络，将网络复制两份，两个人脸分别输入两个网络，网络共享参数，最后计算出两个输入的特征向量的距离，用一个FC层映射为一个0/1逻辑单元作为判别，为了防止过拟合，训练的时候只训练最高的两层。  

## Experiments  

### SFC  

作者首先用了三个不同的数据集DF-1.5K，DF-3.3K，DF-4.4K，数字表示人数（1.5M，3.3M，4.4M的人脸），发现从1.5K到3.3K错误提升不大，说明模型可以容纳3.3M的人脸。到4K时错误率上升到了8.74%，可以使用更多的人脸（错误率升的不大）。接着又测了另外三个DF-10%，20%，50%，百分号表示人脸数量比，10%的时候错误率为20.7%，表明数据集太少，过拟合了。最后又测试了sub1表示砍掉C3层，sub2表示砍掉L4和L5，sub3表示这三层都砍了，结果错误率3>2>1，由结果可以看出网络深度的必要性。  

### LFW  

训练结果显示DeepFace的结果基本上等同于人类的表现，表现好于其他所有方式，而使用多个网络做集成成为resemble的时候效果再次提升。  

### YTF  

---
本博客支持disqus实时评论功能，如有错误或者建议，欢迎在下方评论区提出，共同探讨。  
