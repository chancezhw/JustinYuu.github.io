---
layout: post
title: "Daily Paper 19: A3C for VLN"
description: "Notes"
categories: [MMML-VLN]
tags: [Paper]
redirect_from:
  - /2019/10/28/
---

# Daily Paper 19 - Attention Based Natural Language Grounding by Navigating Virtual Environment  

## Introduction  

这篇paper是Adobe和U Montreal在WACV2019上发表的，也是非常新的一篇paper了。这篇paper的主要贡献是做了一个可行的端到端VLN系统，该系统使用了视觉和语言双模态进行训练，使用了注意力机制实现双模态的融合，在2D和3D环境中都有高效高准确率的表现。此外，该模型在未知环境和zero-shot环境上的泛化能力非常优秀。  

作者认为VLN的模型应该要解决以下问题：agent必须有识别多个物体的能力，并且有记住一些之前遇到过的物体的能力，模型必须能够结构语言指令并将指令与环境中的实物相对应，最后还要能够在避开障碍物的前提下找到并到达目标位置。作者主要通过一个注意力模型进行模态融合，从而解决上述问题，读完Introduction看起来没什么创新点，应该是一个工程性质的论文。  

## System  

作者在两个不同的系统下进行了实验，第一个是作者自己创建的一个模拟2D环境，另一个是基于Vizdoom的3D环境。在2D环境中，agent只做向上、向下、向左、向右四个活动，类似于一个推箱子游戏，每一个物体和障碍物的尺寸、颜色等都有所差异，整个环境被封装在JSON文件中以供调用。agent接收一个RGB形式的鸟瞰图，鸟瞰图有一定的范围限制，超出边界的地方被遮挡住，agent同时接收一个自然语言指令，在正确完成指令的时候得到+1奖励，撞墙得到-1惩罚，碰到任何非目标物体的时候得到-0.5惩罚。每过一轮，整个环境都会重新的随机重组。而3D环境作者使用的是Vizdoom环境，并没有详细介绍。  

VLN系统的结构是这篇paper的重点，整个模型由3部分组成，分别是输入处理模块、多模态融合模块和策略学习（policy learning）模块。  

首先看输入处理模块，agent在2D环境中得到了整个系统的鸟瞰图，在3D系统中得到了以自己为中心的主视图，agent得到的图像被导入含有一系列卷积层的CNN中，得到最终的W×H×D特征图R<sub>E</sub>。此外，agent得到的指令中的每一个词都被转成了一个one-hot向量，然后通过GRU连接起来，GRU的最终时间步的输出就是整个指令的特征表示，之后导入n个平行的FC层中，得到了n个向量V。  

其次是多模态融合模块，作者使用一个简单的注意力模型来进行模态的融合。每个向量V，都被看做是一个1×1的2D卷积核，在特征图R上进行1×1卷积，最终得到一个W×H×1的注意力图，由于有n个向量V，所以最终得到了W×H×n尺寸的注意力图M<sub>attn</sub>，作者记为A3C<sub>attn</sub>。这里作者使用了1,5,10作为n的数值进行实验。注意力图被输入到策略学习模块中进行处理。作者这么处理的主要目的就是为了建立一个简洁的注意力图，将不必要的信息全部丢弃，获得一个简约的结果。作者还设计了一组将M和视觉表示合在一起的注意力图M<sub>netattn</sub>，记为A3C<sub>netattn</sub>，作者将这两个模型与之前的其他多模态融合模型A3C<sub>hadamard</sub>和A3C<sub>concat</sub>进行对比，具体结果见下面的Experiment部分。  

最后是策略学习模块，作者使用A3C这一强化学习算法进行策略学习，A3C内部有一个全局网络和多个local worker agents，这些agent有各自独立的权重和参数，它们分别独立的工作，用各自的梯度不断的更新全局网络。A3C是一个非常著名的强化学习算法，这里作者也没有详细的介绍，我找了一篇比较好的博客，这里是[链接](https://www.cnblogs.com/wangxiaocvpr/p/8110120.html)。  

## Experiment  

在具体的实验过程中，作者使用agent的成功率和获得的平均奖励作为评价指标。作者使用了两种不同的评估模式：第一种是Unseen scenario generaliztion(US)，物体的排列方式在训练集中没有出现过，主要评价该模型的泛化性能；第二种是Zero-shot generaliztion(ZS)，物体的种类和属性以及周围的环境都没有在训练集中出现过，主要评价该模型的零样本学习能力。  

具体的参数就不复述了，直接看分组和结果。首先作者探究n=1,5,10的表现，结果显示n=5的时候US和ZS上的mean reward和success rate都明显优于其他组。其次作者比较n=5的模型和其他主流的多模态融合模型在2D和3D场景的表现，结果显示A3C<sub>attn</sub>的表现始终是最好的。作者还可视化了A3C<sub>attn</sub>在2D和3D环境中的注意力图，结果显示不同的注意力图所注重的图像区域是不同的，而agent将这些注意力图的结果融合在一起进行策略学习。  

为了更好的理解GRU的嵌入质量，作者对GRU的输出使用了一个二维PCA，结果显示该模型可以隐式的理解、组织不同的概念，并学习他们之间的内在联系。  

作者发现该模型还有一个副作用，由于模型可以将语言指令对应到物体模型，因此如果模型可以处理不同语种的语言指令，那么该系统就可以进行语言之间的翻译。作者使用英语和法语指令进行训练，以观察该模型的翻译性能，最终发现对指令的翻译准确率能够达到85%。而且作者强调，在训练的过程中给定的语种是任意的，这样最终的结果并不是来源于指令语种的直接对应，而是来自于模型对于语言内含的物体种类的理解，也就是说，该模型可以以一种完全无监督的方式进行翻译。  

## Conclusion  

总结一下，作者使用了一个基于注意力机制的简易模型，通过强化学习实现了VLN的落地应用，该模型在2D和3D场景都有良好的泛化能力，对未知场景的适应性非常好。此外，作者还将场景数据和模型代码全部开源，以帮助其他研究者进行相关领域的研究。  

---
本博客支持disqus实时评论功能，如有错误或者建议，欢迎在下方评论区提出，共同探讨。  
