---
layout: post
title: "Daily Paper 10: NiN"
description: "Notes"
categories: [CV-classic]
tags: [Paper]
redirect_from:
  - /2019/10/17/
---

# Daily Paper 10 - Network in network  

## Introduction  

Network in network也是相当经典的一篇论文了，这是一篇新国大的论文，主要提出了一个新的深度网络结构，称作“Network in network”，可以增强模型在感受野内对局部区域的辨别力。传统的卷积层使用一个线性滤波器后接一个非线性激活函数，而作者构建了一些结构稍复杂的微型神经网络来抽象感受野内部的数据。作者使用多层感知机来实例化微型神经网络，特征图是通过像CNN一样在微型网络上滑动实现的，接着被传到下一层去。通过微型网络增强局部的模型，我们就可以在分类层中利用所有特征图的平均池化层，GAP相比于传统的全连接层来说更容易被解释，同时也更不容易过拟合。NIN在CIFAR-10和CIFAR-100数据集上得到了目前最好的表现，也在SVMH和MNIST上面得到了不错的表现。  

CNN的filter是底层数据块的广义线性模型（GLM），但是作者认为这种抽象程度是低的，抽象是指特征对同种概念的变体而言是不变的，而用一个更强大的非线性函数逼近器来替换GLM可以提高局部模型的抽象能力。作者认为，GLM在样本的隐含概念线性可分的情形下抽象效果很好，举例而言比如这些概念的变体都在GLM分割平面的同一边，而传统的CNN默认了样本的隐含概念是线性可分的，但是问题在于同一概念的数据都是非线性流形的，而通常来说都是通过输入的高维非线性函数来捕捉这些概念，因此GLM在这方面并不能很完美的起到抽象的作用。作者采取了用一种微型网络的方式进行代替，选择多层感知机来实例网络，这个多层感知机既是一个函数逼近器，也是一个通过反向传播训练的神经网络。  

最后作者将这个结构称作mlpconv层，即MLP+Conv，线性卷积层和mlpconv层都从局部感受野映射到了输出的特征向量，但是mlpconv将局部块的输入通过一个由FC层和非线性激活函数组成的MLP映射到了输出的特征向量中。在mlpconv中，MLP在所有局部感受野中共享，而NIN的总体结构就是多个mlpconv层的堆叠，这些堆叠组成了一个新的网络，我们称之为Network in network。  

同时作者并没有采取传统的全连接层进行CNN的分类，而是使用最后一个mlpconv层的特征向量的空间平均值，通过一个全局平均池化层实现以上想法作为类别可信度，然后将得到的结果传入softmax里面，这是由于GAP具有更好的可解释性，它使用微型网络构成的局部网络很好的实现了强化特征图和分类之间关系的功能。此外，全连接层更容易过拟合，并严重依赖dropout进行正则化，而GAP本身就是一个结构化的正则化器，从而从整个结构上就避免了过拟合。在我的理解下，全连接层由于参数实在是过多，导致训练这么多参数很容易过拟合，而GAP本身参数就很少，并且去除了将特征向量展开称为一维向量再重新分类的过程，直接对最后的每个channel都赋予实际的意义，这样正则化就不再是曲线的正则化，而是整体结构上的正则化了。  

## Convolutional Neural Network  

经典的卷积网络，如果使用ReLU作为激活函数，那么公式是这样的：f<sub>i,j,k</sub> = max(w<sub>k</sub><sup>T</sup>x<sub>i,j</sub>,0),(i,j)代表处理特征图像素的索引，x<sub>i,j</sub>代表以位置(i,j)为中心的输入块，k代表channel。  

当隐含概念线性可分时，这个公式可以很好的进行抽象，但是要提取更好的抽象，应该使用输入数据的高度非线性函数，我们可以使用一套filter来弥补，但是使用过多的filter会带来太多的计算负担。我们知道高层的filter映射到原始输入的感受野非常大，它通过结合下层的较低级概念来生成较高级概念，作者希望可以在生成高级概念之前就进行很好的抽象。  

作者拿来进行对比的是maxout网络，在该网络中，特征图的数目通过affine feature map，即未进入ReLU的线性卷积结果来做最大池化，这种线性函数的最大化使其能逼近任何凸函数，因此这个网络能够很好的分离凸函数集内的概念，使得其表现非常出色。但是问题在于，maxout的前提是隐含概念位于输入空间的凸集中，但是这是不一定的，因此会产生意外情况，所以作者想找到一个更为通用的函数逼近器。  

## System  

接下来看NIN的具体结构，其实上面已经说得比较具体了，这里就简要提一下。  

### mlpconv  

首先来看mlpconv层，这也是整个体系的关键部分，作者在选取函数逼近器模型时有两种选择，一种是Radial basis network，另一种是MLP，作者选择了MLP。原因有2:第一，MLP对神经网络和反向传播这种训练模式更为兼容；第二:MLP本身就是一个深度模型，这也与特征的重新利用相符合。在mlpconv层内部，使用ReLU作为非线性激活函数。  

mlpconv层的公式如下：  
f<sub>i,j,k<sub>1</sub></sub><sup>1</sup> = max(w<sub>k<sub>1</sub></sub><sup>1</sup> <sup>T</sup>x<sub>i,j</sub>+b<sub>k<sub>1</sub></sub>,0)  
f<sub>i,j,k<sub>n</sub></sub><sup>n</sup> = max(w<sub>k<sub>n</sub></sub><sup>n</sup> <sup>T</sup>x<sub>i,j</sub>+b<sub>k<sub>n</sub></sub>,0)  

接下来的内容是我认为这篇paper最难理解的地方，我先将内容复述一下。从cross channel pooling的角度来说，mlpconv的公式是在一个普通的卷积层上的cascaded cross channel parametric pooling（级联交叉频道参数池化），每一个卷积层起到在输入特征图上加权重组的效果，然后导入ReLU函数中，然后在后续的层中一遍一遍的重复上述步骤，级联交叉频道参数池化允许复杂的和可学习的交叉频道信息进行交互。我第一遍看的时候完全不知道他在说些什么，所以慢慢查阅资料进行研究，所谓级联，就是不同对象之间的映射关系，简单来讲两个对象级联，代表一个对象发生某种变化的时候另一个对象也会发生某些变化。之后是交叉通道池化，这个概念在Maxout那篇论文中有解释，具体而言通过该池化方式，可以完成m个输出channel到n个输出channel的映射(m>n)，池化过程一般是最大池化，简单来讲就是跨channel的最大池化。那么回过头来重新看，所谓的cross不是交叉，而应当翻译成跨，那么从跨频道池化的角度来说，该公式是在一个普通的卷积层上进行级联跨频道参数池化，每一个卷积层都在输入的特征图上做加权的特征重组后导入ReLU激活函数中。而我们这个公式出发来看，的确也是这个意思，因为这个公式只是不断的进行线性变换和ReLU，线性变换乘的值就是权重w，因此本身就是加权的特征重组，而级联体现在这个公式是一层一层递进的，后续层所乘的参数直接联系到了所有前面的层中。  

那么继续往下，作者认为跨通道参数池化层也等同于1×1卷积，至于1×1我就明白了，因为我学过inception。事实上应该是这篇paper首先提出的1×1卷积，跨通道参数池化层本身就等同于一个卷积核为1×1的卷积层，因为它是跨通道的，所以相当于乘了一个相同规模的卷积核，那么mlpconv层就等同于conv层+1×1conv层。所谓的1×1卷积，其实就是只改变第三维度channel数值的一个线性计算，可以实现多个feature map的线性组合，并实现feature map在通道数上的变化。  

到这里还是很乱，所以我整理了一下顺序重新表述一下。由于卷积操作的目的是进行特征提取，而卷积本身是线性操作，所以只能进行线性特征的提取，而我们想要提取非线性特征，作者就想能不能加一个MLP层在原有的卷积层后面，从而提取出想要的非线性特征。而所谓的MLP层，就是同一层中不同channel相同位置的MLP，而这个MLP本身可以看做是一个级联的跨频道的池化层，因为最终的结果是跨频道信息的降维，相当于一个池化。这也可以看做是一个1×1卷积，因为MLP可以用两个1×1卷积来实现。具体做法是用一个1×1×c的卷积核与当前n×n×c的图像层进行卷积运算，然后再用一个该尺寸的一维卷积核进行第二次卷积运算，最终的尺寸不变，但是变的是最初的n×n×c同一个位置不同层之间的像素点，因为他们相当于过了一个c×c×c的MLP网络。  

作者接下来进行了maxout和mlpconv的对比，其实结论上面已经说过了，就是maxout只是一个凸函数的逼近器，而mlpconv是一个更为泛化的通用函数逼近器，可以对更多的隐含分布进行建模。  

### Global Average Pooling  

接下来介绍代替全连接层的GAP，GAP的做法是不在特征图的最顶端增加FC层，而是求每个特征图的平均值，然后将结果导入softmax层，这个方法的优点在Introduction已经说的很详细了，就是通过增强特征图和类之间的对应关系，并将卷积结构保留的很好，避免了展开再分类的方式，从而使特征图分类可解释并且可信。此外，GAP没有参数设置，而FC层有大量的参数，因此避免了过拟合，GAP还汇聚了空间的信息，从而对输入的空间转换更鲁棒。  

### NIN Structure  

整个网络的结构很简单，三个mlpconv层后加一个GAP层，mlpconv层内部的神经网络的子层为3层，内部的网络层和外部的网络层都是很灵活的，可以根据需要和表现来加减。  

## Experiment  

作者在四个数据集上测试了NIN的性能，分别是CIFAR-10,CIFAR-100,SVHN和MNIST，mlpconv层后面都跟随一个最大池化层，把原输入样本缩减一倍。作为正则化器，除了最后一个mlpconv层外所有输出都加一个dropout。训练过程和Krizhevsky一样，其实就是为了和AlexNet的性能做对比。下面简单的说一下在各个数据集上的表现。在CIFAR-10上，NIN+Dropout的错误率为10.41%，比当前最优结果降低了1%，增加数据增强后错误率为8.81%，也创了新纪录。在CIFAR-100上，NIN+Dropout的错误率是35.68%，比当前无数据增强的最好表现还好了1%。在SVHN上达到了2.35%的准确率，排名第三。而MINST上NIN+Dropout的错误率为0.47%，比最好的maxout高了0.02%，这是因为目前最好的性能已经非常好了。  

接着作者比较了GAP和全连接层的差距。结果显示全连接层没有dropout的表现最差，为11.59%，会产生过拟合，全连接层前增加了dropout后测试集错误率为10.88%。GAP在三者比较中得到了最低错误率10.41%。作者也实验了用GAP在传统的CNN上的表现，结果显示比没有dropout的CNN提高了1%，表示GAP的确有一定程度的正则化作用。  

作者还进行了数据的可视化，结果显示特征图的最大激活区域和输入的相关真实分类吻合，这明显是GAP加强过的，证明了NIN的有效性，通过mlpconv可以得到一个更强的局部感受野。  

## Conclusion  

这里就直接翻译了：我们提出了一个新的深度网络，叫做“Network In Network”（NIN），用于分类任务。这个新结构有mlpconv层组成，使用多层感知器对输入进行卷积，用GAP代替传统CNN中的全连接层。mlpconv层对局部块建模更好，GAP充当结构化正则化器，防止了过拟合。用NIN的这两个组件，我们得到了当前在CIFAR-10，CIFAR-100和SVHN数据集上最好的表现。通过可视化特征图，我们证明了来自NIN的最后一个mlpconv层的特征图得到的分类是可信的，并且使通过NIN做物体侦测变成了可能。  

---
本博客支持disqus实时评论功能，如有错误或者建议，欢迎在下方评论区提出，共同探讨。  
