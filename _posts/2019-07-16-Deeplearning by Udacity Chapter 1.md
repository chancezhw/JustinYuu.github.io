---
layout: post
title: "Deeplearning by Udacity Chapter 1"
description: "Notes"
categories: [Deeplearning-by-Udacity]
tags: [Python]
redirect_from:
  - /2019/07/16/
---

# Deeplearning by Udacity Chapter 1  

结束了deeplearning.ai吴恩达的系列MOOC，这里趁热打铁把Udacity的MOOC也看了，就当是一次总结和回顾。Udacity的这门MOOC也是大名鼎鼎的深度学习入门MOOC之一，由Google首席科学家Vincent Vanhoucke大神开设，主要从机器学习入手，过渡到深度学习并进行全面的讲解，并用一些项目实战作为练习和知识补充。由于之前已经学习过Mahcine Learning和Deep Learning，所以这门MOOC的笔记会较为简略，只记录一些没学习过或者学习不到位的知识。      

第一周的课程标题叫做从机器学习到深度学习，显然要从基础的机器学习开始学起，首先介绍了监督学习和分类。我发现这门课程的很重要的一个特点是轻理论重实践，好在之前学过吴恩达的两门课程，所以对其理论有一定的了解，而这门课的大量实践部分恰好补足了我实践方面的缺失。  

介绍完基本的分类，接下来介绍了softmax模型，即将结果用概率的形式表达出来，并用一个小练习的形式让我们自行用python实现了softmax函数。此外，当softmax的值很小的时候，分布会趋于均匀，而当值非常大的时候，分布会趋于0或者1.总结一下，当softmax的自变量x很大的时候，softmax模型会趋向于自信，而当x值很小的时候，softmax反而会变得不自信，让所有的值都均匀分布，这个现象在以后将会用到。而当softmax能够正确分辨正确的值和错误的值，也就是将正确的值赋值为1，错误的值赋值为0时，softmax的结果就成为了one-hot向量。  

one-hot向量我们之前接触过，但是当数据规模非常大的时候，one-hot向量的规模也就必然会非常大，每一个one-hot向量的规模都是整个数据规模大小，并且大部分都是0，让编码效率变低。这个时候就引入了交叉熵(cross entropy)，用来计算两个向量之间的距离。这样将softmax的输出向量与正确值的one-hot向量进行交叉熵运算，也就是求距离，让正确值的交叉熵尽可能少，错误值的交叉熵尽可能大，那么最后的损失函数是将所有的交叉熵相加，那么显然，我们的目的就是让机器分类尽可能的正确，也就是让损失函数尽可能的变小。这样，我们就将机器学习问题转化成了函数极值的优化问题，这也是之前我没有弄明白的一点。事实上，从机器学习问题到极值优化问题的转变非常重要，也必须理解透彻。  

值得注意的一点是，在数值很大的情况下，一般稳定性较差。这里做了一个小实验，将1亿加上0.000001，做一百万次，最后减去1亿。正常情况下应该结果是1，但是python运算后结果只是0.953674316406，这就是python数值计算的不稳定性。所以我们要保证自己的数据不会太大或者太小，一般情况下保证均值为0并尽可能的同方差，所以我们会采取均一化的方法。此外，在初始化权重的时候我们采取随机初始化，从正则分布0-σ的区域内选择，σ一般从小的值开始选择，从而让训练性能逐渐变好。接下来通过梯度下降不断缩小损失函数。这里Udacity的MOOC中讲的非常快速和简略，并没有像吴恩达一样将具体的细节和原理介绍清楚，而是简单的告诉我just do it。不得不说这个层面上这门MOOC做的比较粗糙。  

这门课没有coursera那么方便的hub作为编程环境，需要自己通过docker搭建一个容器把课程提供的官方编译环境放进去，虽然有点麻烦，不过也挺好，可以学一下使用docker。

对于模型的性能评估，同样是使用训练集、测试集、交叉验证集三个数据集来进行。这里和之前学习的几乎一模一样，也很基础，就不再记录了。接下来还是过拟合和欠拟合的问题，数据集越大越不容易欠拟合，反之数据集过小容易造成欠拟合。而交叉验证集的规模越大越不容易造成过拟合，当然交叉验证集的数据分布要和测试集相同，这是Andrew Ng对我的谆谆教诲:) 有一个比较有趣的经验法则：如果一个改变在交叉验证集上影响了30个实例，那么一般这个改变是有效而影响显著的。  

讲完数据集的确立，接下来介绍损失函数的优化算法。那么首先肯定介绍SGD。随机梯度下降法，事实上这门课直接就介绍了mini-batch SGD，即小批量的随机梯度下降算法。我已经适应了这门课的风格，没有任何解释，没有任何原理说明，用了两分半的视频和1张PPT就结束了- -。接下来介绍动量法，以及学习率衰减，我实在是受不了这种讲解风格了，于是搬出了吴恩达课程的笔记看了一遍[Improving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimization Chapter 2](http://justin-yu.me/blog/2019/03/13/Improving-Deep-Neural-Networks-Hyperparameter-tuning,-Regularization-and-Optimization-Chapter-2/),笔记可真是个好东西。  

这样这一周就结束了，我很怀疑一个初学者能否很轻松的理解这一周的内容，特别是讲解过程中只有结论，没有任何推导和原理解释。不管怎样，我还是继续学习下去，就当快速复习一遍深度学习。  

---
本博客支持disqus实时评论功能，如有错误或者建议，欢迎在下方评论区提出，共同探讨。  
