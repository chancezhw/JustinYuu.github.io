---
layout: post
title: "Convolutional Neural Networks Chapter 2"
description: "Notes"
categories: [Convolutional Neural Networks]
tags: [Python]
redirect_from:
  - /2019/06/29/
---

# Convolutional Neural Networks Chapter 2  

第一周硬是学了两个星期，剩下的这三周课程进度要加快了，争取一周内学完。  

## Case Study  

这周的课程主要是介绍CNN的应用，吴恩达认为通过研究实例可以使自己的神经网络变得更好更有效，也可以更为有效的学会神经网络的应用。  
首先介绍了一些应用比较广泛的网络，例如LeNet-5、AlexNet、VGG-16等经典的神经网络，其差异主要体现在步长、过滤器数量、池化方式、激活函数的不同上。  
以上介绍的都是比较古老的网络，接下来介绍了一个比较新的神经网络——残差网络(ResNets)，它建立了一个'short cut'，也叫做'skip connection',使得激活函数计算后的结果al通过快捷路径而不是原始路径插入到很靠后的ReLU之间。而插入的部分称为残差块，多个残差块便构成了残差网络，其在深层网络中的表现比普通网络更好。  
接下来的问题在于，残差网络成功的原理是什么。吴恩达认为，对于这些额外的层来说，学习恒等函数非常容易，因此可以保证不会影响性能。很多时候，你可能会很幸运的提高网络的性能，或者至少有个不损害性能的底线，然后运用梯度下降来逐步提高其性能。我感觉吴恩达讲的不是太详细，因此找了一篇博客学习一下：[残差网络](https://www.cnblogs.com/wuliytTaotao/p/9560205.html)。  残差网络的原文链接在这里：[Deep Residual Learning for Image Recognition](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf)  

接下来吴恩达介绍了1×1卷积的作用。显然1×1卷积并不能起到卷积的效果，它只是将所有的输入数据放大或者缩小了同倍数，但是它还是有别的作用，首先比较好理解的作用便是升维/降维，通过设置不同的过滤器数量，从而将数据的维度变成过滤器数量的大小，也就是缩小了数据的通道数。至此，缩小长宽可以使用池化层，缩小通道数可以使用1×1卷积，数据中所有的部分都可以被缩小/增加了。此外，它还可以利用后接的非线性激活函数来增加非线性，具体来讲，可以使用通道规模相同的1×1过滤器，使得完成一次卷积+非线性整流后可以达到规模不变的效果，那么在这一次操作中，由于平白无故的多进行了一次非线性操作，因此所训练的网络可以学习到更为复杂的函数形式，从而使神经网络更为强大。  


---
本博客支持disqus实时评论功能，如有错误或者建议，欢迎在下方评论区提出，共同探讨。  
