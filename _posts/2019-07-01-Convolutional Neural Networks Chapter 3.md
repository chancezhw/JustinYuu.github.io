---
layout: post
title: "Convolutional Neural Networks Chapter 3"
description: "Notes"
categories: [Convolutional-Neural-Networks]
tags: [Python]
redirect_from:
  - /2019/07/01/
---

# Convolutional Neural Networks Chapter 3  

本周是这门课程的第三周，主要介绍对象检测和自动驾驶领域的一些内容，编程作业也用了这一领域很火的yolo物体检测算法。  

## Detection algorithms  

实现对象检测的前提是能够对对象定位，因此吴恩达在这一章首先讲的算法就是对象定位(classification with localization)。通常来讲，需要定位和识别的有多种物体，因此一般情况下需要通过softmax进行分类识别。定位一般分为单物体定位和多物体定位，本周主要研究前者。  
如果是简单的classfication问题，那么只需要通过softmax分类就可以，但是现在的问题在于要识别物体的边框坐标，那么这时候就要定义三个向量，分别是表示物体是否存在的向量pc，边框的四个参数，分别是bx,by,bh,bw，代表着边框中心的横纵坐标，边框的高度和宽度，以及物体分类的one-hot向量。那么如果要探测物体是否存在，那么上述向量都是需要的，但是如果要探测物体是不是不存在，那么上述向量中只需要研究pc和one-hot中的该物体对应的值，并且事实上，不同的向量可以用不同的损失函数。一般的，可以设立一些特征点（landmark），进行特征点检测，从而识别关键轮廓上的坐标。但是值得注意的是，在所有的样本中，特征点要保持一致。  
完成了这些，就可以进行对象检测了。对于图像的检测，一般通过滑动窗口进行检测，窗口的大小由小到大不断增大，一般需要遍历多次，而且颗粒度不能够太粗，这就导致了对象检测的计算成本会变得很高，这就需要运用卷积的方式来进行滑动窗口。卷积之所以能够节省计算量，是因为滑动的窗口会不可避免的重复计算很多数据，因此造成了很多重复的计算量。卷积的做法是将被测图片直接输入到滑动窗口的convnet中，那么最后的尺寸较小的输出数据也就是滑动窗口切分原图片组成的与输出相同尺寸的区域的预测输出。  
事实上这部分内容出自一篇很著名的论文，也就是基于OverFeat的图像分类、定位和检测。运用卷积的方式实现滑动窗口的关键之处在于，将Conv->FC的这一步看做是对一整张图片的卷积层运算，而将FC->FC的这一步看作是采用1* 1大小的卷积核进行卷积核运算。这一思想又称之为FCN(Fully Convolutional Network),这一思想可以使得一个已经设计好的CNN模型可以输入任意大小的图片，大大增强了该模型的可迁移性。具体来讲，FC的思想是将三个维度的数据展开成一个维度的数据，那么这和与同等规模的过滤器进行卷积操作得到的结果是相同的，只是看待的角度不同了而已。那么如果把FC看成把原数据与相同规模的过滤器进行卷积的话，把原图像更换为任意大小的图像，结果可能不是1×1大小的图片，但是其结果与原图像息息相关。假设结果是2×2，那么4个像素点每一个就对应于一个角落裁剪下来的图片预测分类结果。  
由于sliding window是固定选取的，所以识别出的bounding box可能位置不够精确，和目标的形状实际不符，比如实际更宽或更高，或者stride比较大。这时候就引入了一个著名的算法：YOLO(You Only Look Once)，具体来讲，YOLO中没有滑动窗口，而是将整个图片分为n个网格（grid），对于每一个网格单元，都设置上文所描述的pc，one-hot和四个参数这些labels，同样的这只是识别是否存在目标物体的时候，如果识别是否不存在的时候，只需要关心pc即可，也就是说对每一个grid都应用classification with localization算法。值得注意的是，在grid里识别物体的时候，确定是否在网格内的依据是看中心点是否在网格内，并且整体坐标是以grid为参考的。此外，四个参数中，bx和by是在0-1之间，但是bh和bw的大小可以超过grid的大小，也就是说高度和宽度是按照物体的实际尺寸来确定的，不受grid大小的约束。那么对每个grid都进行一次classification with localization之后，假设grid是3×3，检测的物体种类有3种，那么总共的参数是4+3+1=8，那么输出的总尺寸便是3×3×8，按照这个总输出便可以设计一个卷积网络，只需要让最终的output volume等于3×3×8即可。这时候便可以根据输入尺寸来选择convnet和池化层的参数大小，使其有我们规定的输出规模。YOLO的优势在于可以精确的输出bounding box，并且它是通过一整个卷积实现的，用一个卷积网络进行一次正向计算即可，并不需要进行网格数量规模的计算，这使得它的运算速度非常快，甚至可以达到实时的程度。  
接下来介绍了一些可以提高YOLO算法性能的改进算法，首先是交并化（IoU），可以用于检测目标检测算法的性能，也可以向目标检测算法里添加一些其他的辅助工具以提高其综合表现。具体来讲，它的计算方式是用预测和真实边界框的交集除以并集，当然在此之前要先计算交集和并集的大小，如果结果大于等于0.5，那么说明预测的结果是正确的，当然结果越高越好，代表预测的边界框与真实的边界框越符合，0.5只是国际惯例，如果想严格一点就将其提高到0.6或者是自己满意的值。其次是非极大值抑制，目前的目标检测算法存在的问题是由于网格的判定规则是通过识别是否是中间点来确定是否在这一网格内的，因此有可能对于物体过大或者网格过小的场景下，同一目标可能会被多个网格识别为中心点在该网格内，这就会导致同一个物体被多次检测，这样就造成了计算结果的不确定性，因此便引入了非极大值抑制这一改进方法，使得每个对象确保只被检测一次。非极大值抑制应用了上文所述的IOU，首先选取一个最大的pc值的bounding box，然后抑制所有和它有高IOU值的bounding box，从而输出具有最大可能性的结果。  
锚框(Anchor Box)的作用是让一个网格可以识别多个物体，具体的做法是首先定义多个不同的边框作为"锚框"，然后将图像中要识别的多个物体分别和多个形状关联起来，即用一个图像匹配多个锚框，每个锚框对应一个需要识别的物体。简单来讲，锚框算法将图片中的每个目标分配到包含该物体中心点的grid和与其bounding box有最大IoU的anchor box中。总体来讲这相当于增加了一个匹配维度，即grid cell+anchor box。虽然锚框算法很强，但是还是有没法处理的情况，比如设置了两个锚框但是grid里面有三个物体，又比如两个物体的anchor box一样或者十分相似。吴恩达认为虽然锚框在课程里用于解决一个物体被多个grid检测到中心点的情况，但是事实上如果grid非常精密，这种情况很难出现，因此锚框的更大作用是通过设置锚框与特定的物体相对应，从而让算法更专注于训练某种特定的物体。最后，锚框怎么选择呢？一般情况下是手工选择的，但是也可以用更为高级的k-means聚类算法，通过聚类的结果来定义锚框。  
接下来总结一下YOLO算法，首先确定要识别的目标数量和具体的目标名称，确定网格划分的数量和锚框的数量，最终确定labels的维度，即网格高×网格高×锚框数量×((1+4+目标数量)×锚框数量)。首先建立training set，人工查看并标注每个grid里每一个锚框的pc，对于pc=0的网格或者锚框，不需要关心其他参数，但是对于pc=1的锚框，都要确定其他的4+目标数量的参数值。接下来用设计的卷积网络进行训练和预测，最后运行非极大值抑制，产生最后的预测值。  
介绍了YOLO怎么可能不介绍Faster R-CNN，即候选区域算法。R-CNN的中心思想是通过Segmentation Algorithm找出候选区域，对有可能出现的区域，也就是色块进行检测，这样可以减少检测量。RCNN不仅输出label，还会输出一个边界框，提高了准确性的同时，这个算法非常的慢。Fast R-CNN是对其的改进，其借鉴卷积的思想，运用卷积的滑动窗口去识别候选区域，增加了运行速度，但是它仍然需要通过聚类来得出候选区域，因此Faster R-CNN运用了卷积神经网络代替了传统的聚类分割算法来得出候选区域，这也进一步提高了R-CNN的速度。但是即便如此，Faster R-CNN的速度还是比YOLO算法要慢的，不过Andrew仍然认为将候选区域和进行检测这两步合为一体这个想法非常有前途，因此他认为Faster R-CNN是一个前景广阔的算法。  

## Programming Assignments  
这周的编程作业我觉得是最有收获的一次，运用YOLOv2模型实现了自动驾驶汽车的碰撞检测。整个编程作业内容适中，难度适中，有一定的挑战性，不像以往的编程作业偏简单，并且基本上除了训练YOLOv2的权重之外整个流程都走了一遍，我认为这么一个流程下来收获还是比较大的，对整个流程也有了一定的把握。唯一美中不足的一点就是对于TensorFlow和keras框架的教学还不够到位，很多时候我都会觉得很懵，不知道是什么操作。这一部分还需要看看其他教程或者课程来补充一下。  


---
本博客支持disqus实时评论功能，如有错误或者建议，欢迎在下方评论区提出，共同探讨。  
