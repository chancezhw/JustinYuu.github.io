---
layout: post
title: "Daily Paper 04"
description: "Notes"
categories: [Daily Paper]
tags: [Paper]
redirect_from:
  - /2019/10/11/
---

# Daily Paper 04 - Speaker Change Detection Using Fundamental Frequency with Application to Multi-talker Segmentation  

## Introduction  

今天的这篇文章是IC的大牛专业Dept EEE发在ICASSP 2019上的，main contribution是证明了时变音高特性可以显著的提高多说话者应答系统的分割表现，并用基频(音高)做了多说话者分割的SCD应用。  

从摘要出发，作者首先验证了音高的改变与说话者的改变具有强关联，这是由于个体的音高是平稳的变化的，因此可以通过Kalman过滤器来预测，那么当音高无法预测的时候，说明音高改变了，很简单的三段论。那么作者基于此提出了一个新系统，与常规的MFCC SCD系统进行对比，以AMI语料库作为原始数据，成功的将训练正确率从43.3%提高到了70.5%。那么作者的contribution主要就是这两点：提出了音高和讲话者变化的强关联性以及设计了一个基于此且表现原优于MFCC作为baseline的新系统。  

可是baseline怎么可能是43.3%呢？上一篇paper里面作为baseline的i-vector的准确率都可以达到86.6%啊我去……这里师兄给我答疑解惑说这里使用的AMI数据集杂音和重叠声太多，所以会导致表现不好，上一篇paper里面是用的别的数据集，所以表现会好一些。  

传统的ASR（自动演讲识别）只能实现识别说话者所说的内容，而不能回答是谁在说话。那么要实现这个功能，也就是Multi-talker recognition，必须要实现两个分目标，即who spoke when? 那么when的问题可以归结到SCD问题上，而who的问题则是聚类问题。整个问题看起来实现并不复杂，但是作者提到，由于回声和噪声两方面的影响，这个功能实现起来还是挺复杂的。  

那么SCD的采样流程其实看了这几篇论文已经很熟悉了，首先要应用不同的子系统进行segmentation，主要有MFCC进行BIC分割，使用uniform segmentation或者使用进化隐马尔可夫模型形式的一步式分割和聚类算法等等。那么这篇文章将会聚焦在segmentation本身，研究并证明音高的变化可以改进segmentation这一过程。之前的方法已经应用音高去改善了segmentation过程，比如用MFCC、LSP和音高一起改善对SCD边界的差值阈值的计算，也有用音高作为主要特征来进行实时应答的系统，但是之前的研究都没有将音高应用到模型中，或者对音高建模。对音高建模有两大好处：首先这个模型可以用于减少音高估计的错误率，其次模型估计音高所产生的错误可以用来检测用户的改变。  

具体来讲，作者所开发的新系统使用了一个叫做Kalman过滤器的东东去预测该说话者未来的音高，有研究已经使用过Kalman filter在之前预测过音高，因此可以直接使用，而Kalman filter只进行了音高的预测而并没有估计(estimation),所以我们还需要再额外加上音高的估计器，这里用了PEFAC，一个2014年研发的音高估计系统。那么整个方法背后的原理就是一开始讲的三段论：音高是可以预测的，如果不能预测，那么就是换人了。  

## System  

首先要证明这个系统，也就是上述三段论的可行性，这里作者并不打算用原理来证明，而是直接用实验证明其性能是优秀的。那么要实现这个系统肯定先要追踪当前讲话者的音高pitch，那么我们用PEFAC进行当前音高的估计，再用Kalman filter来进行音高的估计。但是事实上，不同的人的音高其实也会差不多，男女之间的差距会比较大，但是同性别的讲话者的平均音高都是差不多的，所以通过讲话者的平均音高来判断具体是谁是不可行的。那么怎么解决识别问题呢？  

上述部分已经说明了不同说话者的平均音高可能是类似的，但是就算是这样，我们也可以通过音调的改变来判断是否更换了人。之前已经提到过，一个人的音高基本上只在很平缓的幅度内改变，因此我们可以通过说话者当前的音高来判断未来的音高，作者再一次重复了一遍他的三段论，但是没有讲为什么在音调都差不多的情况下还是可以分辨出来，只是单纯的告诉我，我们可以的。  

具体的证明在下一部分，作者用AMI语料库做了一个实验，探究pitch和speaker之间的具体关系。作者列了一个表格，用来表示两个概率：音高改变同时也是讲话者改变的概率，以及讲话者改变同时也是音高改变的概率，结果发现音高的改变基本上等同于讲话者的改变，而讲话者的改变也很大程度上伴随着音高的改变，这就用实验直接证明了音高的改变是可以推导出讲话者的改变的，同时用了一个figure来表示，音高的变化的确会导致speaker的变化，总之用这个实验来证明了其逻辑是可行的。  

## Experiment  

接下来就是具体的实现，首先使用PEFAC来进行音高的估计，然后使用Kalman过滤器来预测音高的轨迹，具体的原理公式就不放了。由于这是一个时序检测，所以我们需要在每一帧都做出预测，但是只有在检测到声音的时候才进行参数的更新，作者设置了一个阈值ξ来判断是否有声音。没声音的时候估计的音高不变，但是预测的音高估计值方差会不断增加，因为在每一个时间步方差都会增加，也就使得预测更加不准确，因此我们还是希望无声音的时间步尽可能的少和稀疏。预测的结果会权衡Kalman filter在上一个时间步的输出值和PEFAC在当前时间步的估计值。  

那么每一个时间步都会有上一个时间步对当前时间步的预测和当前时间步对真实音调的估计两个不同的值，那么这两个差值，也就是error值使得我们可以能够更好的实现SCD问题。如果error大于某个阈值φ，那么就说明error太大以至于这个音高不可估计，反之则说明这个音高可以估计。那么当error大于阈值φ的时候，Kalman filter会建立一个新的filter，去追踪第二个讲话者，这时候就可以确定讲话者变化了，但是之前的filter还要继续保留，因为那个讲话者的信息需要被保留从而被识别。当讲话者变化被检测到的时候，PEFAC的音高测量会与之前所有的filter追踪的讲话者记录作比较，找到测量值和预测值差距最小的那一个。如果最小的那个filter和当前的测量值差距在一个新的阈值ρ以内，那么说明这个人就是之前的filter对应的那个人，那么就直接调用那个filter就好了，否则一个新的filter将会被建立,对应一个新的讲话者。不过之前的老问题还是没解决，不同的人可能会有类似的音调，那这时候该系统就分辨不出来了。  

在生成最终的segmentation的时候，我们将以pitch为依据的SCD检测结果和VAD的结果merge一下，VAD的输出主要是检测speech region，并将其中的小空白删掉连接在一起，此外VAD探测到的演讲起点和pitch检测到的演讲者改变会合并在一起分析，如果在间隔ζ内同时发生两者，则只保留后者。  

最后评价一下这个系统的性能，和经典的SIDEKIT segmentation系统作对比，比较一下准确率和可依赖性，用miss rate和hit rate以及multi-hit来量化，新系统的表现明显要好很多，为70.5%：43.3%，此外MSE（均方差）显示pitch还是略高一点，表示其鲁棒性略差，不过两者差距并不大。  

## Conclusion  

本篇paper总结一下，其最大的特色也是其创新点，就是它引入了pitch作为重要的SCD评判依据，其创新性的利用了音高pitch建模，通过预测pitch来对SCD进行优化，算是用新方法解决旧问题，从两方面对比来看进步还算是比较大，效果是不错的。  

---
本博客支持disqus实时评论功能，如有错误或者建议，欢迎在下方评论区提出，共同探讨。  
