---
layout: post
title: "Neural Network and Deep Learning Chapter 3"
description: "Notes"
categories: [Neural-Networks-and-Deep-Learning]
tags: [Python]
redirect_from:
  - /2019/02/16/
---
# Neural Network and Deep Learning Chapter 3  
第三章的内容比较重要，主要是深入探讨神经网络的细节，并实现一个神经网络。  

## Shallow Neural Network  
首先讲了有一层隐藏层的神经网络是什么样子，以及在这种神经网络下的反向/正向传播，这在我之前的博客中都有提到，之前也都学过，就不重复写笔记了。  
接来来讲了具体的表示方法，在神经网络的每一个结点中都包含两个步骤，第一个是计算z=wTx+b，也就是θx，第二个是计算a=σ(z),之后令yhat=a。这里吃过上次学习的亏，一定要把上标和下标搞清楚，上标指的是层数，下标指的是层内的结点数。  
在接下来是计算输出和向量化，由于一组数据可能包含多个数据，所以在上标的层数[i]后面再加一个数据号(j)，代表这组数据中的第j个数据。然后还是按照层的顺序进行z和a的计算。这样很自然的就想到了向量化，首先是将每一层的每一批结点向量化，然后是将每一批结点内的数据向量化，说白了就是将矩阵的横竖两个方向都向量化。事实上，参数z，a，还有x都是一个矩阵，a矩阵的列代表不同的隐藏层，行代表不同的训练数据，同理z矩阵也是，而x的行是不同的训练实例，而列是不同的特征。  
紧接着吴大大证明了向量化的正确性，证明方式是将行列数写清楚然后乘起来，从而得到数学运算的正确性，值得注意的一点是先将b设置为0，也就是计算不含b的部分，然后通过python的广播机制来加上b。  
![3-1](/images/Neural Network and Deep Learning)  
上图是目前的总结，我觉得这页PPT很好，就放到了这里。接下来是介绍了激活函数，吴大大介绍了另一个函数tanh，他认为这个函数严格优于sigmoid，数据中心化后平均值更接近0，因为其原始范围是-1到-1，而在二元分类的时候sigmoid会更好一点，其范围是0到1，因为在二元分类时我们只需要0和1，不需要负数。但是我想起了第一章采访时介绍的ReLU（线性整流）函数，我感觉ReLU函数好像比这两个都要好一点。果然后面就介绍了ReLU，ReLU函数的图像很简单，x负半轴上值为0，x正半轴上导数为1。  
这样吴大大最终确定了使用规则：当二元分类问题时用sigmoid，其他神经元上用ReLU，tanh刚刚闪亮登场就被无情抛弃，不过后来吴大大又提了一句有时候也会用tanh，但是没有说明具体情形。那我就很好奇了，于是我去Google了一下，发现了一篇足够新的文章，发表于19年2月11号[也谈激活函数Sigmoid,Tanh,ReLu,softplus,softmax - weibo248的文章 - 知乎](https://zhuanlan.zhihu.com/p/48776056)，假设他说的对，那么tanh好像的确用处不大，还有一个回答：[神经网络激励函数的作用是什么？有没有形象的解释？ - 论智的回答 - 知乎](https://www.zhihu.com/question/22334626/answer/465380541)。我又看了几个不同的回答，都没有提到太多tanh的用处，只是说比sigmoid要好，结合ReLU的缺点是learning rate过大会导致太多神经元失效，我只能理解当无法确定Learning rate的具体值的时候可以尝试一下tanh？此处存疑。  
学了这么久，我突然发现我好像没有彻底弄懂激活函数的作用，为什么没有激活函数就没法非线性分类呢？于是我找了一篇知乎回答[神经网络激励函数的作用是什么？有没有形象的解释？ - 颜沁睿的回答 - 知乎](https://www.zhihu.com/question/22334626/answer/103835591)，还有这一篇[神经网络激励函数的作用是什么？有没有形象的解释？ - 非理的回答 - 知乎](https://www.zhihu.com/question/22334626/answer/21036590)。  
吴大大还介绍了一个函数Leaky ReLU,该函数的特点为x负半轴上斜率很小，但不为0，这个函数的效果比ReLU要好，但是使用较少。吴大大说当你们以后用的时候，可以大胆的去尝试这个函数，那么我如果以后有机会做这方面的项目时，就大胆的用这个函数好了，看看效果咋样。



---
本博客支持disqus实时评论功能，如有错误或者建议，欢迎在下方评论区提出，共同探讨。  
