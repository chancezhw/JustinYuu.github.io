---
layout: post
title: "Machine Learning by Andrew Ng Chapter 10"
description: "Notes"
categories: [Machine-Learning-by-Andrew-Ng]
tags: [Octave]
redirect_from:
  - /2019/02/08/
---
# Machine Learning by Andrew Ng Chapter 10
 
## 前言  
过年实在太忙了，我都四天没学习了，这两天要赶紧把这门课结束。  
本章主要讲的是大数据样本的分析，包括随机梯度算法、在线学习和映射约简等重要算法。    
[第十章笔记](https://www.coursera.org/learn/machine-learning/resources/srQ23)  

## Gradient Descent with Large Datasets  
本节首先介绍了随机梯度下降算法，这是一种改进的梯度下降算法，通过这个算法可以实现较大数量的训练集的优化。  
随机梯度下降的第一步是随机打乱数据集，重新排列数据；然后对m个数据进行遍历，遍历的公式和普通梯度下降算法不一致，区别在于只计算这一单个数据的梯度下降，公式如下图。![10-1](/images/Machine-Learning-by-Andrew-Ng/10-1.png)  
这样来总结一下批量梯度下降和随机梯度下降的区别，批量梯度下降在每一轮θ的更新中，都用到了所有的数据集，得到一个标准梯度，即全局最优；而随机梯度下降在每一轮θ的更新中只用到了一组随机的数据，当然这组随机的数据并不一定能够很好的反映标准梯度，因此在某些轮循环中有可能会朝着不正确的方向进行梯度下降，但是在总体上来说最终还是能够收敛的（未证明）。这里引用一篇知乎的回答来更详细的解释一下[如何理解随机梯度下降(Stochastic gradient descent，SGD)？ - Evan的回答 - 知乎](https://www.zhihu.com/question/264189719/answer/291167114)。  
至于为什么最终可以收敛，我在google上搜到一篇综述，但是以我的水平我还看不懂，先贴在上面：[Optimization Methods for Large-Scale Machine Learning](https://arxiv.org/abs/1606.04838)  
我所理解的收敛是指找到极小值，这似乎可以理解到，而这类凸函数问题中，极小值就是最小值，那么最小值便也可以找到。这么理解倒是很简单，但是涉及到不动点迭代和鞍点之后就会更加复杂，以我贫瘠的数学知识还是无法理解更为复杂的东西。  

而小批量梯度下降恰好时批量梯度下降和随机梯度下降的中间体，每次循环中用b个数据，b的取值大于1，小于数据总量m，而b的值一般在2-100不等，因此叫做小批量梯度下降。具体的公式在官方笔记中有给出。  

最后一节介绍了评判是否收敛的办法，对于随机梯度下降，Andrew给出的建议是每1000个迭代都将1000个cost函数值求平均，然后画出图像，从而观察是否收敛。如果不收敛，可以改变学习速率，改变数据集数量或者改变特征变量等等。  

## Advanced Topics  
第二节介绍了在线学习机制，在线学习机制是一种大样本下的自学习算法，可以通过大量的数据流来优化自身。举个栗子，一家在线包裹公司需要得到用户对于其开出价格的满意程度，即可接受的价格，这时就可以通过在线学习算法来不断的调整自身的价格，从而达到一种用户可接受的合理价位。  
这种算法的特点是数据流需要稳定，一个数据只会在算法中使用一次，每次输入算法的数据都是新的数据。此外，算法会随着用户群更新而自适应更新。这种算法还可以引用在CTR（点击率）预测上，从而向用户推荐更适合、更流行的内容或商品。  

此外，本章还介绍了映射约简（Map reduce），这是进行大规模机器学习的另一种方法。映射约简我感觉很好理解，由于数据太多导致一台电脑无法独立处理，因此用一台中央处理器作为中介，多台计算机共同计算，也可以用多核处理器在一台电脑上进行多核运算。课程中还讲了分解的方式，以梯度下降为例，求所有样本的和，可以分成n份，由n个处理器独立计算，再将结果相加，由中央处理器来得出最后结果。  

---
本博客支持disqus实时评论功能，如有错误或者建议，欢迎在下方评论区提出，共同探讨。
