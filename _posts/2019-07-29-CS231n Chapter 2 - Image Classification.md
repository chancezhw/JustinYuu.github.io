---
layout: post
title: "CS231n Chapter 2 - Image Classification"
description: "Notes"
categories: [CS231n]
tags: [Python]
redirect_from:
  - /2019/07/29/
---

# CS231n Chapter 2 - Image Classification    

## Introduction  

这节课是博士生讲师Justin Johnson主讲。首先Justin讲了一大堆目前的CNN所能达到的成就，较为生硬的引入到了图像分类器的原理介绍中。对于图像分类器的识别原理，我们自然而然的会想到告诉机器需要识别的图像的特征，比如对于猫识别器来说，我们手动告诉机器什么是猫的耳朵、眼睛、鼻子等，从而让机器学会判定是猫，但是这种方式的可扩展性并不好，如果分类器从猫换成狗或者其他东西，那么就需要从头开始，所以我们力图寻求到一个新的可移植性的通用算法。  

在吴恩达的TensorFlow specialization这系列MOOC中，他说过一个令我印象深刻的话，即传统算法的目的是给定的输入和计算规则，产生特定的输出，但是机器学习算法的目的则是给定特定的输入和输出，产生独特的规则。那么这个找规则的过程就需要大量数据的参与，我们通过大量的数据训练我们的机器学习算法，然后用训练出来的规则来进行实际应用。  

## K-Nearest Neighbor  

那么对于分类器来说机器学习领域有非常多种，我们这门课程中接触的第一种分类器叫做Nearest Nighbor，这个分类器简单到stupid，它只是记住了所有训练数据的输入和输出，然后这个算法只能够预测和训练数据非常相似的测试数据的标签。我们将这个算法在CIFAR10数据集上进行训练，最后训练的结果比较不错，虽然准确度不是很高，但是基本上实现了相似图片的聚类。这里具体而讲，分类的依据，也就是判断与训练数据相似程度的方法是计算两者之间的L1距离，也就是曼哈顿距离，将两向量/矩阵之差加和即可求得。最近邻居法是最简单的机器学习算法之一，这个算法的局限性之一在于，训练的成本很低，只需要记住就可以，如果以指针的形式存在那么时间复杂度仅为O(1),但是每次预测都需要计算与所有label样例之间的曼哈顿距离，所以预测每个数据的时间复杂度都为O(n),这种情况其实使我们所不愿意看到的，因为我们最终的目的是应用的时候非常迅速，但是在训练的时候可以稍微花费多一点的时间，现状却恰恰相反，不适合实际应用。  

这个算法的另一个局限性在于，我们所有的距离计算都是L1距离，也就是说我们将所有的数据都映射到一个二维平面上，这就会导致我们的算法区域性强、局限性大，一些错误的数据或是噪声会产生较大的影响。对此我们的解决方式是找到K个最近的点，然后根据我们事先确定好的权重设置方法计算K个最近的点的最大权重在何处，然后选择该点。通过课程给出的图像实例可以看出，K数值变大，整个分类结果趋于平滑，较为突出的线条和孤岛不再存在，噪声造成的影响被削弱，这种算法叫做K邻近算法，简称KNN。  

此外，K邻近算法的距离评价指标不唯一，可以用L1距离，也就是曼哈顿距离，也可以用L2距离，也就是欧几里得距离。关于这两种坐标系，如果变换坐标系的话L1距离可能会改变，但是L2距离保持不变。对于两者的选择，Justin建议对于目标有明确的了解的话，可以用L1距离，如果对目标不太了解或对权重的大小不太清楚的时候，用L2可能会更好。事实上，Justin建议可以用自己确定的metric函数来自己定义距离，从而更广泛的扩展我们的K邻近算法，使其能够适应更多种需求。这里K的取值，距离的标尺都是不由训练数据得出的参数，我们称之为超参数，需要自行设置。  

这里Justin讲了一些选择超参数的tips，和吴恩达在DeepLearning.ai系列课程MOOC的第二门MOOC相比内容少了许多，只是讲了一些他认为比较重要的点：关于选择超参数，不要选择在训练集表现最好的超参数，也不要选择在测试集表现最好的超参数，而是将所有的数据分成三组：训练集、验证集、测试集。将所有的超参数都在交叉验证集上跑一遍，找出最适合的超参数和算法，然后在最后的最后再在测试集上得出最终的测试正确率。此外，我们还可以采用更好的交叉验证集，将data放入不同的fold中，将所有的fold结果取平均，从而得到更为准确的答案，不过这在数据集比较小的时候比较好用，在深度学习领域并不实用。  

但是，在图像识别中，K邻近算法无法使用……讲了这么多还没法用，我也是醉了。不能用的原因有三点，第一是在测试的时候速度慢，这在之前提到过；第二点是在像素上的距离标尺无法反应很多信息，比如色彩和像素的移动等，这就导致无法识别相似但实际上不同的图片；第三点叫做curse of dimensionality，随着dimension的增长，我们需要的数据会随着指数增长，也就是说我们几乎不会有足够大的数据集来覆盖多维空间，从而完成训练（为什么要完全覆盖？）。这里Stanford同学们的踊跃提问其实非常好，很多是我听懂的，有些是我没听懂但是没发现的，有的是我也有相同疑问的，不管是哪种都是一种答疑解惑或是重新复习加深。  

## Linear Classification  

线性分类是整个神经网络的基础，不管什么网络都需要线性分类的参与。首先，我们要学习将算法的数据权重化，储存在参数矩阵W里面，之后与数据x相乘，再加上一个常数权重，从而得到最后的预测结果，这样可以使得训练过程模型化、标准化。那么这就是线性分类的全部内容了，用公式表示就是f(x,W)=Wx+b，那么我们知道只做线性变换很难将数据完全分开，所以之后我们会加上非线性变换，甚至在三维或者高维空间中进行分类。  

这里讲的比较简略，但是事实上这里是机器学习的一个重要的思想，即用参数来表示权重，用权重来完成分类，那么我们就将复杂的机器学习任务变成了简单的（目前为止的）线性计算，这种转变其实非常重要，也是机器学习之所以有效的基础之一。  

## Python/Numpy Tutorial  

课后给了一个Justin Johnson创建的python和numpy的简单tutorial，并对matplotlib，scipy都有简要的介绍，内容和之前我的博客记录的MOOC"Data Processing Using Python"大同小异，可以参考当时的笔记，这里是cs231n的tutorial[链接](https://cs231n.github.io/python-numpy-tutorial/)，可供查阅。  

---
本博客支持disqus实时评论功能，如有错误或者建议，欢迎在下方评论区提出，共同探讨。  
