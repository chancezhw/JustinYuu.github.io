---
layout: post
title: "Daily Paper 15: YOLO9000"
description: "Notes"
categories: [CV-classic]
tags: [Paper]
redirect_from:
  - /2019/10/23/
---

# Daily Paper 15 - YOLO9000 Better,Faster,Stronger  

## Introduction  

这篇paper是YOLO的改进版YOLO9000，9000的意思是它可以检测9000个类，可见检测的物体范围相当广。首先作者提出了YOLO的改进版本YOLOv2，并取得了当今最好的识别率。其次作者提出了一个联合训练物体的检测和分类的方法，并在COCO和ImageNet上联合训练了YOLO9000，YOLO9000可以检测没有标注检测数据的物体类别，在只有200中的44个检测数据的情况下，在ImageNet检测验证集上得到了19.7的mAP，并且YOLO9000是一个实时检测系统，其应用性非常强。  

目前的检测问题的训练集规模都变小，原因是标注检测训练集要比标注分类训练集麻烦的多，因此作者想用一种新的方式，来将检测的训练集规模提升到分类的训练集规模大小，从而提升检测的效果。作者采取了目标分类的分层视图，使得可以将不同的数据集组合在一起，之后作者提出了一个重要的联合训练算法，采用分类数据来提升检测的数据集规模和检测系统的鲁棒性，用检测数据来提升物体定位的准确性。  

总体来说，作者一共提出了两个系统，一个基于YOLOv1改进的YOLOv2，一个基于联合训练得到的YOLO9000.  

## Better  

首先作者进行了一系列改进来提高YOLOv1的性能，即YOLOv2。YOLOv1存在一系列问题，比如用速度换精度导致其定位的错误率较高，相对于区域提案算法来说召回率太低，因此作者主要基于在维持分类的准确率的前提下提高召回率和定位准确度两方面进行优化。  

YOLOv2的改动主要有以下几点：  

1.批量均一化。批量均一化使得训练的收敛更为迅速，此外批量均一化具有正则化的作用，可以在不适用dropout的前提下避免过拟合，将mAP提高了2%左右。  

2.高分辨率分类器。一般来讲图片的尺寸都不会超过256×256，YOLO一开始是用的224×224，YOLOv2改成了448×448，将mAP提高了约4%。  

3.带锚框的卷积。锚框是一个很重要的概念，YOLOv1只是直接使用FC层来预测边框的坐标，而YOLOv2使用锚框来预测坐标。锚框(Anchor Box)的作用是让一个网格可以识别多个物体，具体的做法是首先定义多个不同的边框作为"锚框"，然后将图像中要识别的多个物体分别和多个形状关联起来，即用一个图像匹配多个锚框，每个锚框对应一个需要识别的物体。简单来讲，锚框算法将图片中的每个目标分配到包含该物体中心点的grid和与其bounding box有最大IoU的anchor box中。总体来讲这相当于增加了一个匹配维度，即grid cell+anchor box。使用锚框后mAP会略微降低0.3%，但是召回率会大幅提升，从82%提升到88%。  

4.维度聚类。当使用锚框的时候，作者遇到了两个问题，第一个是锚框的维度是手动选择的，如果选的好，那么网络的学习会更轻松，预测的结果也会更好，但是如果选的不好，那么结果就会不理想，所以作者采取了k-means聚类算法，用一种更为有效、风险更为可控的方式来解决这一问题，作者使用的聚类距离公式为d(box,centroid) = 1 - IOU(box,centroid)，这种公式相较于标准的欧氏距离来说更为直接有效，因为在这里并不是中心点的距离越近越好，而是整个区域重合度越高越好，即IOU越高越好。作者经过试验发现k=5的时候表现最好，聚类的结果和手动的选择有很大的不同，锚框更多的是高瘦的形状而非短宽的形状。作者随即对手动和聚类的效果进行了对比，结果显示聚类的效果要明显优于手动选择。  

5.直接定位预测。使用锚框会遇到第二个问题：模型的不稳定，特别是在训练的早期。几乎所有的不稳定都来源于锚框的xy坐标预测，这是由于x和y求解公式的无约束性导致的，这会导致x和y的值可以取到任意值，由于权重是初始化的，所以在训练的早期得到的坐标值可能会匪夷所思，因此会造成模型的不稳定。作者的改进方式是将预测的坐标值由绝对坐标改为相对于网格的相对坐标，这样真实的相对坐标值就会被约束在0到1之间，作者就可以使用logistic激活函数来进行预测值的约束。总体来说，该网络对每个网格grid cell都预测五个边界框，对每个边界框都预测5个坐标x,y,w,h,Pr\*IOU，这种改进方法比之前的锚框算法效果又提升了5%。  

6.更为细致的特征。YOLOv2的特征图尺寸是13×13的，不过作者想要进一步细化，因此通过添加了一个中间层，将尺寸调整到了26×26，具体方式是通过堆叠邻近的特征到不同的频道中，就像ResNet中的identity mapping恒等映射一样，这使得26×26×512的层可以转变为13×13×1024的层，这使得最终的尺寸不变，但是检测器运行在中间层之下，导致检测器可以获得更细致的特征图，这使得最终的表现提升了1%。  

7.多尺寸训练。多尺寸训练已经不是一个新鲜的trick了，很多paper都在用，作者在每10轮过后换一次尺寸，尺寸最小320，最大608，中间的差值为32，由于这个网络在小尺寸上的运行速度非常快，因此这个方式极大的提升了判断的速度。  

至此YOLOv2的所有改动都已介绍完毕，将这些改动全部合在一起，得到的mAP为78.6%，此外还显著的提升了召回率，并降低了计算复杂度33%，可见提升相当之大。  

## Faster  

一般的网络架构都是基于VGG-16的，而YOLO框架基于GoogLeNet进行开发，这是因为GoogLeNet所需的操作数更少，不过YOLOv2使用的框架是一种新型的网络，称作Darknet-19，是由19个卷积层和5个最大池化层组成的，它需要的操作数更少，但是在ImageNet上的表现也不差，top-1 72.9%,top-5 91.2%。具体的训练细节文中说的很详细，但是由于并不是主要内容，我就不一一转述了。  

## Stronger  

最后就是YOLO9000了，作者在这一部分详细的介绍了他们所提出的联合训练算法。作者采用检测数据来学习一些目标检测的信息，比如边框的坐标值和区分常见的物体，而使用分类数据来扩展该网络能探测的类的数量。在训练中，作者将检测数据集和分类数据集混在了一起，当数据来自检测数据集时，数据在整个YOLOv2损失函数上反向传播，但是当数据来自分类数据集时，数据只在网络的特定分类部分中反向传播。  

这个方法遇到了一系列问题，具体来讲，检测数据只有一些比较常见的类，比如猫、狗等等，但是检测的数据却分类更为广泛和详细，广泛并不是问题，但是详细却是问题，因为狗可能分为多种品种，那么这时候就会出现同一张图片在检测数据和分类数据的类别不同的情况出现，因此作者需要一种耦合这些不同标签的方法。有些人用softmax来进行二次分类，但是这在数据集合并的时候并不好用，因为不同数据集可能并不互斥，所以作者使用多标签模型来解决这一问题。  

具体来讲，作者使用树状的分类。ImageNet的标签来自于WordNet，那么作者就从WordNet的单词层级入手，比如Norfolk terrier和Yorkshire terrier都属于terrier这一个大类，那么terrier又属于hunting dog这一个更大的类，而它又属于dog这一个超大的类，这么一级一级的递推，那么分类的时候就可以按照层级进行树状分类。WordNet并不是采用的树状，而是一种多对多的对应层级，因为一个动物可能同时兼具多种属性，但是为了简化，作者还是将其看做一棵树来进行分层，得到了最终的结果WordTree。那么具体计算的时候，要计算某一个叶子的概率，只需要从最上方的节点概率开始算起，逐步乘以叶子的父节点即可。  

这种方法可以使类别从1000增加到1369，除此之外还有别的好处，在网络遇到一个不确定的新物体时，它会更倾向于预测大类而非细分类别，这样就提高了预测的准确率，提高了分类的表现。作者将ImageNet和COCO数据集通过WordTree结合在了一起进行联合训练，共有9418个类，基于此作者训练了YOLO9000，使用了YOLOv2架构，但是将5层的卷积块换成了3层的以降低输出的规模，最终得到了19.7的mAP，在156种从未见过的物体类别中也能得到16.0的mAP，不过YOLO9000对于一些COCO数据集中泛化的比较好的类，比如动物类识别良好，但是对于一些泛化的不好的类，比如衣服等基本无法识别。  

## Conclusion  

最后再做一个小总结，作者开发了两个系统，YOLOv2和YOLO9000，前者是对YOLOv1的改良，后者是通过联合训练得出的对9000个类别起到不错的检测效果的目标检测系统，YOLOv2代表了目前业界目标检测的最高水平，而YOLO9000率先提出了将检测和分类联合在一起进行联合训练的想法，对缩进检测和分类两者之间的gap具有重大的意义。  

---
本博客支持disqus实时评论功能，如有错误或者建议，欢迎在下方评论区提出，共同探讨。  
