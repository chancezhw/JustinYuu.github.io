---
layout: post
title: "Daily Paper 26"
description: "Notes"
categories: [MMML-Self_Supervised]
tags: [Paper]
redirect_from:
  - /2019/11/04/
---

# Daily Paper 26 - On Attention Modules for Audio-Visual Synchronization  

## Introduction  

因为昨天过周末没看论文，所以今天补上。这篇paper是之前multisensory那篇paper的改进版，主要改进是添加了注意力机制，文章貌似还没有发表，一直挂在arxiv上。这篇文章是UCF和Netflix合作的paper，一作是UCF的学生，在Netflix实习的时候搞出来的。这篇paper主要研究视听时序对齐(AVTS)任务中注意力机制发挥的作用。  

在人对视听对齐任务的判断中，人会更倾向于注意那些有辨别力的声音出现的部位，比如说话的嘴唇，而更不会去注意那些无差别的声音，比如BGM，因此作者也想通过注意力机制使得网络学习这一判别方法。作者用了巨长无比的篇幅来介绍了AVTS任务的背景和应用场景，以及详细的介绍了一下AVTS任务究竟为何物，由于AVTS之前介绍过了，这里就不再叙述了。作者训练了两个注意力模型，第一个是仅时序模型，第二个是时序+空间模型，同时作者使用软注意力机制，使得模型可以耦合多个差别性物体做出最终的结果判断，避免了逐个判断而产生的判别结果冲突。  

为了将时序注意力考虑在内，作者将每一个视频都分成了多个时序块，对于每一个时序块分别提取视听特征，在块内计算跨空间和时序域的全局池化特征。来自不同块的特征通过时序注意力模块中，在该模块里网络对每一个视频的时序块定义一个置信度分数，最终使用softmax输出所有的置信度分数，得到最后的结果。  




---
本博客支持disqus实时评论功能，如有错误或者建议，欢迎在下方评论区提出，共同探讨。  
